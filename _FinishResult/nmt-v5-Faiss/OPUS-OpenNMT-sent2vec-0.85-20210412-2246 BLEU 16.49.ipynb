{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OPUS-OpenNMT-sent2vec-0.85-20210412-2246 BLEU 16.49.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618306867092,"user_tz":-420,"elapsed":23510,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"280c8548-9414-4d9b-919d-b3591ee15e7f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618306873301,"user_tz":-420,"elapsed":4687,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"23ffc1fd-f2e6-468d-c55e-99f4f4d5c545"},"source":["# import os\n","# path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"OPUS-OpenNMT-sent2vec-0.85-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-sent2vec-0.85-20210412-2246'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-sent2vec-0.85-20210412-2246\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267621404,"user_tz":-420,"elapsed":24484,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"f72d2920-6dae-4404-e88e-113f56e75709"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mon Apr 12 22:47:01 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267637617,"user_tz":-420,"elapsed":40695,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"7f7bf187-2648-494c-f3af-78aaac8e44ba"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 8.5MB/s \n","\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/63/17c6ac0d8a0cfa5ff7257e52edb6759d12dc266392f6c97f5c65c0c7238c/pyonmttok-1.25.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 34.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n","\u001b[?25hCollecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.8.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=a61f626d04ab2c812ecaca07878436d95bc51f84795cbfb2b548eda09e78104b\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: pyonmttok, torchtext, waitress, configargparse, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.25.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 6.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267646396,"user_tz":-420,"elapsed":49471,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"7c66bc8c-f199-4cda-bcd4-d32d8072a5f6"},"source":["!wget https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_sent2vec.tar.gz\n","!mkdir data_bin\n","!tar -xvf 'opus_sent2vec.tar.gz'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2021-04-12 22:47:17--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_sent2vec.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 94255466 (90M) [application/octet-stream]\n","Saving to: ‘opus_sent2vec.tar.gz’\n","\n","opus_sent2vec.tar.g 100%[===================>]  89.89M  65.8MB/s    in 1.4s    \n","\n","2021-04-12 22:47:21 (65.8 MB/s) - ‘opus_sent2vec.tar.gz’ saved [94255466/94255466]\n","\n","en_train_EM_score_0.95\n","vi_valid\n","en_train_EM_0.95\n","en_train_EM_factor_0.85\n","en_train_EM_score_0.8\n","vi_train\n","en_train_EM_factor_0.8\n","en_train_EM_0.8\n","en_valid\n","en_train_EM_factor_0.95\n","en_train\n","en_train_EM_score_0.85\n","vi_test\n","en_train_EM_0.85\n","en_train_EM_score_0.9\n","en_test\n","en_train_EM_factor_0.9\n","en_train_EM_0.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267709580,"user_tz":-420,"elapsed":112653,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"be3ba083-37f0-43cb-c73d-0ee2afafa937"},"source":["!mkdir -p output\n","!onmt_preprocess -train_src 'en_train_EM_0.85' \\\\\n","-train_tgt 'vi_train' \\\\\n","-valid_src 'en_valid' \\\\\n","-valid_tgt 'vi_valid' \\\\\n","-save_data 'output/en-vi' "],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-12 22:47:29,715 INFO] Extracting features...\n","[2021-04-12 22:47:29,721 INFO]  * number of source features: 0.\n","[2021-04-12 22:47:29,722 INFO]  * number of target features: 0.\n","[2021-04-12 22:47:29,722 INFO] Building `Fields` object...\n","[2021-04-12 22:47:29,722 INFO] Building & saving training data...\n","[2021-04-12 22:47:31,058 INFO] Building shard 0.\n","[2021-04-12 22:48:03,626 INFO]  * saving 0th train data shard to output/en-vi.train.0.pt.\n","[2021-04-12 22:48:22,218 INFO]  * tgt vocab size: 50004.\n","[2021-04-12 22:48:22,557 INFO]  * src vocab size: 50002.\n","[2021-04-12 22:48:23,233 INFO] Building & saving validation data...\n","[2021-04-12 22:48:23,831 INFO] Building shard 0.\n","[2021-04-12 22:48:26,144 INFO]  * saving 0th valid data shard to output/en-vi.valid.0.pt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYyQECmi0TX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618292226395,"user_tz":-420,"elapsed":12476954,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"78fe93d1-011b-407c-cb18-b8b8b3c33322"},"source":["!mkdir -p model\n","!onmt_train -data 'output/en-vi' \\\\\n","-save_model 'model/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30000  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 1000 -save_checkpoint_steps 1000 \\\\\n","-report_every 1000 -world_size 1 -gpu_ranks 0"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-12 22:48:32,706 INFO]  * src vocab size = 50002\n","[2021-04-12 22:48:32,706 INFO]  * tgt vocab size = 50004\n","[2021-04-12 22:48:32,706 INFO] Building model...\n","[2021-04-12 22:48:40,927 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50002, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50004, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=50004, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-12 22:48:41,006 INFO] encoder: 44516352\n","[2021-04-12 22:48:41,006 INFO] decoder: 76479316\n","[2021-04-12 22:48:41,007 INFO] * number of parameters: 120995668\n","[2021-04-12 22:48:41,013 INFO] Starting training on GPU: [0]\n","[2021-04-12 22:48:41,013 INFO] Start training loop and validate every 1000 steps...\n","[2021-04-12 22:48:41,013 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 22:49:00,104 INFO] number of examples: 802825\n","[2021-04-12 23:00:20,658 INFO] Step 1000/30000; acc:  13.55; ppl: 550.74; xent: 6.31; lr: 0.00012; 7988/9751 tok/s;    700 sec\n","[2021-04-12 23:00:20,659 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 23:00:22,416 INFO] number of examples: 100400\n","[2021-04-12 23:01:57,843 INFO] Validation perplexity: 272.7\n","[2021-04-12 23:01:57,843 INFO] Validation accuracy: 19.1807\n","[2021-04-12 23:01:59,398 INFO] Saving checkpoint model/en-vi_step_1000.pt\n","[2021-04-12 23:02:09,291 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 23:02:33,883 INFO] number of examples: 802825\n","[2021-04-12 23:13:56,639 INFO] Step 2000/30000; acc:  27.91; ppl: 68.05; xent: 4.22; lr: 0.00025; 6848/8368 tok/s;   1516 sec\n","[2021-04-12 23:13:56,641 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 23:13:58,136 INFO] number of examples: 100400\n","[2021-04-12 23:15:32,798 INFO] Validation perplexity: 54.4362\n","[2021-04-12 23:15:32,798 INFO] Validation accuracy: 36.2968\n","[2021-04-12 23:15:34,346 INFO] Saving checkpoint model/en-vi_step_2000.pt\n","[2021-04-12 23:15:48,088 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 23:16:21,029 INFO] number of examples: 802825\n","[2021-04-12 23:27:36,406 INFO] Step 3000/30000; acc:  38.98; ppl: 25.83; xent: 3.25; lr: 0.00037; 6816/8320 tok/s;   2335 sec\n","[2021-04-12 23:27:36,407 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 23:27:41,200 INFO] number of examples: 100400\n","[2021-04-12 23:29:15,663 INFO] Validation perplexity: 28.3019\n","[2021-04-12 23:29:15,663 INFO] Validation accuracy: 42.7345\n","[2021-04-12 23:29:17,207 INFO] Saving checkpoint model/en-vi_step_3000.pt\n","[2021-04-12 23:29:34,329 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 23:30:01,059 INFO] number of examples: 802825\n","[2021-04-12 23:41:12,813 INFO] Step 4000/30000; acc:  43.98; ppl: 16.40; xent: 2.80; lr: 0.00049; 6842/8362 tok/s;   3152 sec\n","[2021-04-12 23:41:12,814 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 23:41:14,344 INFO] number of examples: 100400\n","[2021-04-12 23:42:48,885 INFO] Validation perplexity: 20.7773\n","[2021-04-12 23:42:48,885 INFO] Validation accuracy: 45.8051\n","[2021-04-12 23:42:50,428 INFO] Saving checkpoint model/en-vi_step_4000.pt\n","[2021-04-12 23:43:12,620 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 23:43:41,806 INFO] number of examples: 802825\n","[2021-04-12 23:54:48,769 INFO] Step 5000/30000; acc:  46.70; ppl: 12.90; xent: 2.56; lr: 0.00062; 6849/8363 tok/s;   3968 sec\n","[2021-04-12 23:54:48,770 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 23:54:50,299 INFO] number of examples: 100400\n","[2021-04-12 23:56:24,798 INFO] Validation perplexity: 18.429\n","[2021-04-12 23:56:24,799 INFO] Validation accuracy: 47.0011\n","[2021-04-12 23:56:26,307 INFO] Saving checkpoint model/en-vi_step_5000.pt\n","[2021-04-12 23:56:51,283 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 23:57:20,025 INFO] number of examples: 802825\n","[2021-04-13 00:08:22,713 INFO] Step 6000/30000; acc:  48.44; ppl: 11.09; xent: 2.41; lr: 0.00074; 6864/8380 tok/s;   4782 sec\n","[2021-04-13 00:08:22,714 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 00:08:24,213 INFO] number of examples: 100400\n","[2021-04-13 00:09:58,208 INFO] Validation perplexity: 17.7581\n","[2021-04-13 00:09:58,208 INFO] Validation accuracy: 47.4517\n","[2021-04-13 00:09:59,705 INFO] Saving checkpoint model/en-vi_step_6000.pt\n","[2021-04-13 00:10:29,203 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 00:10:57,553 INFO] number of examples: 802825\n","[2021-04-13 00:21:56,253 INFO] Step 7000/30000; acc:  49.78; ppl:  9.93; xent: 2.30; lr: 0.00086; 6862/8386 tok/s;   5595 sec\n","[2021-04-13 00:21:56,254 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 00:21:57,806 INFO] number of examples: 100400\n","[2021-04-13 00:23:32,077 INFO] Validation perplexity: 17.3127\n","[2021-04-13 00:23:32,078 INFO] Validation accuracy: 47.7376\n","[2021-04-13 00:23:33,583 INFO] Saving checkpoint model/en-vi_step_7000.pt\n","[2021-04-13 00:24:07,378 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 00:24:35,375 INFO] number of examples: 802825\n","[2021-04-13 00:35:30,484 INFO] Step 8000/30000; acc:  50.93; ppl:  9.07; xent: 2.21; lr: 0.00099; 6860/8379 tok/s;   6409 sec\n","[2021-04-13 00:35:30,485 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 00:35:32,024 INFO] number of examples: 100400\n","[2021-04-13 00:37:06,293 INFO] Validation perplexity: 16.9581\n","[2021-04-13 00:37:06,293 INFO] Validation accuracy: 48.0828\n","[2021-04-13 00:37:07,798 INFO] Saving checkpoint model/en-vi_step_8000.pt\n","[2021-04-13 00:37:45,257 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 00:38:12,273 INFO] number of examples: 802825\n","[2021-04-13 00:49:03,538 INFO] Step 9000/30000; acc:  52.37; ppl:  8.19; xent: 2.10; lr: 0.00093; 6875/8400 tok/s;   7223 sec\n","[2021-04-13 00:49:03,539 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 00:49:10,773 INFO] number of examples: 100400\n","[2021-04-13 00:50:44,727 INFO] Validation perplexity: 16.1345\n","[2021-04-13 00:50:44,727 INFO] Validation accuracy: 48.7663\n","[2021-04-13 00:50:46,209 INFO] Saving checkpoint model/en-vi_step_9000.pt\n","[2021-04-13 00:51:26,368 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 00:52:01,427 INFO] number of examples: 802825\n","[2021-04-13 01:02:49,721 INFO] Step 10000/30000; acc:  54.32; ppl:  7.23; xent: 1.98; lr: 0.00088; 6763/8254 tok/s;   8049 sec\n","[2021-04-13 01:02:49,722 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 01:02:51,288 INFO] number of examples: 100400\n","[2021-04-13 01:04:25,300 INFO] Validation perplexity: 15.8235\n","[2021-04-13 01:04:25,300 INFO] Validation accuracy: 49.2076\n","[2021-04-13 01:04:26,794 INFO] Saving checkpoint model/en-vi_step_10000.pt\n","[2021-04-13 01:05:11,214 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 01:05:39,305 INFO] number of examples: 802825\n","[2021-04-13 01:16:22,510 INFO] Step 11000/30000; acc:  56.14; ppl:  6.50; xent: 1.87; lr: 0.00084; 6868/8393 tok/s;   8861 sec\n","[2021-04-13 01:16:22,511 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 01:16:24,080 INFO] number of examples: 100400\n","[2021-04-13 01:17:58,359 INFO] Validation perplexity: 15.8445\n","[2021-04-13 01:17:58,359 INFO] Validation accuracy: 49.5701\n","[2021-04-13 01:17:59,899 INFO] Saving checkpoint model/en-vi_step_11000.pt\n","[2021-04-13 01:18:48,087 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 01:19:15,606 INFO] number of examples: 802825\n","[2021-04-13 01:29:56,267 INFO] Step 12000/30000; acc:  57.83; ppl:  5.91; xent: 1.78; lr: 0.00081; 6864/8380 tok/s;   9675 sec\n","[2021-04-13 01:29:56,269 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 01:30:03,882 INFO] number of examples: 100400\n","[2021-04-13 01:31:38,173 INFO] Validation perplexity: 15.9101\n","[2021-04-13 01:31:38,173 INFO] Validation accuracy: 49.6837\n","[2021-04-13 01:31:39,653 INFO] Saving checkpoint model/en-vi_step_12000.pt\n","[2021-04-13 01:32:33,001 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 01:33:09,398 INFO] number of examples: 802825\n","[2021-04-13 01:43:45,773 INFO] Step 13000/30000; acc:  59.39; ppl:  5.43; xent: 1.69; lr: 0.00078; 6734/8226 tok/s;  10505 sec\n","[2021-04-13 01:43:45,775 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 01:43:47,292 INFO] number of examples: 100400\n","[2021-04-13 01:45:21,205 INFO] Validation perplexity: 16.5937\n","[2021-04-13 01:45:21,205 INFO] Validation accuracy: 49.6905\n","[2021-04-13 01:45:22,671 INFO] Saving checkpoint model/en-vi_step_13000.pt\n","[2021-04-13 01:46:18,556 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 01:46:41,475 INFO] number of examples: 802825\n","[2021-04-13 01:57:13,838 INFO] Step 14000/30000; acc:  60.79; ppl:  5.05; xent: 1.62; lr: 0.00075; 6906/8447 tok/s;  11313 sec\n","[2021-04-13 01:57:13,840 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 01:57:21,029 INFO] number of examples: 100400\n","[2021-04-13 01:58:55,701 INFO] Validation perplexity: 16.9064\n","[2021-04-13 01:58:55,701 INFO] Validation accuracy: 49.7576\n","[2021-04-13 01:58:57,374 INFO] Saving checkpoint model/en-vi_step_14000.pt\n","[2021-04-13 01:59:57,437 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 02:00:32,451 INFO] number of examples: 802825\n","[2021-04-13 02:11:01,472 INFO] Step 15000/30000; acc:  62.11; ppl:  4.73; xent: 1.55; lr: 0.00072; 6748/8243 tok/s;  12140 sec\n","[2021-04-13 02:11:01,474 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 02:11:03,075 INFO] number of examples: 100400\n","[2021-04-13 02:12:36,841 INFO] Validation perplexity: 16.997\n","[2021-04-13 02:12:36,841 INFO] Validation accuracy: 49.8657\n","[2021-04-13 02:12:38,347 INFO] Saving checkpoint model/en-vi_step_15000.pt\n","[2021-04-13 02:13:41,444 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 02:14:14,415 INFO] number of examples: 802825\n","[2021-04-13 02:24:39,301 INFO] Step 16000/30000; acc:  63.31; ppl:  4.46; xent: 1.49; lr: 0.00070; 6827/8346 tok/s;  12958 sec\n","[2021-04-13 02:24:39,302 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 02:24:40,878 INFO] number of examples: 100400\n","[2021-04-13 02:26:14,572 INFO] Validation perplexity: 17.3233\n","[2021-04-13 02:26:14,573 INFO] Validation accuracy: 49.9459\n","[2021-04-13 02:26:16,092 INFO] Saving checkpoint model/en-vi_step_16000.pt\n","[2021-04-13 02:27:23,866 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 02:27:46,968 INFO] number of examples: 802825\n","[2021-04-13 02:38:07,390 INFO] Step 17000/30000; acc:  64.41; ppl:  4.22; xent: 1.44; lr: 0.00068; 6917/8446 tok/s;  13766 sec\n","[2021-04-13 02:38:07,391 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 02:38:14,599 INFO] number of examples: 100400\n","[2021-04-13 02:39:48,279 INFO] Validation perplexity: 17.6902\n","[2021-04-13 02:39:48,279 INFO] Validation accuracy: 50.1207\n","[2021-04-13 02:39:49,791 INFO] Saving checkpoint model/en-vi_step_17000.pt\n","[2021-04-13 02:41:00,027 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 02:41:34,611 INFO] number of examples: 802825\n","[2021-04-13 02:51:53,028 INFO] Step 18000/30000; acc:  65.46; ppl:  4.02; xent: 1.39; lr: 0.00066; 6772/8266 tok/s;  14592 sec\n","[2021-04-13 02:51:53,031 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 02:51:54,598 INFO] number of examples: 100400\n","[2021-04-13 02:53:28,876 INFO] Validation perplexity: 18.2908\n","[2021-04-13 02:53:28,876 INFO] Validation accuracy: 50.1221\n","[2021-04-13 02:53:30,392 INFO] Saving checkpoint model/en-vi_step_18000.pt\n","[2021-04-13 02:54:44,954 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 02:55:12,247 INFO] number of examples: 802825\n","[2021-04-13 03:05:26,684 INFO] Step 19000/30000; acc:  66.42; ppl:  3.85; xent: 1.35; lr: 0.00064; 6872/8387 tok/s;  15406 sec\n","[2021-04-13 03:05:26,685 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 03:05:33,976 INFO] number of examples: 100400\n","[2021-04-13 03:07:08,517 INFO] Validation perplexity: 18.2557\n","[2021-04-13 03:07:08,517 INFO] Validation accuracy: 50.1966\n","[2021-04-13 03:07:10,059 INFO] Saving checkpoint model/en-vi_step_19000.pt\n","[2021-04-13 03:08:28,324 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 03:09:03,095 INFO] number of examples: 802825\n","[2021-04-13 03:19:14,062 INFO] Step 20000/30000; acc:  67.31; ppl:  3.69; xent: 1.31; lr: 0.00062; 6748/8253 tok/s;  16233 sec\n","[2021-04-13 03:19:14,064 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 03:19:15,644 INFO] number of examples: 100400\n","[2021-04-13 03:20:49,655 INFO] Validation perplexity: 18.2478\n","[2021-04-13 03:20:49,655 INFO] Validation accuracy: 50.114\n","[2021-04-13 03:20:51,136 INFO] Saving checkpoint model/en-vi_step_20000.pt\n","[2021-04-13 03:22:13,046 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 03:22:46,296 INFO] number of examples: 802825\n","[2021-04-13 03:32:53,155 INFO] Step 21000/30000; acc:  68.17; ppl:  3.55; xent: 1.27; lr: 0.00061; 6824/8330 tok/s;  17052 sec\n","[2021-04-13 03:32:53,156 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 03:32:54,729 INFO] number of examples: 100400\n","[2021-04-13 03:34:28,915 INFO] Validation perplexity: 18.8976\n","[2021-04-13 03:34:28,915 INFO] Validation accuracy: 50.2716\n","[2021-04-13 03:34:30,428 INFO] Saving checkpoint model/en-vi_step_21000.pt\n","[2021-04-13 03:35:56,633 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 03:36:19,721 INFO] number of examples: 802825\n","[2021-04-13 03:46:22,790 INFO] Step 22000/30000; acc:  68.94; ppl:  3.43; xent: 1.23; lr: 0.00060; 6903/8433 tok/s;  17862 sec\n","[2021-04-13 03:46:22,792 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 03:46:30,042 INFO] number of examples: 100400\n","[2021-04-13 03:48:03,973 INFO] Validation perplexity: 20.3064\n","[2021-04-13 03:48:03,973 INFO] Validation accuracy: 50.3759\n","[2021-04-13 03:48:05,441 INFO] Saving checkpoint model/en-vi_step_22000.pt\n","[2021-04-13 03:49:34,530 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 03:50:09,497 INFO] number of examples: 802825\n","[2021-04-13 04:00:08,418 INFO] Step 23000/30000; acc:  69.68; ppl:  3.32; xent: 1.20; lr: 0.00058; 6762/8265 tok/s;  18687 sec\n","[2021-04-13 04:00:08,420 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 04:00:09,965 INFO] number of examples: 100400\n","[2021-04-13 04:01:43,850 INFO] Validation perplexity: 20.8161\n","[2021-04-13 04:01:43,850 INFO] Validation accuracy: 50.2887\n","[2021-04-13 04:01:45,319 INFO] Saving checkpoint model/en-vi_step_23000.pt\n","[2021-04-13 04:03:17,856 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 04:03:45,231 INFO] number of examples: 802825\n","[2021-04-13 04:13:40,436 INFO] Step 24000/30000; acc:  70.40; ppl:  3.22; xent: 1.17; lr: 0.00057; 6880/8405 tok/s;  19499 sec\n","[2021-04-13 04:13:40,437 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 04:13:47,953 INFO] number of examples: 100400\n","[2021-04-13 04:15:21,633 INFO] Validation perplexity: 20.3918\n","[2021-04-13 04:15:21,633 INFO] Validation accuracy: 50.3558\n","[2021-04-13 04:15:23,095 INFO] Saving checkpoint model/en-vi_step_24000.pt\n","[2021-04-13 04:16:59,818 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 04:17:35,640 INFO] number of examples: 802825\n","[2021-04-13 04:27:26,738 INFO] Step 25000/30000; acc:  71.04; ppl:  3.13; xent: 1.14; lr: 0.00056; 6761/8260 tok/s;  20326 sec\n","[2021-04-13 04:27:26,739 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 04:27:28,266 INFO] number of examples: 100400\n","[2021-04-13 04:29:01,717 INFO] Validation perplexity: 20.2063\n","[2021-04-13 04:29:01,717 INFO] Validation accuracy: 50.5836\n","[2021-04-13 04:29:03,245 INFO] Saving checkpoint model/en-vi_step_25000.pt\n","[2021-04-13 04:30:44,931 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 04:31:13,757 INFO] number of examples: 802825\n","[2021-04-13 04:41:01,537 INFO] Step 26000/30000; acc:  71.66; ppl:  3.05; xent: 1.12; lr: 0.00055; 6854/8381 tok/s;  21141 sec\n","[2021-04-13 04:41:01,538 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 04:41:03,166 INFO] number of examples: 100400\n","[2021-04-13 04:42:36,975 INFO] Validation perplexity: 20.546\n","[2021-04-13 04:42:36,976 INFO] Validation accuracy: 50.5446\n","[2021-04-13 04:42:38,450 INFO] Saving checkpoint model/en-vi_step_26000.pt\n","[2021-04-13 04:44:22,499 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 04:44:51,230 INFO] number of examples: 802825\n","[2021-04-13 04:54:35,185 INFO] Step 27000/30000; acc:  72.26; ppl:  2.98; xent: 1.09; lr: 0.00054; 6868/8387 tok/s;  21954 sec\n","[2021-04-13 04:54:35,187 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 04:54:36,701 INFO] number of examples: 100400\n","[2021-04-13 04:56:10,564 INFO] Validation perplexity: 20.9004\n","[2021-04-13 04:56:10,564 INFO] Validation accuracy: 50.6231\n","[2021-04-13 04:56:12,009 INFO] Saving checkpoint model/en-vi_step_27000.pt\n","[2021-04-13 04:57:59,317 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 04:58:27,868 INFO] number of examples: 802825\n","[2021-04-13 05:08:07,935 INFO] Step 28000/30000; acc:  72.85; ppl:  2.91; xent: 1.07; lr: 0.00053; 6875/8397 tok/s;  22767 sec\n","[2021-04-13 05:08:07,936 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 05:08:09,579 INFO] number of examples: 100400\n","[2021-04-13 05:09:43,099 INFO] Validation perplexity: 21.5449\n","[2021-04-13 05:09:43,100 INFO] Validation accuracy: 50.5916\n","[2021-04-13 05:09:44,574 INFO] Saving checkpoint model/en-vi_step_28000.pt\n","[2021-04-13 05:11:35,973 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 05:12:04,773 INFO] number of examples: 802825\n","[2021-04-13 05:21:41,526 INFO] Step 29000/30000; acc:  73.38; ppl:  2.84; xent: 1.04; lr: 0.00052; 6868/8388 tok/s;  23581 sec\n","[2021-04-13 05:21:41,527 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 05:21:43,063 INFO] number of examples: 100400\n","[2021-04-13 05:23:17,021 INFO] Validation perplexity: 22.2358\n","[2021-04-13 05:23:17,021 INFO] Validation accuracy: 50.4962\n","[2021-04-13 05:23:18,529 INFO] Saving checkpoint model/en-vi_step_29000.pt\n","[2021-04-13 05:25:14,341 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-13 05:25:43,133 INFO] number of examples: 802825\n","[2021-04-13 05:35:16,924 INFO] Step 30000/30000; acc:  73.91; ppl:  2.78; xent: 1.02; lr: 0.00051; 6850/8370 tok/s;  24396 sec\n","[2021-04-13 05:35:16,925 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-13 05:35:18,510 INFO] number of examples: 100400\n","[2021-04-13 05:36:52,599 INFO] Validation perplexity: 22.6086\n","[2021-04-13 05:36:52,600 INFO] Validation accuracy: 50.5648\n","[2021-04-13 05:36:54,089 INFO] Saving checkpoint model/en-vi_step_30000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XOEz-j6IbNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618292226396,"user_tz":-420,"elapsed":26,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"d0b85ead-3de8-45f2-cc96-8d1cfef1ba53"},"source":["!ls -al model model/"],"execution_count":8,"outputs":[{"output_type":"stream","text":["model:\n","total 43401240\n","-rw------- 1 root root 1481428927 Apr 13 01:04 en-vi_step_10000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:02 en-vi_step_1000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:18 en-vi_step_11000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:31 en-vi_step_12000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:45 en-vi_step_13000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:59 en-vi_step_14000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:12 en-vi_step_15000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:26 en-vi_step_16000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:39 en-vi_step_17000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:53 en-vi_step_18000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:07 en-vi_step_19000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:20 en-vi_step_20000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:15 en-vi_step_2000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:34 en-vi_step_21000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:48 en-vi_step_22000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:01 en-vi_step_23000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:15 en-vi_step_24000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:29 en-vi_step_25000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:42 en-vi_step_26000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:56 en-vi_step_27000.pt\n","-rw------- 1 root root 1481428927 Apr 13 05:09 en-vi_step_28000.pt\n","-rw------- 1 root root 1481428927 Apr 13 05:23 en-vi_step_29000.pt\n","-rw------- 1 root root 1481428927 Apr 13 05:37 en-vi_step_30000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:29 en-vi_step_3000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:42 en-vi_step_4000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:56 en-vi_step_5000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:10 en-vi_step_6000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:23 en-vi_step_7000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:37 en-vi_step_8000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:50 en-vi_step_9000.pt\n","\n","model/:\n","total 43401240\n","-rw------- 1 root root 1481428927 Apr 13 01:04 en-vi_step_10000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:02 en-vi_step_1000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:18 en-vi_step_11000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:31 en-vi_step_12000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:45 en-vi_step_13000.pt\n","-rw------- 1 root root 1481428927 Apr 13 01:59 en-vi_step_14000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:12 en-vi_step_15000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:26 en-vi_step_16000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:39 en-vi_step_17000.pt\n","-rw------- 1 root root 1481428927 Apr 13 02:53 en-vi_step_18000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:07 en-vi_step_19000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:20 en-vi_step_20000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:15 en-vi_step_2000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:34 en-vi_step_21000.pt\n","-rw------- 1 root root 1481428927 Apr 13 03:48 en-vi_step_22000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:01 en-vi_step_23000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:15 en-vi_step_24000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:29 en-vi_step_25000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:42 en-vi_step_26000.pt\n","-rw------- 1 root root 1481428927 Apr 13 04:56 en-vi_step_27000.pt\n","-rw------- 1 root root 1481428927 Apr 13 05:09 en-vi_step_28000.pt\n","-rw------- 1 root root 1481428927 Apr 13 05:23 en-vi_step_29000.pt\n","-rw------- 1 root root 1481428927 Apr 13 05:37 en-vi_step_30000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:29 en-vi_step_3000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:42 en-vi_step_4000.pt\n","-rw------- 1 root root 1481428927 Apr 12 23:56 en-vi_step_5000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:10 en-vi_step_6000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:23 en-vi_step_7000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:37 en-vi_step_8000.pt\n","-rw------- 1 root root 1481428927 Apr 13 00:50 en-vi_step_9000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1S26AN4rHUAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618303675748,"user_tz":-420,"elapsed":11449369,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2c753c70-3a8b-467d-a13f-98b6e945be62"},"source":["!onmt_translate -model model/en-vi_step_30000.pt -src en_test -tgt vi_test -output predict.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-13 05:37:14,673 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [10, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-13 05:55:57,580 INFO] PRED AVG SCORE: -0.6147, PRED PPL: 1.8490\n","[2021-04-13 05:55:57,581 INFO] GOLD AVG SCORE: -3.1167, GOLD PPL: 22.5717\n","[2021-04-13 05:55:57,604 INFO] Translating shard 1.\n","tcmalloc: large alloc 1518125056 bytes == 0x5654fe35a000 @  0x7fe3a6e7fb6b 0x7fe3a6e9f379 0x7fe3535d825e 0x7fe3535d99d2 0x7fe3902bc8e6 0x7fe39071edd9 0x7fe390c2977a 0x7fe390bf4ef9 0x7fe390bab657 0x7fe390a4f929 0x7fe390567516 0x7fe390c2a7af 0x7fe3909d9846 0x7fe3909dee6f 0x7fe3922c2bcc 0x7fe3922c313f 0x7fe390e2ba86 0x7fe390e2fcaf 0x7fe39055916a 0x7fe390559b3a 0x7fe390d3e7f8 0x7fe390d3e83f 0x7fe3909d9846 0x7fe3909df22f 0x7fe39053d0b1 0x7fe390d3d4c0 0x7fe390d6005d 0x7fe390b30a59 0x7fe3a204e8de 0x565482007050 0x565482006de0\n","tcmalloc: large alloc 1518125056 bytes == 0x565558b26000 @  0x7fe3a6e7fb6b 0x7fe3a6e9f379 0x7fe3535d825e 0x7fe3535d99d2 0x7fe3902bc8e6 0x7fe39071edd9 0x7fe390c2977a 0x7fe390bf4ef9 0x7fe390bab657 0x7fe390a4f929 0x7fe3907286a2 0x7fe3906b85c5 0x7fe390c2a573 0x7fe390b9a904 0x7fe390a04f09 0x7fe39224c444 0x7fe39224c783 0x7fe390b9a904 0x7fe390a04f09 0x7fe3906b15d0 0x7fe390d3e0e0 0x7fe390d3e132 0x7fe390ba8054 0x7fe390e4c735 0x7fe3a1dabf4f 0x565482007050 0x5654820f899d 0x56548207afe9 0x565482075b0e 0x56548200877a 0x56548207786a\n","[2021-04-13 06:14:52,986 INFO] PRED AVG SCORE: -0.6196, PRED PPL: 1.8581\n","[2021-04-13 06:14:52,987 INFO] GOLD AVG SCORE: -3.1242, GOLD PPL: 22.7415\n","[2021-04-13 06:14:53,007 INFO] Translating shard 2.\n","[2021-04-13 06:34:17,572 INFO] PRED AVG SCORE: -0.6128, PRED PPL: 1.8456\n","[2021-04-13 06:34:17,573 INFO] GOLD AVG SCORE: -3.1115, GOLD PPL: 22.4543\n","[2021-04-13 06:34:17,593 INFO] Translating shard 3.\n","[2021-04-13 06:53:20,191 INFO] PRED AVG SCORE: -0.6122, PRED PPL: 1.8445\n","[2021-04-13 06:53:20,191 INFO] GOLD AVG SCORE: -3.0952, GOLD PPL: 22.0911\n","[2021-04-13 06:53:20,213 INFO] Translating shard 4.\n","[2021-04-13 07:12:33,237 INFO] PRED AVG SCORE: -0.6148, PRED PPL: 1.8492\n","[2021-04-13 07:12:33,237 INFO] GOLD AVG SCORE: -3.1239, GOLD PPL: 22.7347\n","[2021-04-13 07:12:33,259 INFO] Translating shard 5.\n","[2021-04-13 07:31:15,841 INFO] PRED AVG SCORE: -0.6156, PRED PPL: 1.8508\n","[2021-04-13 07:31:15,841 INFO] GOLD AVG SCORE: -3.1056, GOLD PPL: 22.3232\n","[2021-04-13 07:31:15,862 INFO] Translating shard 6.\n","[2021-04-13 07:49:45,306 INFO] PRED AVG SCORE: -0.6142, PRED PPL: 1.8482\n","[2021-04-13 07:49:45,306 INFO] GOLD AVG SCORE: -3.0953, GOLD PPL: 22.0947\n","[2021-04-13 07:49:45,327 INFO] Translating shard 7.\n","[2021-04-13 08:08:40,254 INFO] PRED AVG SCORE: -0.6136, PRED PPL: 1.8470\n","[2021-04-13 08:08:40,254 INFO] GOLD AVG SCORE: -3.0799, GOLD PPL: 21.7573\n","[2021-04-13 08:08:40,276 INFO] Translating shard 8.\n","[2021-04-13 08:27:40,437 INFO] PRED AVG SCORE: -0.6171, PRED PPL: 1.8536\n","[2021-04-13 08:27:40,437 INFO] GOLD AVG SCORE: -3.1224, GOLD PPL: 22.7018\n","[2021-04-13 08:27:40,458 INFO] Translating shard 9.\n","[2021-04-13 08:47:06,277 INFO] PRED AVG SCORE: -0.6172, PRED PPL: 1.8538\n","[2021-04-13 08:47:06,277 INFO] GOLD AVG SCORE: -3.0942, GOLD PPL: 22.0700\n","[2021-04-13 08:47:06,297 INFO] Translating shard 10.\n","[2021-04-13 08:47:53,911 INFO] PRED AVG SCORE: -0.6278, PRED PPL: 1.8736\n","[2021-04-13 08:47:53,911 INFO] GOLD AVG SCORE: -2.9467, GOLD PPL: 19.0437\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPGsYFs_XpAO"},"source":["!tail vi_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rf5W-T8MzRK3"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw_Gma2gz0PK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618303698011,"user_tz":-420,"elapsed":13,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"81b750cb-32c5-42e7-dee7-4854d8d53913"},"source":["!ls -al"],"execution_count":12,"outputs":[{"output_type":"stream","text":["total 355330\n","drwx------  2 root root     4096 Apr 12 22:47 data_bin\n","-rw-------  1 root root  3318349 Apr 12 02:53 en_test\n","-rw-------  1 root root 26563375 Apr 12 02:53 en_train\n","-rw-------  1 root root 31608088 Apr 12 03:46 en_train_EM_0.8\n","-rw-------  1 root root 29845474 Apr 12 03:46 en_train_EM_0.85\n","-rw-------  1 root root 28377219 Apr 12 03:46 en_train_EM_0.9\n","-rw-------  1 root root 27245135 Apr 12 03:46 en_train_EM_0.95\n","-rw-------  1 root root 11930822 Apr 12 03:46 en_train_EM_factor_0.8\n","-rw-------  1 root root 11270762 Apr 12 03:46 en_train_EM_factor_0.85\n","-rw-------  1 root root 10719696 Apr 12 03:46 en_train_EM_factor_0.9\n","-rw-------  1 root root 10293086 Apr 12 03:46 en_train_EM_factor_0.95\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.8\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.85\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.9\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.95\n","-rw-------  1 root root  3328557 Apr 12 02:53 en_valid\n","drwx------  2 root root     4096 Apr 13 05:37 model\n","drwx------ 11 root root     4096 Apr 13 08:48 OpenNMT-py\n","-rw-------  1 root root 94255466 Apr 12 22:47 opus_sent2vec.tar.gz\n","drwx------  2 root root     4096 Apr 12 22:48 output\n","-rw-------  1 root root  3722244 Apr 13 08:47 predict.txt\n","-rw-------  1 root root  4365722 Apr 12 02:53 vi_test\n","-rw-------  1 root root 35019161 Apr 12 02:53 vi_train\n","-rw-------  1 root root  4382771 Apr 12 02:53 vi_valid\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7T7xCaDdR469","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618306895895,"user_tz":-420,"elapsed":11784,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"5e208b46-bc87-49f0-c03c-894791d1fa6e"},"source":["!perl OpenNMT-py/tools/multi-bleu.perl vi_test < predict.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["BLEU = 16.49, 42.1/23.5/14.4/9.7 (BP=0.855, ratio=0.865, hyp_len=655694, ref_len=758454)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uLGJCSK_Qih1","executionInfo":{"status":"ok","timestamp":1618303706471,"user_tz":-420,"elapsed":21,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}}},"source":[""],"execution_count":13,"outputs":[]}]}