{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OPUS-OpenNMT-20210412-0158 BLEU 17.36.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618266890252,"user_tz":-420,"elapsed":26244,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"54da9541-2f19-49ce-bad1-fbac8d2e9f18"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618266896205,"user_tz":-420,"elapsed":4123,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"c49aa64b-8265-40aa-fecc-7998f3d7087a"},"source":["# import os\n","# path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"OPUS-OpenNMT-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-20210412-0158'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-20210412-0158\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618266899355,"user_tz":-420,"elapsed":912,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ff51021e-a4af-4540-a07b-2cffe6fd8886"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mon Apr 12 22:34:58 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618266918782,"user_tz":-420,"elapsed":16554,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ed487f1a-1387-4004-c122-514fefc00160"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 8.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n","\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/63/17c6ac0d8a0cfa5ff7257e52edb6759d12dc266392f6c97f5c65c0c7238c/pyonmttok-1.25.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 6.6MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.8.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=e3857eaed9bff25b9b3cf179e038a312e2d383825be3a4429f8f5f8325574d33\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: configargparse, pyonmttok, waitress, torchtext, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.25.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618192737819,"user_tz":-420,"elapsed":43905,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"5bc0434a-80da-438f-99a5-df3821f0ddef"},"source":["!wget https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_bert.tar.gz\n","!mkdir data_bin\n","!tar -xvf 'opus_bert.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-12 01:58:49--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_bert.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 100661014 (96M) [application/octet-stream]\n","Saving to: ‘opus_bert.tar.gz’\n","\n","opus_bert.tar.gz    100%[===================>]  96.00M  87.0MB/s    in 1.1s    \n","\n","2021-04-12 01:58:53 (87.0 MB/s) - ‘opus_bert.tar.gz’ saved [100661014/100661014]\n","\n","en_train_EM_score_0.95\n","vi_valid\n","en_train_EM_0.95\n","en_train_EM_factor_0.85\n","en_train_EM_score_0.8\n","vi_train\n","en_train_EM_factor_0.8\n","en_train_EM_0.8\n","en_valid\n","en_train_EM_factor_0.95\n","en_train\n","en_train_EM_score_0.85\n","vi_test\n","en_train_EM_0.85\n","en_train_EM_score_0.9\n","en_test\n","en_train_EM_factor_0.9\n","en_train_EM_0.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618192789194,"user_tz":-420,"elapsed":95278,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"db99cc49-fd5a-4c0e-d226-eaf22ee7fdd4"},"source":["!mkdir -p output\n","!onmt_preprocess -train_src 'en_train' \\\\\n","-train_tgt 'vi_train' \\\\\n","-valid_src 'en_valid' \\\\\n","-valid_tgt 'vi_valid' \\\\\n","-save_data 'output/en-vi' "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-12 01:59:00,750 INFO] Extracting features...\n","[2021-04-12 01:59:00,755 INFO]  * number of source features: 0.\n","[2021-04-12 01:59:00,755 INFO]  * number of target features: 0.\n","[2021-04-12 01:59:00,755 INFO] Building `Fields` object...\n","[2021-04-12 01:59:00,755 INFO] Building & saving training data...\n","[2021-04-12 01:59:01,823 INFO] Building shard 0.\n","[2021-04-12 01:59:27,926 INFO]  * saving 0th train data shard to output/en-vi.train.0.pt.\n","[2021-04-12 01:59:42,926 INFO]  * tgt vocab size: 50004.\n","[2021-04-12 01:59:43,236 INFO]  * src vocab size: 50002.\n","[2021-04-12 01:59:43,794 INFO] Building & saving validation data...\n","[2021-04-12 01:59:44,301 INFO] Building shard 0.\n","[2021-04-12 01:59:46,215 INFO]  * saving 0th valid data shard to output/en-vi.valid.0.pt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYyQECmi0TX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618217165469,"user_tz":-420,"elapsed":4940867,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"9b5c095d-0969-45cf-8a8b-7215cf274055"},"source":["!mkdir -p model\n","!onmt_train -data 'output/en-vi' \\\\\n","-save_model 'model/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30000  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 1000 -save_checkpoint_steps 1000 \\\\\n","-report_every 1000 -world_size 1 -gpu_ranks 0"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-12 01:59:51,696 INFO]  * src vocab size = 50002\n","[2021-04-12 01:59:51,696 INFO]  * tgt vocab size = 50004\n","[2021-04-12 01:59:51,696 INFO] Building model...\n","[2021-04-12 01:59:59,154 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50002, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50004, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=50004, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-12 01:59:59,233 INFO] encoder: 44516352\n","[2021-04-12 01:59:59,233 INFO] decoder: 76479316\n","[2021-04-12 01:59:59,233 INFO] * number of parameters: 120995668\n","[2021-04-12 01:59:59,237 INFO] Starting training on GPU: [0]\n","[2021-04-12 01:59:59,237 INFO] Start training loop and validate every 1000 steps...\n","[2021-04-12 01:59:59,237 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 02:00:14,523 INFO] number of examples: 802833\n","[2021-04-12 02:11:03,049 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 02:11:22,088 INFO] number of examples: 802833\n","[2021-04-12 02:12:09,803 INFO] Step 1000/30000; acc:  13.58; ppl: 553.53; xent: 6.32; lr: 0.00012; 7286/9996 tok/s;    731 sec\n","[2021-04-12 02:12:09,804 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 02:12:11,035 INFO] number of examples: 100400\n","[2021-04-12 02:13:36,136 INFO] Validation perplexity: 269.729\n","[2021-04-12 02:13:36,136 INFO] Validation accuracy: 19.196\n","[2021-04-12 02:13:37,463 INFO] Saving checkpoint model/en-vi_step_1000.pt\n","[2021-04-12 02:23:50,265 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 02:24:18,465 INFO] number of examples: 802833\n","[2021-04-12 02:25:48,242 INFO] Step 2000/30000; acc:  27.64; ppl: 68.77; xent: 4.23; lr: 0.00025; 6509/8935 tok/s;   1549 sec\n","[2021-04-12 02:25:48,243 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 02:25:49,486 INFO] number of examples: 100400\n","[2021-04-12 02:27:14,265 INFO] Validation perplexity: 54.7948\n","[2021-04-12 02:27:14,265 INFO] Validation accuracy: 36.1706\n","[2021-04-12 02:27:15,632 INFO] Saving checkpoint model/en-vi_step_2000.pt\n","[2021-04-12 02:36:41,392 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 02:37:03,249 INFO] number of examples: 802833\n","[2021-04-12 02:39:13,845 INFO] Step 3000/30000; acc:  39.05; ppl: 25.59; xent: 3.24; lr: 0.00037; 6606/9071 tok/s;   2355 sec\n","[2021-04-12 02:39:13,846 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 02:39:15,103 INFO] number of examples: 100400\n","[2021-04-12 02:40:39,998 INFO] Validation perplexity: 26.9384\n","[2021-04-12 02:40:39,998 INFO] Validation accuracy: 43.0094\n","[2021-04-12 02:40:41,307 INFO] Saving checkpoint model/en-vi_step_3000.pt\n","[2021-04-12 02:49:26,570 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 02:49:48,842 INFO] number of examples: 802833\n","[2021-04-12 02:52:39,658 INFO] Step 4000/30000; acc:  44.18; ppl: 16.08; xent: 2.78; lr: 0.00049; 6598/9063 tok/s;   3160 sec\n","[2021-04-12 02:52:39,659 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 02:52:40,937 INFO] number of examples: 100400\n","[2021-04-12 02:54:05,729 INFO] Validation perplexity: 21.0854\n","[2021-04-12 02:54:05,730 INFO] Validation accuracy: 45.5665\n","[2021-04-12 02:54:07,064 INFO] Saving checkpoint model/en-vi_step_4000.pt\n","[2021-04-12 03:02:12,030 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 03:02:34,776 INFO] number of examples: 802833\n","[2021-04-12 03:06:07,596 INFO] Step 5000/30000; acc:  47.01; ppl: 12.53; xent: 2.53; lr: 0.00062; 6591/9040 tok/s;   3968 sec\n","[2021-04-12 03:06:07,597 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 03:06:08,896 INFO] number of examples: 100400\n","[2021-04-12 03:07:33,674 INFO] Validation perplexity: 18.4028\n","[2021-04-12 03:07:33,674 INFO] Validation accuracy: 47.023\n","[2021-04-12 03:07:34,980 INFO] Saving checkpoint model/en-vi_step_5000.pt\n","[2021-04-12 03:14:58,111 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 03:15:20,275 INFO] number of examples: 802833\n","[2021-04-12 03:19:34,347 INFO] Step 6000/30000; acc:  48.94; ppl: 10.64; xent: 2.36; lr: 0.00074; 6604/9065 tok/s;   4775 sec\n","[2021-04-12 03:19:34,348 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 03:19:39,373 INFO] number of examples: 100400\n","[2021-04-12 03:21:04,171 INFO] Validation perplexity: 17.0884\n","[2021-04-12 03:21:04,171 INFO] Validation accuracy: 47.958\n","[2021-04-12 03:21:05,473 INFO] Saving checkpoint model/en-vi_step_6000.pt\n","[2021-04-12 03:27:46,887 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 03:28:10,217 INFO] number of examples: 802833\n","[2021-04-12 03:33:06,185 INFO] Step 7000/30000; acc:  50.35; ppl:  9.48; xent: 2.25; lr: 0.00086; 6583/9011 tok/s;   5587 sec\n","[2021-04-12 03:33:06,186 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 03:33:07,495 INFO] number of examples: 100400\n","[2021-04-12 03:34:32,207 INFO] Validation perplexity: 16.5916\n","[2021-04-12 03:34:32,208 INFO] Validation accuracy: 48.0617\n","[2021-04-12 03:34:33,502 INFO] Saving checkpoint model/en-vi_step_7000.pt\n","[2021-04-12 03:40:33,133 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 03:40:56,215 INFO] number of examples: 802833\n","[2021-04-12 03:46:33,511 INFO] Step 8000/30000; acc:  51.66; ppl:  8.60; xent: 2.15; lr: 0.00099; 6597/9043 tok/s;   6394 sec\n","[2021-04-12 03:46:33,512 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 03:46:34,825 INFO] number of examples: 100400\n","[2021-04-12 03:47:59,567 INFO] Validation perplexity: 16.1685\n","[2021-04-12 03:47:59,567 INFO] Validation accuracy: 48.6401\n","[2021-04-12 03:48:00,862 INFO] Saving checkpoint model/en-vi_step_8000.pt\n","[2021-04-12 03:53:19,160 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 03:53:41,840 INFO] number of examples: 802833\n","[2021-04-12 04:00:00,715 INFO] Step 9000/30000; acc:  53.22; ppl:  7.73; xent: 2.05; lr: 0.00093; 6607/9057 tok/s;   7201 sec\n","[2021-04-12 04:00:00,716 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:00:07,567 INFO] number of examples: 100400\n","[2021-04-12 04:01:32,357 INFO] Validation perplexity: 15.6948\n","[2021-04-12 04:01:32,357 INFO] Validation accuracy: 49.1841\n","[2021-04-12 04:01:33,654 INFO] Saving checkpoint model/en-vi_step_9000.pt\n","[2021-04-12 04:06:09,687 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:06:39,640 INFO] number of examples: 802833\n","[2021-04-12 04:13:39,396 INFO] Step 10000/30000; acc:  55.15; ppl:  6.86; xent: 1.93; lr: 0.00088; 6499/8917 tok/s;   8020 sec\n","[2021-04-12 04:13:39,396 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:13:40,725 INFO] number of examples: 100400\n","[2021-04-12 04:15:05,475 INFO] Validation perplexity: 16.1222\n","[2021-04-12 04:15:05,475 INFO] Validation accuracy: 49.1211\n","[2021-04-12 04:15:06,784 INFO] Saving checkpoint model/en-vi_step_10000.pt\n","[2021-04-12 04:19:02,228 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:19:25,815 INFO] number of examples: 802833\n","[2021-04-12 04:27:06,725 INFO] Step 11000/30000; acc:  57.12; ppl:  6.13; xent: 1.81; lr: 0.00084; 6586/9063 tok/s;   8827 sec\n","[2021-04-12 04:27:06,726 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:27:08,051 INFO] number of examples: 100400\n","[2021-04-12 04:28:32,808 INFO] Validation perplexity: 15.5095\n","[2021-04-12 04:28:32,808 INFO] Validation accuracy: 49.7604\n","[2021-04-12 04:28:34,122 INFO] Saving checkpoint model/en-vi_step_11000.pt\n","[2021-04-12 04:31:49,008 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:32:13,015 INFO] number of examples: 802833\n","[2021-04-12 04:40:35,220 INFO] Step 12000/30000; acc:  58.87; ppl:  5.58; xent: 1.72; lr: 0.00081; 6590/9035 tok/s;   9636 sec\n","[2021-04-12 04:40:35,221 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:40:36,550 INFO] number of examples: 100400\n","[2021-04-12 04:42:01,341 INFO] Validation perplexity: 16.653\n","[2021-04-12 04:42:01,341 INFO] Validation accuracy: 49.5145\n","[2021-04-12 04:42:02,647 INFO] Saving checkpoint model/en-vi_step_12000.pt\n","[2021-04-12 04:44:37,437 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:45:05,117 INFO] number of examples: 802833\n","[2021-04-12 04:54:09,266 INFO] Step 13000/30000; acc:  60.51; ppl:  5.12; xent: 1.63; lr: 0.00078; 6534/8959 tok/s;  10450 sec\n","[2021-04-12 04:54:09,267 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:54:10,597 INFO] number of examples: 100400\n","[2021-04-12 04:55:35,368 INFO] Validation perplexity: 16.426\n","[2021-04-12 04:55:35,368 INFO] Validation accuracy: 50.1523\n","[2021-04-12 04:55:36,735 INFO] Saving checkpoint model/en-vi_step_13000.pt\n","[2021-04-12 04:57:28,468 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:57:53,371 INFO] number of examples: 802833\n","[2021-04-12 05:07:38,892 INFO] Step 14000/30000; acc:  61.85; ppl:  4.77; xent: 1.56; lr: 0.00075; 6557/9003 tok/s;  11260 sec\n","[2021-04-12 05:07:38,893 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:07:40,228 INFO] number of examples: 100400\n","[2021-04-12 05:09:05,026 INFO] Validation perplexity: 16.5137\n","[2021-04-12 05:09:05,026 INFO] Validation accuracy: 50.2629\n","[2021-04-12 05:09:06,329 INFO] Saving checkpoint model/en-vi_step_14000.pt\n","[2021-04-12 05:10:15,327 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:10:34,699 INFO] number of examples: 802833\n","[2021-04-12 05:21:02,451 INFO] Step 15000/30000; acc:  63.23; ppl:  4.46; xent: 1.50; lr: 0.00072; 6594/9061 tok/s;  12063 sec\n","[2021-04-12 05:21:02,452 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:21:09,155 INFO] number of examples: 100400\n","[2021-04-12 05:22:33,899 INFO] Validation perplexity: 17.5452\n","[2021-04-12 05:22:33,900 INFO] Validation accuracy: 49.9036\n","[2021-04-12 05:22:35,204 INFO] Saving checkpoint model/en-vi_step_15000.pt\n","[2021-04-12 05:23:03,176 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:23:32,887 INFO] number of examples: 802833\n","[2021-04-12 05:34:22,165 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:34:46,134 INFO] number of examples: 802833\n","[2021-04-12 05:35:12,390 INFO] Step 16000/30000; acc:  64.40; ppl:  4.21; xent: 1.44; lr: 0.00070; 6258/8579 tok/s;  12913 sec\n","[2021-04-12 05:35:12,391 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:35:13,722 INFO] number of examples: 100400\n","[2021-04-12 05:36:38,480 INFO] Validation perplexity: 17.2597\n","[2021-04-12 05:36:38,480 INFO] Validation accuracy: 50.2689\n","[2021-04-12 05:36:39,801 INFO] Saving checkpoint model/en-vi_step_16000.pt\n","[2021-04-12 05:47:10,368 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:47:34,101 INFO] number of examples: 802833\n","[2021-04-12 05:48:41,689 INFO] Step 17000/30000; acc:  65.46; ppl:  4.01; xent: 1.39; lr: 0.00068; 6581/9029 tok/s;  13722 sec\n","[2021-04-12 05:48:41,690 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:48:43,041 INFO] number of examples: 100400\n","[2021-04-12 05:50:08,042 INFO] Validation perplexity: 17.4614\n","[2021-04-12 05:50:08,042 INFO] Validation accuracy: 50.524\n","[2021-04-12 05:50:09,412 INFO] Saving checkpoint model/en-vi_step_17000.pt\n","[2021-04-12 05:59:58,060 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:00:26,470 INFO] number of examples: 802833\n","[2021-04-12 06:02:15,578 INFO] Step 18000/30000; acc:  66.55; ppl:  3.81; xent: 1.34; lr: 0.00066; 6547/8982 tok/s;  14536 sec\n","[2021-04-12 06:02:15,579 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:02:16,908 INFO] number of examples: 100400\n","[2021-04-12 06:03:41,692 INFO] Validation perplexity: 18.1606\n","[2021-04-12 06:03:41,692 INFO] Validation accuracy: 50.3088\n","[2021-04-12 06:03:43,007 INFO] Saving checkpoint model/en-vi_step_18000.pt\n","[2021-04-12 06:12:50,367 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:13:09,954 INFO] number of examples: 802833\n","[2021-04-12 06:15:40,152 INFO] Step 19000/30000; acc:  67.43; ppl:  3.66; xent: 1.30; lr: 0.00064; 6609/9087 tok/s;  15341 sec\n","[2021-04-12 06:15:40,153 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:15:47,057 INFO] number of examples: 100400\n","[2021-04-12 06:17:11,895 INFO] Validation perplexity: 18.958\n","[2021-04-12 06:17:11,896 INFO] Validation accuracy: 50.3638\n","[2021-04-12 06:17:13,202 INFO] Saving checkpoint model/en-vi_step_19000.pt\n","[2021-04-12 06:25:38,736 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:26:08,311 INFO] number of examples: 802833\n","[2021-04-12 06:29:19,661 INFO] Step 20000/30000; acc:  68.36; ppl:  3.51; xent: 1.26; lr: 0.00062; 6497/8908 tok/s;  16160 sec\n","[2021-04-12 06:29:19,662 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:29:21,019 INFO] number of examples: 100400\n","[2021-04-12 06:30:45,828 INFO] Validation perplexity: 18.5602\n","[2021-04-12 06:30:45,828 INFO] Validation accuracy: 50.6056\n","[2021-04-12 06:30:47,136 INFO] Saving checkpoint model/en-vi_step_20000.pt\n","[2021-04-12 06:38:37,487 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:39:07,680 INFO] number of examples: 802833\n","[2021-04-12 06:43:00,435 INFO] Step 21000/30000; acc:  69.17; ppl:  3.38; xent: 1.22; lr: 0.00061; 6494/8906 tok/s;  16981 sec\n","[2021-04-12 06:43:00,436 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:43:01,776 INFO] number of examples: 100400\n","[2021-04-12 06:44:26,588 INFO] Validation perplexity: 18.9298\n","[2021-04-12 06:44:26,589 INFO] Validation accuracy: 50.7245\n","[2021-04-12 06:44:27,900 INFO] Saving checkpoint model/en-vi_step_21000.pt\n","[2021-04-12 06:51:32,080 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:51:56,648 INFO] number of examples: 802833\n","[2021-04-12 06:56:30,732 INFO] Step 22000/30000; acc:  70.01; ppl:  3.27; xent: 1.18; lr: 0.00060; 6574/9024 tok/s;  17791 sec\n","[2021-04-12 06:56:30,733 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:56:32,063 INFO] number of examples: 100400\n","[2021-04-12 06:57:56,851 INFO] Validation perplexity: 19.9854\n","[2021-04-12 06:57:56,851 INFO] Validation accuracy: 50.7486\n","[2021-04-12 06:57:58,147 INFO] Saving checkpoint model/en-vi_step_22000.pt\n","[2021-04-12 07:04:20,123 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:04:44,408 INFO] number of examples: 802833\n","[2021-04-12 07:10:00,131 INFO] Step 23000/30000; acc:  70.75; ppl:  3.16; xent: 1.15; lr: 0.00058; 6604/9033 tok/s;  18601 sec\n","[2021-04-12 07:10:00,132 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:10:01,482 INFO] number of examples: 100400\n","[2021-04-12 07:11:26,211 INFO] Validation perplexity: 20.2431\n","[2021-04-12 07:11:26,211 INFO] Validation accuracy: 50.6902\n","[2021-04-12 07:11:27,509 INFO] Saving checkpoint model/en-vi_step_23000.pt\n","[2021-04-12 07:17:07,363 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:17:31,782 INFO] number of examples: 802833\n","[2021-04-12 07:23:29,002 INFO] Step 24000/30000; acc:  71.41; ppl:  3.08; xent: 1.12; lr: 0.00057; 6578/9024 tok/s;  19410 sec\n","[2021-04-12 07:23:29,003 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:23:30,320 INFO] number of examples: 100400\n","[2021-04-12 07:24:55,105 INFO] Validation perplexity: 20.4648\n","[2021-04-12 07:24:55,105 INFO] Validation accuracy: 50.8384\n","[2021-04-12 07:24:56,415 INFO] Saving checkpoint model/en-vi_step_24000.pt\n","[2021-04-12 07:29:54,649 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:30:18,737 INFO] number of examples: 802833\n","[2021-04-12 07:36:56,675 INFO] Step 25000/30000; acc:  72.10; ppl:  2.99; xent: 1.09; lr: 0.00056; 6597/9048 tok/s;  20217 sec\n","[2021-04-12 07:36:56,676 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:36:58,013 INFO] number of examples: 100400\n","[2021-04-12 07:38:22,717 INFO] Validation perplexity: 20.8902\n","[2021-04-12 07:38:22,717 INFO] Validation accuracy: 50.5679\n","[2021-04-12 07:38:24,028 INFO] Saving checkpoint model/en-vi_step_25000.pt\n","[2021-04-12 07:42:42,648 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:43:06,682 INFO] number of examples: 802833\n","[2021-04-12 07:50:25,521 INFO] Step 26000/30000; acc:  72.69; ppl:  2.91; xent: 1.07; lr: 0.00055; 6568/9030 tok/s;  21026 sec\n","[2021-04-12 07:50:25,522 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:50:26,872 INFO] number of examples: 100400\n","[2021-04-12 07:51:51,611 INFO] Validation perplexity: 20.8643\n","[2021-04-12 07:51:51,611 INFO] Validation accuracy: 50.6498\n","[2021-04-12 07:51:52,913 INFO] Saving checkpoint model/en-vi_step_26000.pt\n","[2021-04-12 07:55:29,297 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:55:52,860 INFO] number of examples: 802833\n","[2021-04-12 08:03:52,362 INFO] Step 27000/30000; acc:  73.29; ppl:  2.84; xent: 1.04; lr: 0.00054; 6599/9059 tok/s;  21833 sec\n","[2021-04-12 08:03:52,363 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:03:53,688 INFO] number of examples: 100400\n","[2021-04-12 08:05:18,454 INFO] Validation perplexity: 22.0661\n","[2021-04-12 08:05:18,454 INFO] Validation accuracy: 50.6029\n","[2021-04-12 08:05:19,751 INFO] Saving checkpoint model/en-vi_step_27000.pt\n","[2021-04-12 08:08:15,115 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:08:38,104 INFO] number of examples: 802833\n","[2021-04-12 08:17:20,808 INFO] Step 28000/30000; acc:  73.89; ppl:  2.77; xent: 1.02; lr: 0.00053; 6594/9045 tok/s;  22642 sec\n","[2021-04-12 08:17:20,809 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:17:27,620 INFO] number of examples: 100400\n","[2021-04-12 08:18:52,360 INFO] Validation perplexity: 21.7314\n","[2021-04-12 08:18:52,360 INFO] Validation accuracy: 50.8583\n","[2021-04-12 08:18:53,660 INFO] Saving checkpoint model/en-vi_step_28000.pt\n","[2021-04-12 08:21:06,250 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:21:36,475 INFO] number of examples: 802833\n","[2021-04-12 08:31:00,217 INFO] Step 29000/30000; acc:  74.44; ppl:  2.72; xent: 1.00; lr: 0.00052; 6478/8891 tok/s;  23461 sec\n","[2021-04-12 08:31:00,218 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:31:01,553 INFO] number of examples: 100400\n","[2021-04-12 08:32:26,283 INFO] Validation perplexity: 22.6752\n","[2021-04-12 08:32:26,283 INFO] Validation accuracy: 50.7246\n","[2021-04-12 08:32:27,579 INFO] Saving checkpoint model/en-vi_step_29000.pt\n","[2021-04-12 08:33:59,348 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:34:23,221 INFO] number of examples: 802833\n","[2021-04-12 08:44:28,530 INFO] Step 30000/30000; acc:  75.02; ppl:  2.66; xent: 0.98; lr: 0.00051; 6564/9008 tok/s;  24269 sec\n","[2021-04-12 08:44:28,531 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:44:29,860 INFO] number of examples: 100400\n","[2021-04-12 08:45:54,614 INFO] Validation perplexity: 23.8749\n","[2021-04-12 08:45:54,614 INFO] Validation accuracy: 50.7666\n","[2021-04-12 08:45:55,942 INFO] Saving checkpoint model/en-vi_step_30000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XOEz-j6IbNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618217165472,"user_tz":-420,"elapsed":27,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"5f5123db-6a79-4d86-823b-0bbef9f35c48"},"source":["!ls -al model model/"],"execution_count":8,"outputs":[{"output_type":"stream","text":["model:\n","total 43395360\n","-rw------- 1 root root 1481228159 Apr 12 04:15 en-vi_step_10000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:13 en-vi_step_1000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:28 en-vi_step_11000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:42 en-vi_step_12000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:55 en-vi_step_13000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:09 en-vi_step_14000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:22 en-vi_step_15000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:36 en-vi_step_16000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:50 en-vi_step_17000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:03 en-vi_step_18000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:17 en-vi_step_19000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:30 en-vi_step_20000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:27 en-vi_step_2000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:44 en-vi_step_21000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:58 en-vi_step_22000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:11 en-vi_step_23000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:25 en-vi_step_24000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:38 en-vi_step_25000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:51 en-vi_step_26000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:05 en-vi_step_27000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:18 en-vi_step_28000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:32 en-vi_step_29000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:46 en-vi_step_30000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:40 en-vi_step_3000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:54 en-vi_step_4000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:07 en-vi_step_5000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:21 en-vi_step_6000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:34 en-vi_step_7000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:48 en-vi_step_8000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:01 en-vi_step_9000.pt\n","\n","model/:\n","total 43395360\n","-rw------- 1 root root 1481228159 Apr 12 04:15 en-vi_step_10000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:13 en-vi_step_1000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:28 en-vi_step_11000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:42 en-vi_step_12000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:55 en-vi_step_13000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:09 en-vi_step_14000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:22 en-vi_step_15000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:36 en-vi_step_16000.pt\n","-rw------- 1 root root 1481228159 Apr 12 05:50 en-vi_step_17000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:03 en-vi_step_18000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:17 en-vi_step_19000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:30 en-vi_step_20000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:27 en-vi_step_2000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:44 en-vi_step_21000.pt\n","-rw------- 1 root root 1481228159 Apr 12 06:58 en-vi_step_22000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:11 en-vi_step_23000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:25 en-vi_step_24000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:38 en-vi_step_25000.pt\n","-rw------- 1 root root 1481228159 Apr 12 07:51 en-vi_step_26000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:05 en-vi_step_27000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:18 en-vi_step_28000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:32 en-vi_step_29000.pt\n","-rw------- 1 root root 1481228159 Apr 12 08:46 en-vi_step_30000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:40 en-vi_step_3000.pt\n","-rw------- 1 root root 1481228159 Apr 12 02:54 en-vi_step_4000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:07 en-vi_step_5000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:21 en-vi_step_6000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:34 en-vi_step_7000.pt\n","-rw------- 1 root root 1481228159 Apr 12 03:48 en-vi_step_8000.pt\n","-rw------- 1 root root 1481228159 Apr 12 04:01 en-vi_step_9000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1S26AN4rHUAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618227043448,"user_tz":-420,"elapsed":9877662,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"6e024abb-d685-47d3-922c-0716d4202079"},"source":["!onmt_translate -model model/en-vi_step_30000.pt -src en_test -tgt vi_test -output predict.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-12 08:46:12,244 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [10, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-12 09:02:29,563 INFO] PRED AVG SCORE: -0.5941, PRED PPL: 1.8114\n","[2021-04-12 09:02:29,563 INFO] GOLD AVG SCORE: -3.1655, GOLD PPL: 23.7017\n","[2021-04-12 09:02:29,580 INFO] Translating shard 1.\n","tcmalloc: large alloc 1518125056 bytes == 0x555fdd9b4000 @  0x7f023701bb6b 0x7f023703b379 0x7f01e377425e 0x7f01e37759d2 0x7f02204588e6 0x7f02208badd9 0x7f0220dc577a 0x7f0220d90ef9 0x7f0220d47657 0x7f0220beb929 0x7f0220703516 0x7f0220dc67af 0x7f0220b75846 0x7f0220b7ae6f 0x7f022245ebcc 0x7f022245f13f 0x7f0220fc7a86 0x7f0220fcbcaf 0x7f02206f516a 0x7f02206f5b3a 0x7f0220eda7f8 0x7f0220eda83f 0x7f0220b75846 0x7f0220b7b22f 0x7f02206d90b1 0x7f0220ed94c0 0x7f0220efc05d 0x7f0220ccca59 0x7f02321ea8de 0x555f61195050 0x555f61194de0\n","tcmalloc: large alloc 1518125056 bytes == 0x556038180000 @  0x7f023701bb6b 0x7f023703b379 0x7f01e377425e 0x7f01e37759d2 0x7f02204588e6 0x7f02208badd9 0x7f0220dc577a 0x7f0220d90ef9 0x7f0220d47657 0x7f0220beb929 0x7f02208c46a2 0x7f02208545c5 0x7f0220dc6573 0x7f0220d36904 0x7f0220ba0f09 0x7f02223e8444 0x7f02223e8783 0x7f0220d36904 0x7f0220ba0f09 0x7f022084d5d0 0x7f0220eda0e0 0x7f0220eda132 0x7f0220d44054 0x7f0220fe8735 0x7f0231f47f4f 0x555f61195050 0x555f6128699d 0x555f61208fe9 0x555f61203b0e 0x555f6119677a 0x555f6120586a\n","[2021-04-12 09:18:46,034 INFO] PRED AVG SCORE: -0.5960, PRED PPL: 1.8149\n","[2021-04-12 09:18:46,034 INFO] GOLD AVG SCORE: -3.1806, GOLD PPL: 24.0613\n","[2021-04-12 09:18:46,053 INFO] Translating shard 2.\n","[2021-04-12 09:35:21,098 INFO] PRED AVG SCORE: -0.5907, PRED PPL: 1.8052\n","[2021-04-12 09:35:21,098 INFO] GOLD AVG SCORE: -3.1797, GOLD PPL: 24.0400\n","[2021-04-12 09:35:21,116 INFO] Translating shard 3.\n","[2021-04-12 09:51:44,284 INFO] PRED AVG SCORE: -0.5960, PRED PPL: 1.8149\n","[2021-04-12 09:51:44,284 INFO] GOLD AVG SCORE: -3.1546, GOLD PPL: 23.4435\n","[2021-04-12 09:51:44,320 INFO] Translating shard 4.\n","[2021-04-12 10:08:21,365 INFO] PRED AVG SCORE: -0.5931, PRED PPL: 1.8096\n","[2021-04-12 10:08:21,365 INFO] GOLD AVG SCORE: -3.1811, GOLD PPL: 24.0742\n","[2021-04-12 10:08:21,383 INFO] Translating shard 5.\n","[2021-04-12 10:24:49,857 INFO] PRED AVG SCORE: -0.5955, PRED PPL: 1.8140\n","[2021-04-12 10:24:49,857 INFO] GOLD AVG SCORE: -3.1571, GOLD PPL: 23.5020\n","[2021-04-12 10:24:49,877 INFO] Translating shard 6.\n","[2021-04-12 10:41:13,256 INFO] PRED AVG SCORE: -0.5928, PRED PPL: 1.8090\n","[2021-04-12 10:41:13,256 INFO] GOLD AVG SCORE: -3.1412, GOLD PPL: 23.1313\n","[2021-04-12 10:41:13,278 INFO] Translating shard 7.\n","[2021-04-12 10:57:43,360 INFO] PRED AVG SCORE: -0.5905, PRED PPL: 1.8050\n","[2021-04-12 10:57:43,361 INFO] GOLD AVG SCORE: -3.1303, GOLD PPL: 22.8800\n","[2021-04-12 10:57:43,380 INFO] Translating shard 8.\n","[2021-04-12 11:14:03,045 INFO] PRED AVG SCORE: -0.5947, PRED PPL: 1.8125\n","[2021-04-12 11:14:03,045 INFO] GOLD AVG SCORE: -3.1776, GOLD PPL: 23.9895\n","[2021-04-12 11:14:03,065 INFO] Translating shard 9.\n","[2021-04-12 11:30:02,283 INFO] PRED AVG SCORE: -0.5993, PRED PPL: 1.8209\n","[2021-04-12 11:30:02,284 INFO] GOLD AVG SCORE: -3.1765, GOLD PPL: 23.9632\n","[2021-04-12 11:30:02,301 INFO] Translating shard 10.\n","[2021-04-12 11:30:42,462 INFO] PRED AVG SCORE: -0.6021, PRED PPL: 1.8260\n","[2021-04-12 11:30:42,462 INFO] GOLD AVG SCORE: -3.0499, GOLD PPL: 21.1135\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPGsYFs_XpAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618266919675,"user_tz":-420,"elapsed":1081,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"9c3d1017-de26-44ff-c381-cb6bc2fbe031"},"source":["!tail vi_test"],"execution_count":5,"outputs":[{"output_type":"stream","text":["And nobody questions him, because they don't want to hear the answer because it's a lie!\n","Kubo?\n","Họ rất vui vẻ, và lúc nào cũng hát với nến.\n","Nghe này, anh không thể nói chuyện bây giờ được.\n","Vậy thì con có thể dùng trí tưởng tượng của mình.\n","Không hề.\n","Tôi đang nhìn hắn ngay lúc này đây.\n","Bác không để tâm chứ?\n","Anh nghĩ cậu ta phản ứng với thuốc?\n","Bị làm sao mà anh lại đi dự lễ đặt tên em bé của Cuddy chứ?\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rf5W-T8MzRK3"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw_Gma2gz0PK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618227060948,"user_tz":-420,"elapsed":304,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bbb34aac-89f8-4f1f-dc5e-2677589a3f58"},"source":["!ls -al"],"execution_count":12,"outputs":[{"output_type":"stream","text":["total 376662\n","drwx------  2 root root      4096 Apr 12 01:58 data_bin\n","-rw-------  1 root root   3318349 Apr 12 01:48 en_test\n","-rw-------  1 root root  26563375 Apr 12 01:48 en_train\n","-rw-------  1 root root  34866334 Apr 12 01:07 en_train_EM_0.8\n","-rw-------  1 root root  32006284 Apr 12 01:07 en_train_EM_0.85\n","-rw-------  1 root root  29696900 Apr 12 01:07 en_train_EM_0.9\n","-rw-------  1 root root  27849828 Apr 12 01:07 en_train_EM_0.95\n","-rw-------  1 root root  13223858 Apr 12 01:07 en_train_EM_factor_0.8\n","-rw-------  1 root root  12138268 Apr 12 01:07 en_train_EM_factor_0.85\n","-rw-------  1 root root  11254364 Apr 12 01:07 en_train_EM_factor_0.9\n","-rw-------  1 root root  10539008 Apr 12 01:07 en_train_EM_factor_0.95\n","-rw-------  1 root root   8152745 Apr 12 01:07 en_train_EM_score_0.8\n","-rw-------  1 root root   8152745 Apr 12 01:07 en_train_EM_score_0.85\n","-rw-------  1 root root   8152745 Apr 12 01:07 en_train_EM_score_0.9\n","-rw-------  1 root root   8152745 Apr 12 01:07 en_train_EM_score_0.95\n","-rw-------  1 root root   3328557 Apr 12 01:48 en_valid\n","drwx------  2 root root      4096 Apr 12 08:46 model\n","drwx------ 11 root root      4096 Apr 12 11:31 OpenNMT-py\n","-rw-------  1 root root 100661014 Apr 12 01:58 opus_bert.tar.gz\n","drwx------  2 root root      4096 Apr 12 01:59 output\n","-rw-------  1 root root   3854488 Apr 12 11:30 predict.txt\n","-rw-------  1 root root   4365722 Apr 12 01:48 vi_test\n","-rw-------  1 root root  35019161 Apr 12 01:48 vi_train\n","-rw-------  1 root root   4382771 Apr 12 01:48 vi_valid\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7T7xCaDdR469","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618266934096,"user_tz":-420,"elapsed":11013,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e563d908-cdfb-43ab-fc2e-2cc3d7ee8fec"},"source":["!perl OpenNMT-py/tools/multi-bleu.perl vi_test < predict.txt"],"execution_count":6,"outputs":[{"output_type":"stream","text":["BLEU = 17.36, 42.0/23.7/14.7/10.0 (BP=0.888, ratio=0.894, hyp_len=677925, ref_len=758454)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uLGJCSK_Qih1","executionInfo":{"status":"ok","timestamp":1618227066893,"user_tz":-420,"elapsed":12,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}}},"source":[""],"execution_count":13,"outputs":[]}]}