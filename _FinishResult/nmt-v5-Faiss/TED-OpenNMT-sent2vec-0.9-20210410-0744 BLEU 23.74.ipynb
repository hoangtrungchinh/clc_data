{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-sent2vec-0.9-20210410-0744 BLEU 23.74.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040652202,"user_tz":-420,"elapsed":18672,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"d5bea44e-8ceb-4456-a6ea-cd2f057b6475"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040654509,"user_tz":-420,"elapsed":20967,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"c141b570-be6f-461a-ca73-a5777386bc21"},"source":["import os\n","path = \"\"\n","path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/'\n","os.chdir(path)\n","import time\n","FOLDERNAME = \"TED-OpenNMT-sent2vec-0.9-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","!mkdir $FOLDERNAME\n","\n","path = path + FOLDERNAME\n","os.chdir(path)\n","!pwd\n","\n","# import os\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v3/OpenNMT-TED-EM-bert-ratio-8-2-2-20210128-0637'\n","# os.chdir(path)\n","# !pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/TED-OpenNMT-sent2vec-0.9-20210410-0744\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040654510,"user_tz":-420,"elapsed":20964,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"6d16c7f1-a408-44a4-8fca-462a6f4d6abb"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Sat Apr 10 07:44:13 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040668380,"user_tz":-420,"elapsed":34830,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2e9a10e0-e7a4-4301-8a61-ecef83395b00"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 23.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 27.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 19.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 15.2MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/63/17c6ac0d8a0cfa5ff7257e52edb6759d12dc266392f6c97f5c65c0c7238c/pyonmttok-1.25.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 13.0MB/s \n","\u001b[?25hCollecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n","\u001b[?25hCollecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Collecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.8.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=f677d44a651965bd1ee1f73ff27341bcd60cf96543fc0024be2ac9190a173e6b\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: pyonmttok, configargparse, torchtext, waitress, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.25.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.6MB/s \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040670659,"user_tz":-420,"elapsed":37105,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"8eca834d-3e15-4cdd-e9f9-8e63824edf42"},"source":["!wget https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/en_vi_iwslt_sent2vec.tar.gz\n","!mkdir data_bin\n","!tar -xvf 'en_vi_iwslt_sent2vec.tar.gz'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2021-04-10 07:44:27--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/en_vi_iwslt_sent2vec.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 22512408 (21M) [application/octet-stream]\n","Saving to: ‘en_vi_iwslt_sent2vec.tar.gz’\n","\n","en_vi_iwslt_sent2ve 100%[===================>]  21.47M  78.6MB/s    in 0.3s    \n","\n","2021-04-10 07:44:28 (78.6 MB/s) - ‘en_vi_iwslt_sent2vec.tar.gz’ saved [22512408/22512408]\n","\n","en_train_EM_score_0.95\n","vi_valid\n","en_train_EM_0.95\n","en_train_EM_factor_0.85\n","en_train_EM_score_0.8\n","vi_train\n","en_train_EM_factor_0.8\n","en_train_EM_0.8\n","en_valid\n","en_train_EM_factor_0.95\n","en_train\n","en_train_EM_score_0.85\n","vi_test\n","en_train_EM_0.85\n","en_train_EM_score_0.9\n","en_test\n","en_train_EM_factor_0.9\n","en_train_EM_0.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618040682707,"user_tz":-420,"elapsed":49150,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"b47e7882-383b-4905-a373-6b14406148a5"},"source":["!mkdir -p output\n","!onmt_preprocess -train_src 'en_train_EM_0.9' \\\\\n","-train_tgt 'vi_train' \\\\\n","-valid_src 'en_valid' \\\\\n","-valid_tgt 'vi_valid' \\\\\n","-save_data 'output/en-vi' "],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-10 07:44:33,376 INFO] Extracting features...\n","[2021-04-10 07:44:33,380 INFO]  * number of source features: 0.\n","[2021-04-10 07:44:33,380 INFO]  * number of target features: 0.\n","[2021-04-10 07:44:33,380 INFO] Building `Fields` object...\n","[2021-04-10 07:44:33,380 INFO] Building & saving training data...\n","[2021-04-10 07:44:33,550 INFO] Building shard 0.\n","[2021-04-10 07:44:36,958 INFO]  * saving 0th train data shard to output/en-vi.train.0.pt.\n","[2021-04-10 07:44:40,148 INFO]  * tgt vocab size: 18250.\n","[2021-04-10 07:44:40,198 INFO]  * src vocab size: 40209.\n","[2021-04-10 07:44:40,383 INFO] Building & saving validation data...\n","[2021-04-10 07:44:40,503 INFO] Building shard 0.\n","[2021-04-10 07:44:40,739 INFO]  * saving 0th valid data shard to output/en-vi.valid.0.pt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYyQECmi0TX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618059095364,"user_tz":-420,"elapsed":18461805,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2cfcb3d3-b1a9-4a9d-ad2e-e3f6081214bc"},"source":["!mkdir -p model\n","!onmt_train -data 'output/en-vi' \\\\\n","-save_model 'model/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30000  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 1000 -save_checkpoint_steps 1000 \\\\\n","-report_every 1000 -world_size 1 -gpu_ranks 0"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-10 07:44:43,345 INFO]  * src vocab size = 40209\n","[2021-04-10 07:44:43,345 INFO]  * tgt vocab size = 18250\n","[2021-04-10 07:44:43,345 INFO] Building model...\n","[2021-04-10 07:44:50,564 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(40209, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-10 07:44:50,605 INFO] encoder: 39502336\n","[2021-04-10 07:44:50,605 INFO] decoder: 43931466\n","[2021-04-10 07:44:50,605 INFO] * number of parameters: 83433802\n","[2021-04-10 07:44:50,609 INFO] Starting training on GPU: [0]\n","[2021-04-10 07:44:50,610 INFO] Start training loop and validate every 1000 steps...\n","[2021-04-10 07:44:50,610 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:44:53,210 INFO] number of examples: 77471\n","[2021-04-10 07:47:00,483 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:47:03,683 INFO] number of examples: 77471\n","[2021-04-10 07:49:10,926 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:49:13,671 INFO] number of examples: 77471\n","[2021-04-10 07:51:21,001 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:51:24,147 INFO] number of examples: 77471\n","[2021-04-10 07:53:31,401 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:53:34,702 INFO] number of examples: 77471\n","[2021-04-10 07:54:42,183 INFO] Step 1000/30000; acc:  13.21; ppl: 265.08; xent: 5.58; lr: 0.00012; 10336/12911 tok/s;    592 sec\n","[2021-04-10 07:54:42,184 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 07:54:42,474 INFO] number of examples: 10362\n","[2021-04-10 07:54:59,183 INFO] Validation perplexity: 114.688\n","[2021-04-10 07:54:59,183 INFO] Validation accuracy: 25.0402\n","[2021-04-10 07:54:59,367 INFO] Saving checkpoint model/en-vi_step_1000.pt\n","[2021-04-10 07:56:03,779 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:56:07,244 INFO] number of examples: 77471\n","[2021-04-10 07:58:14,555 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 07:58:17,492 INFO] number of examples: 77471\n","[2021-04-10 08:00:24,913 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:00:28,345 INFO] number of examples: 77471\n","[2021-04-10 08:02:35,705 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:02:39,385 INFO] number of examples: 77471\n","[2021-04-10 08:04:46,787 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:04:49,956 INFO] number of examples: 77471\n","[2021-04-10 08:04:57,002 INFO] Step 2000/30000; acc:  39.41; ppl: 22.36; xent: 3.11; lr: 0.00025; 9906/12406 tok/s;   1206 sec\n","[2021-04-10 08:04:57,003 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 08:04:57,304 INFO] number of examples: 10362\n","[2021-04-10 08:05:14,031 INFO] Validation perplexity: 23.0166\n","[2021-04-10 08:05:14,031 INFO] Validation accuracy: 45.326\n","[2021-04-10 08:05:14,206 INFO] Saving checkpoint model/en-vi_step_2000.pt\n","[2021-04-10 08:07:19,390 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:07:22,524 INFO] number of examples: 77471\n","[2021-04-10 08:09:29,789 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:09:33,441 INFO] number of examples: 77471\n","[2021-04-10 08:11:40,749 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:11:43,947 INFO] number of examples: 77471\n","[2021-04-10 08:13:51,178 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:13:54,283 INFO] number of examples: 77471\n","[2021-04-10 08:15:08,130 INFO] Step 3000/30000; acc:  56.73; ppl:  6.94; xent: 1.94; lr: 0.00037; 10001/12500 tok/s;   1818 sec\n","[2021-04-10 08:15:08,131 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 08:15:08,429 INFO] number of examples: 10362\n","[2021-04-10 08:15:25,152 INFO] Validation perplexity: 15.4566\n","[2021-04-10 08:15:25,153 INFO] Validation accuracy: 51.1151\n","[2021-04-10 08:15:25,331 INFO] Saving checkpoint model/en-vi_step_3000.pt\n","[2021-04-10 08:16:23,566 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:16:27,260 INFO] number of examples: 77471\n","[2021-04-10 08:18:34,565 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:18:37,786 INFO] number of examples: 77471\n","[2021-04-10 08:20:45,228 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:20:48,408 INFO] number of examples: 77471\n","[2021-04-10 08:22:55,808 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:22:58,878 INFO] number of examples: 77471\n","[2021-04-10 08:25:06,256 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:25:09,815 INFO] number of examples: 77471\n","[2021-04-10 08:25:23,122 INFO] Step 4000/30000; acc:  65.26; ppl:  4.18; xent: 1.43; lr: 0.00049; 9900/12394 tok/s;   2433 sec\n","[2021-04-10 08:25:23,124 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 08:25:23,438 INFO] number of examples: 10362\n","[2021-04-10 08:25:40,167 INFO] Validation perplexity: 16.5255\n","[2021-04-10 08:25:40,167 INFO] Validation accuracy: 51.5087\n","[2021-04-10 08:25:40,343 INFO] Saving checkpoint model/en-vi_step_4000.pt\n","[2021-04-10 08:27:39,360 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:27:43,019 INFO] number of examples: 77471\n","[2021-04-10 08:29:50,382 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:29:53,501 INFO] number of examples: 77471\n","[2021-04-10 08:32:00,829 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:32:04,539 INFO] number of examples: 77471\n","[2021-04-10 08:34:11,849 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:34:15,096 INFO] number of examples: 77471\n","[2021-04-10 08:35:35,209 INFO] Step 5000/30000; acc:  71.62; ppl:  3.05; xent: 1.12; lr: 0.00062; 9986/12489 tok/s;   3045 sec\n","[2021-04-10 08:35:35,210 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 08:35:35,508 INFO] number of examples: 10362\n","[2021-04-10 08:35:52,246 INFO] Validation perplexity: 18.4097\n","[2021-04-10 08:35:52,246 INFO] Validation accuracy: 51.6677\n","[2021-04-10 08:35:52,420 INFO] Saving checkpoint model/en-vi_step_5000.pt\n","[2021-04-10 08:36:44,979 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:36:48,090 INFO] number of examples: 77471\n","[2021-04-10 08:38:55,442 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:38:59,075 INFO] number of examples: 77471\n","[2021-04-10 08:41:06,368 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:41:09,519 INFO] number of examples: 77471\n","[2021-04-10 08:43:16,820 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:43:20,516 INFO] number of examples: 77471\n","[2021-04-10 08:45:27,800 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:45:31,037 INFO] number of examples: 77471\n","[2021-04-10 08:45:50,636 INFO] Step 6000/30000; acc:  76.97; ppl:  2.42; xent: 0.89; lr: 0.00074; 9897/12386 tok/s;   3660 sec\n","[2021-04-10 08:45:50,637 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 08:45:50,946 INFO] number of examples: 10362\n","[2021-04-10 08:46:07,674 INFO] Validation perplexity: 21.0332\n","[2021-04-10 08:46:07,674 INFO] Validation accuracy: 51.5521\n","[2021-04-10 08:46:07,850 INFO] Saving checkpoint model/en-vi_step_6000.pt\n","[2021-04-10 08:48:00,307 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:48:03,479 INFO] number of examples: 77471\n","[2021-04-10 08:50:10,783 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:50:14,473 INFO] number of examples: 77471\n","[2021-04-10 08:52:21,758 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:52:25,012 INFO] number of examples: 77471\n","[2021-04-10 08:54:32,205 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:54:35,346 INFO] number of examples: 77471\n","[2021-04-10 08:56:01,663 INFO] Step 7000/30000; acc:  80.94; ppl:  2.07; xent: 0.73; lr: 0.00086; 10000/12511 tok/s;   4271 sec\n","[2021-04-10 08:56:01,664 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 08:56:01,961 INFO] number of examples: 10362\n","[2021-04-10 08:56:18,692 INFO] Validation perplexity: 23.7524\n","[2021-04-10 08:56:18,692 INFO] Validation accuracy: 51.3884\n","[2021-04-10 08:56:18,862 INFO] Saving checkpoint model/en-vi_step_7000.pt\n","[2021-04-10 08:57:04,590 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:57:08,212 INFO] number of examples: 77471\n","[2021-04-10 08:59:15,505 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 08:59:18,600 INFO] number of examples: 77471\n","[2021-04-10 09:01:25,941 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:01:29,514 INFO] number of examples: 77471\n","[2021-04-10 09:03:36,806 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:03:39,901 INFO] number of examples: 77471\n","[2021-04-10 09:05:47,276 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:05:50,856 INFO] number of examples: 77471\n","[2021-04-10 09:06:16,755 INFO] Step 8000/30000; acc:  84.23; ppl:  1.84; xent: 0.61; lr: 0.00099; 9907/12395 tok/s;   4886 sec\n","[2021-04-10 09:06:16,756 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 09:06:17,060 INFO] number of examples: 10362\n","[2021-04-10 09:06:33,779 INFO] Validation perplexity: 27.1347\n","[2021-04-10 09:06:33,779 INFO] Validation accuracy: 51.2886\n","[2021-04-10 09:06:33,947 INFO] Saving checkpoint model/en-vi_step_8000.pt\n","[2021-04-10 09:08:20,133 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:08:23,795 INFO] number of examples: 77471\n","[2021-04-10 09:10:31,133 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:10:34,290 INFO] number of examples: 77471\n","[2021-04-10 09:12:41,637 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:12:44,753 INFO] number of examples: 77471\n","[2021-04-10 09:14:52,050 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:14:55,620 INFO] number of examples: 77471\n","[2021-04-10 09:16:28,172 INFO] Step 9000/30000; acc:  87.51; ppl:  1.65; xent: 0.50; lr: 0.00093; 9988/12497 tok/s;   5498 sec\n","[2021-04-10 09:16:28,173 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 09:16:28,469 INFO] number of examples: 10362\n","[2021-04-10 09:16:45,190 INFO] Validation perplexity: 26.05\n","[2021-04-10 09:16:45,190 INFO] Validation accuracy: 51.9304\n","[2021-04-10 09:16:45,359 INFO] Saving checkpoint model/en-vi_step_9000.pt\n","[2021-04-10 09:17:24,998 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:17:28,164 INFO] number of examples: 77471\n","[2021-04-10 09:19:35,432 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:19:39,020 INFO] number of examples: 77471\n","[2021-04-10 09:21:46,339 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:21:49,471 INFO] number of examples: 77471\n","[2021-04-10 09:23:56,776 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:24:00,368 INFO] number of examples: 77471\n","[2021-04-10 09:26:07,671 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:26:10,808 INFO] number of examples: 77471\n","[2021-04-10 09:26:43,027 INFO] Step 10000/30000; acc:  91.05; ppl:  1.48; xent: 0.39; lr: 0.00088; 9918/12408 tok/s;   6112 sec\n","[2021-04-10 09:26:43,028 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 09:26:43,828 INFO] number of examples: 10362\n","[2021-04-10 09:27:00,548 INFO] Validation perplexity: 27.9055\n","[2021-04-10 09:27:00,548 INFO] Validation accuracy: 52.1215\n","[2021-04-10 09:27:00,720 INFO] Saving checkpoint model/en-vi_step_10000.pt\n","[2021-04-10 09:28:40,429 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:28:43,470 INFO] number of examples: 77471\n","[2021-04-10 09:30:50,741 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:30:54,352 INFO] number of examples: 77471\n","[2021-04-10 09:33:01,653 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:33:04,732 INFO] number of examples: 77471\n","[2021-04-10 09:35:12,032 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:35:15,580 INFO] number of examples: 77471\n","[2021-04-10 09:36:54,527 INFO] Step 11000/30000; acc:  93.30; ppl:  1.37; xent: 0.32; lr: 0.00084; 9987/12494 tok/s;   6724 sec\n","[2021-04-10 09:36:54,528 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 09:36:54,830 INFO] number of examples: 10362\n","[2021-04-10 09:37:11,569 INFO] Validation perplexity: 29.5404\n","[2021-04-10 09:37:11,569 INFO] Validation accuracy: 52.3287\n","[2021-04-10 09:37:11,738 INFO] Saving checkpoint model/en-vi_step_11000.pt\n","[2021-04-10 09:37:44,901 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:37:48,620 INFO] number of examples: 77471\n","[2021-04-10 09:39:56,002 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:39:59,121 INFO] number of examples: 77471\n","[2021-04-10 09:42:06,450 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:42:10,181 INFO] number of examples: 77471\n","[2021-04-10 09:44:17,469 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:44:20,706 INFO] number of examples: 77471\n","[2021-04-10 09:46:28,051 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:46:31,270 INFO] number of examples: 77471\n","[2021-04-10 09:47:09,820 INFO] Step 12000/30000; acc:  94.85; ppl:  1.31; xent: 0.27; lr: 0.00081; 9905/12393 tok/s;   7339 sec\n","[2021-04-10 09:47:09,821 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 09:47:10,116 INFO] number of examples: 10362\n","[2021-04-10 09:47:26,846 INFO] Validation perplexity: 29.263\n","[2021-04-10 09:47:26,846 INFO] Validation accuracy: 52.8253\n","[2021-04-10 09:47:27,020 INFO] Saving checkpoint model/en-vi_step_12000.pt\n","[2021-04-10 09:49:01,034 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:49:04,802 INFO] number of examples: 77471\n","[2021-04-10 09:51:12,158 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:51:15,382 INFO] number of examples: 77471\n","[2021-04-10 09:53:22,735 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:53:25,944 INFO] number of examples: 77471\n","[2021-04-10 09:55:33,243 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:55:36,373 INFO] number of examples: 77471\n","[2021-04-10 09:57:21,632 INFO] Step 13000/30000; acc:  95.86; ppl:  1.26; xent: 0.23; lr: 0.00078; 9986/12490 tok/s;   7951 sec\n","[2021-04-10 09:57:21,633 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 09:57:21,929 INFO] number of examples: 10362\n","[2021-04-10 09:57:38,659 INFO] Validation perplexity: 31.3951\n","[2021-04-10 09:57:38,659 INFO] Validation accuracy: 52.279\n","[2021-04-10 09:57:38,830 INFO] Saving checkpoint model/en-vi_step_13000.pt\n","[2021-04-10 09:58:05,807 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 09:58:09,312 INFO] number of examples: 77471\n","[2021-04-10 10:00:16,574 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:00:20,233 INFO] number of examples: 77471\n","[2021-04-10 10:02:27,607 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:02:30,817 INFO] number of examples: 77471\n","[2021-04-10 10:04:38,094 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:04:41,150 INFO] number of examples: 77471\n","[2021-04-10 10:06:48,447 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:06:51,994 INFO] number of examples: 77471\n","[2021-04-10 10:07:36,853 INFO] Step 14000/30000; acc:  96.59; ppl:  1.23; xent: 0.21; lr: 0.00075; 9909/12397 tok/s;   8566 sec\n","[2021-04-10 10:07:36,855 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 10:07:37,155 INFO] number of examples: 10362\n","[2021-04-10 10:07:53,884 INFO] Validation perplexity: 31.3378\n","[2021-04-10 10:07:53,884 INFO] Validation accuracy: 52.8813\n","[2021-04-10 10:07:54,057 INFO] Saving checkpoint model/en-vi_step_14000.pt\n","[2021-04-10 10:09:21,134 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:09:24,768 INFO] number of examples: 77471\n","[2021-04-10 10:11:32,049 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:11:35,106 INFO] number of examples: 77471\n","[2021-04-10 10:13:42,382 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:13:45,948 INFO] number of examples: 77471\n","[2021-04-10 10:15:53,241 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:15:56,351 INFO] number of examples: 77471\n","[2021-04-10 10:17:47,829 INFO] Step 15000/30000; acc:  97.16; ppl:  1.20; xent: 0.19; lr: 0.00072; 9990/12502 tok/s;   9177 sec\n","[2021-04-10 10:17:47,830 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 10:17:48,746 INFO] number of examples: 10362\n","[2021-04-10 10:18:05,465 INFO] Validation perplexity: 32.3346\n","[2021-04-10 10:18:05,465 INFO] Validation accuracy: 52.8644\n","[2021-04-10 10:18:05,638 INFO] Saving checkpoint model/en-vi_step_15000.pt\n","[2021-04-10 10:18:26,402 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:18:30,133 INFO] number of examples: 77471\n","[2021-04-10 10:20:37,624 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:20:41,154 INFO] number of examples: 77471\n","[2021-04-10 10:22:48,491 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:22:51,552 INFO] number of examples: 77471\n","[2021-04-10 10:24:58,862 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:25:02,411 INFO] number of examples: 77471\n","[2021-04-10 10:27:09,709 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:27:13,416 INFO] number of examples: 77471\n","[2021-04-10 10:28:04,620 INFO] Step 16000/30000; acc:  97.56; ppl:  1.18; xent: 0.17; lr: 0.00070; 9892/12369 tok/s;   9794 sec\n","[2021-04-10 10:28:04,621 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 10:28:04,931 INFO] number of examples: 10362\n","[2021-04-10 10:28:21,664 INFO] Validation perplexity: 33.0127\n","[2021-04-10 10:28:21,664 INFO] Validation accuracy: 53.2114\n","[2021-04-10 10:28:21,841 INFO] Saving checkpoint model/en-vi_step_16000.pt\n","[2021-04-10 10:29:42,963 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:29:46,108 INFO] number of examples: 77471\n","[2021-04-10 10:31:53,396 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:31:57,070 INFO] number of examples: 77471\n","[2021-04-10 10:34:04,324 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:34:07,511 INFO] number of examples: 77471\n","[2021-04-10 10:36:14,841 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:36:18,009 INFO] number of examples: 77471\n","[2021-04-10 10:38:15,709 INFO] Step 17000/30000; acc:  97.88; ppl:  1.17; xent: 0.16; lr: 0.00068; 9986/12500 tok/s;  10405 sec\n","[2021-04-10 10:38:15,710 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 10:38:16,004 INFO] number of examples: 10362\n","[2021-04-10 10:38:32,721 INFO] Validation perplexity: 33.19\n","[2021-04-10 10:38:32,722 INFO] Validation accuracy: 53.0931\n","[2021-04-10 10:38:32,921 INFO] Saving checkpoint model/en-vi_step_17000.pt\n","[2021-04-10 10:38:47,658 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:38:51,662 INFO] number of examples: 77471\n","[2021-04-10 10:40:59,273 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:41:02,340 INFO] number of examples: 77471\n","[2021-04-10 10:43:09,619 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:43:13,178 INFO] number of examples: 77471\n","[2021-04-10 10:45:20,515 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:45:23,568 INFO] number of examples: 77471\n","[2021-04-10 10:47:30,906 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:47:34,592 INFO] number of examples: 77471\n","[2021-04-10 10:48:32,314 INFO] Step 18000/30000; acc:  98.12; ppl:  1.16; xent: 0.14; lr: 0.00066; 9897/12373 tok/s;  11022 sec\n","[2021-04-10 10:48:32,315 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 10:48:32,624 INFO] number of examples: 10362\n","[2021-04-10 10:48:49,346 INFO] Validation perplexity: 32.8294\n","[2021-04-10 10:48:49,346 INFO] Validation accuracy: 53.0845\n","[2021-04-10 10:48:49,514 INFO] Saving checkpoint model/en-vi_step_18000.pt\n","[2021-04-10 10:50:03,986 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:50:07,624 INFO] number of examples: 77471\n","[2021-04-10 10:52:14,922 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:52:18,138 INFO] number of examples: 77471\n","[2021-04-10 10:54:25,685 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:54:28,923 INFO] number of examples: 77471\n","[2021-04-10 10:56:36,482 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:56:40,202 INFO] number of examples: 77471\n","[2021-04-10 10:58:44,456 INFO] Step 19000/30000; acc:  98.34; ppl:  1.14; xent: 0.13; lr: 0.00064; 9962/12476 tok/s;  11634 sec\n","[2021-04-10 10:58:44,457 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 10:58:44,759 INFO] number of examples: 10362\n","[2021-04-10 10:59:01,556 INFO] Validation perplexity: 34.0738\n","[2021-04-10 10:59:01,556 INFO] Validation accuracy: 53.2697\n","[2021-04-10 10:59:01,727 INFO] Saving checkpoint model/en-vi_step_19000.pt\n","[2021-04-10 10:59:09,747 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 10:59:13,823 INFO] number of examples: 77471\n","[2021-04-10 11:01:21,751 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:01:24,968 INFO] number of examples: 77471\n","[2021-04-10 11:03:32,495 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:03:35,689 INFO] number of examples: 77471\n","[2021-04-10 11:05:43,295 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:05:46,979 INFO] number of examples: 77471\n","[2021-04-10 11:07:54,463 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:07:57,635 INFO] number of examples: 77471\n","[2021-04-10 11:09:01,688 INFO] Step 20000/30000; acc:  98.49; ppl:  1.13; xent: 0.13; lr: 0.00062; 9896/12362 tok/s;  12251 sec\n","[2021-04-10 11:09:01,689 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 11:09:01,988 INFO] number of examples: 10362\n","[2021-04-10 11:09:18,715 INFO] Validation perplexity: 34.5525\n","[2021-04-10 11:09:18,716 INFO] Validation accuracy: 53.0093\n","[2021-04-10 11:09:18,893 INFO] Saving checkpoint model/en-vi_step_20000.pt\n","[2021-04-10 11:10:27,279 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:10:30,364 INFO] number of examples: 77471\n","[2021-04-10 11:12:37,725 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:12:41,233 INFO] number of examples: 77471\n","[2021-04-10 11:14:48,550 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:14:52,228 INFO] number of examples: 77471\n","[2021-04-10 11:16:59,540 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:17:02,749 INFO] number of examples: 77471\n","[2021-04-10 11:19:10,043 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:19:13,189 INFO] number of examples: 77471\n","[2021-04-10 11:19:16,746 INFO] Step 21000/30000; acc:  98.64; ppl:  1.13; xent: 0.12; lr: 0.00061; 9902/12405 tok/s;  12866 sec\n","[2021-04-10 11:19:16,747 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 11:19:17,049 INFO] number of examples: 10362\n","[2021-04-10 11:19:33,777 INFO] Validation perplexity: 35.1843\n","[2021-04-10 11:19:33,777 INFO] Validation accuracy: 53.1295\n","[2021-04-10 11:19:33,958 INFO] Saving checkpoint model/en-vi_step_21000.pt\n","[2021-04-10 11:21:42,345 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:21:45,962 INFO] number of examples: 77471\n","[2021-04-10 11:23:53,217 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:23:56,360 INFO] number of examples: 77471\n","[2021-04-10 11:26:03,682 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:26:07,238 INFO] number of examples: 77471\n","[2021-04-10 11:28:14,522 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:28:17,656 INFO] number of examples: 77471\n","[2021-04-10 11:29:28,012 INFO] Step 22000/30000; acc:  98.75; ppl:  1.12; xent: 0.11; lr: 0.00060; 10002/12498 tok/s;  13477 sec\n","[2021-04-10 11:29:28,013 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 11:29:28,947 INFO] number of examples: 10362\n","[2021-04-10 11:29:45,666 INFO] Validation perplexity: 35.0411\n","[2021-04-10 11:29:45,666 INFO] Validation accuracy: 53.274\n","[2021-04-10 11:29:45,836 INFO] Saving checkpoint model/en-vi_step_22000.pt\n","[2021-04-10 11:30:47,609 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:30:50,991 INFO] number of examples: 77471\n","[2021-04-10 11:32:58,383 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:33:01,912 INFO] number of examples: 77471\n","[2021-04-10 11:35:08,962 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:35:12,540 INFO] number of examples: 77471\n","[2021-04-10 11:37:19,470 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:37:22,652 INFO] number of examples: 77471\n","[2021-04-10 11:39:29,596 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:39:32,665 INFO] number of examples: 77471\n","[2021-04-10 11:39:42,504 INFO] Step 23000/30000; acc:  98.84; ppl:  1.11; xent: 0.11; lr: 0.00058; 9911/12410 tok/s;  14092 sec\n","[2021-04-10 11:39:42,505 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 11:39:42,811 INFO] number of examples: 10362\n","[2021-04-10 11:39:59,455 INFO] Validation perplexity: 36.0802\n","[2021-04-10 11:39:59,455 INFO] Validation accuracy: 53.1601\n","[2021-04-10 11:39:59,623 INFO] Saving checkpoint model/en-vi_step_23000.pt\n","[2021-04-10 11:42:01,458 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:42:04,952 INFO] number of examples: 77471\n","[2021-04-10 11:44:11,888 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:44:14,929 INFO] number of examples: 77471\n","[2021-04-10 11:46:22,058 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:46:25,733 INFO] number of examples: 77471\n","[2021-04-10 11:48:33,100 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:48:36,176 INFO] number of examples: 77471\n","[2021-04-10 11:49:52,787 INFO] Step 24000/30000; acc:  98.92; ppl:  1.11; xent: 0.10; lr: 0.00057; 10013/12519 tok/s;  14702 sec\n","[2021-04-10 11:49:52,788 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 11:49:53,689 INFO] number of examples: 10362\n","[2021-04-10 11:50:10,400 INFO] Validation perplexity: 35.816\n","[2021-04-10 11:50:10,400 INFO] Validation accuracy: 53.3641\n","[2021-04-10 11:50:10,571 INFO] Saving checkpoint model/en-vi_step_24000.pt\n","[2021-04-10 11:51:06,273 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:51:09,636 INFO] number of examples: 77471\n","[2021-04-10 11:53:16,909 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:53:20,400 INFO] number of examples: 77471\n","[2021-04-10 11:55:27,714 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:55:31,388 INFO] number of examples: 77471\n","[2021-04-10 11:57:38,566 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:57:41,665 INFO] number of examples: 77471\n","[2021-04-10 11:59:48,574 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 11:59:51,643 INFO] number of examples: 77471\n","[2021-04-10 12:00:07,743 INFO] Step 25000/30000; acc:  98.99; ppl:  1.10; xent: 0.10; lr: 0.00056; 9905/12395 tok/s;  15317 sec\n","[2021-04-10 12:00:07,744 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 12:00:08,051 INFO] number of examples: 10362\n","[2021-04-10 12:00:24,695 INFO] Validation perplexity: 35.5347\n","[2021-04-10 12:00:24,695 INFO] Validation accuracy: 53.3947\n","[2021-04-10 12:00:24,861 INFO] Saving checkpoint model/en-vi_step_25000.pt\n","[2021-04-10 12:02:20,483 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:02:24,039 INFO] number of examples: 77471\n","[2021-04-10 12:04:30,991 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:04:34,038 INFO] number of examples: 77471\n","[2021-04-10 12:06:41,005 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:06:44,453 INFO] number of examples: 77471\n","[2021-04-10 12:08:51,384 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:08:54,385 INFO] number of examples: 77471\n","[2021-04-10 12:10:17,043 INFO] Step 26000/30000; acc:  99.06; ppl:  1.10; xent: 0.09; lr: 0.00055; 10029/12546 tok/s;  15926 sec\n","[2021-04-10 12:10:17,044 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 12:10:17,883 INFO] number of examples: 10362\n","[2021-04-10 12:10:34,518 INFO] Validation perplexity: 35.8604\n","[2021-04-10 12:10:34,518 INFO] Validation accuracy: 53.3077\n","[2021-04-10 12:10:34,684 INFO] Saving checkpoint model/en-vi_step_26000.pt\n","[2021-04-10 12:11:23,691 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:11:27,785 INFO] number of examples: 77471\n","[2021-04-10 12:13:34,735 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:13:37,913 INFO] number of examples: 77471\n","[2021-04-10 12:15:45,068 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:15:48,246 INFO] number of examples: 77471\n","[2021-04-10 12:17:55,559 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:17:58,750 INFO] number of examples: 77471\n","[2021-04-10 12:20:06,028 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:20:09,136 INFO] number of examples: 77471\n","[2021-04-10 12:20:31,555 INFO] Step 27000/30000; acc:  99.12; ppl:  1.09; xent: 0.09; lr: 0.00054; 9914/12404 tok/s;  16541 sec\n","[2021-04-10 12:20:31,556 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 12:20:31,859 INFO] number of examples: 10362\n","[2021-04-10 12:20:48,575 INFO] Validation perplexity: 37.4468\n","[2021-04-10 12:20:48,576 INFO] Validation accuracy: 53.493\n","[2021-04-10 12:20:48,754 INFO] Saving checkpoint model/en-vi_step_27000.pt\n","[2021-04-10 12:22:38,403 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:22:41,930 INFO] number of examples: 77471\n","[2021-04-10 12:24:49,218 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:24:52,866 INFO] number of examples: 77471\n","[2021-04-10 12:27:00,132 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:27:03,301 INFO] number of examples: 77471\n","[2021-04-10 12:29:10,643 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:29:13,728 INFO] number of examples: 77471\n","[2021-04-10 12:30:42,881 INFO] Step 28000/30000; acc:  99.16; ppl:  1.09; xent: 0.09; lr: 0.00053; 9993/12502 tok/s;  17152 sec\n","[2021-04-10 12:30:42,882 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 12:30:43,794 INFO] number of examples: 10362\n","[2021-04-10 12:31:00,515 INFO] Validation perplexity: 36.7555\n","[2021-04-10 12:31:00,515 INFO] Validation accuracy: 53.3606\n","[2021-04-10 12:31:00,697 INFO] Saving checkpoint model/en-vi_step_28000.pt\n","[2021-04-10 12:31:43,850 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:31:48,154 INFO] number of examples: 77471\n","[2021-04-10 12:33:55,508 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:33:58,865 INFO] number of examples: 77471\n","[2021-04-10 12:36:06,224 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:36:09,442 INFO] number of examples: 77471\n","[2021-04-10 12:38:16,791 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:38:19,990 INFO] number of examples: 77471\n","[2021-04-10 12:40:27,344 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:40:30,482 INFO] number of examples: 77471\n","[2021-04-10 12:40:59,311 INFO] Step 29000/30000; acc:  99.21; ppl:  1.09; xent: 0.08; lr: 0.00052; 9890/12373 tok/s;  17769 sec\n","[2021-04-10 12:40:59,312 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 12:40:59,625 INFO] number of examples: 10362\n","[2021-04-10 12:41:16,349 INFO] Validation perplexity: 38.1356\n","[2021-04-10 12:41:16,349 INFO] Validation accuracy: 53.2439\n","[2021-04-10 12:41:16,521 INFO] Saving checkpoint model/en-vi_step_29000.pt\n","[2021-04-10 12:42:59,965 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:43:03,522 INFO] number of examples: 77471\n","[2021-04-10 12:45:10,860 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:45:14,556 INFO] number of examples: 77471\n","[2021-04-10 12:47:21,854 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:47:25,066 INFO] number of examples: 77471\n","[2021-04-10 12:49:32,369 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-10 12:49:35,451 INFO] number of examples: 77471\n","[2021-04-10 12:51:10,988 INFO] Step 30000/30000; acc:  99.24; ppl:  1.08; xent: 0.08; lr: 0.00051; 9984/12491 tok/s;  18380 sec\n","[2021-04-10 12:51:10,989 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-10 12:51:11,892 INFO] number of examples: 10362\n","[2021-04-10 12:51:28,614 INFO] Validation perplexity: 37.7975\n","[2021-04-10 12:51:28,615 INFO] Validation accuracy: 53.5184\n","[2021-04-10 12:51:28,784 INFO] Saving checkpoint model/en-vi_step_30000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XOEz-j6IbNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618059095366,"user_tz":-420,"elapsed":18461804,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"b3c8eb75-9159-4932-ef67-3acd80f1403a"},"source":["!ls -al model model/"],"execution_count":8,"outputs":[{"output_type":"stream","text":["model:\n","total 29997315\n","-rw------- 1 root root 1023908031 Apr 10 09:27 en-vi_step_10000.pt\n","-rw------- 1 root root 1023908031 Apr 10 07:55 en-vi_step_1000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:37 en-vi_step_11000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:47 en-vi_step_12000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:57 en-vi_step_13000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:07 en-vi_step_14000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:18 en-vi_step_15000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:28 en-vi_step_16000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:38 en-vi_step_17000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:48 en-vi_step_18000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:59 en-vi_step_19000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:09 en-vi_step_20000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:05 en-vi_step_2000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:19 en-vi_step_21000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:29 en-vi_step_22000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:40 en-vi_step_23000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:50 en-vi_step_24000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:00 en-vi_step_25000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:10 en-vi_step_26000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:20 en-vi_step_27000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:31 en-vi_step_28000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:41 en-vi_step_29000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:51 en-vi_step_30000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:15 en-vi_step_3000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:25 en-vi_step_4000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:35 en-vi_step_5000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:46 en-vi_step_6000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:56 en-vi_step_7000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:06 en-vi_step_8000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:16 en-vi_step_9000.pt\n","\n","model/:\n","total 29997315\n","-rw------- 1 root root 1023908031 Apr 10 09:27 en-vi_step_10000.pt\n","-rw------- 1 root root 1023908031 Apr 10 07:55 en-vi_step_1000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:37 en-vi_step_11000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:47 en-vi_step_12000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:57 en-vi_step_13000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:07 en-vi_step_14000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:18 en-vi_step_15000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:28 en-vi_step_16000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:38 en-vi_step_17000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:48 en-vi_step_18000.pt\n","-rw------- 1 root root 1023908031 Apr 10 10:59 en-vi_step_19000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:09 en-vi_step_20000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:05 en-vi_step_2000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:19 en-vi_step_21000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:29 en-vi_step_22000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:40 en-vi_step_23000.pt\n","-rw------- 1 root root 1023908031 Apr 10 11:50 en-vi_step_24000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:00 en-vi_step_25000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:10 en-vi_step_26000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:20 en-vi_step_27000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:31 en-vi_step_28000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:41 en-vi_step_29000.pt\n","-rw------- 1 root root 1023908031 Apr 10 12:51 en-vi_step_30000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:15 en-vi_step_3000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:25 en-vi_step_4000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:35 en-vi_step_5000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:46 en-vi_step_6000.pt\n","-rw------- 1 root root 1023908031 Apr 10 08:56 en-vi_step_7000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:06 en-vi_step_8000.pt\n","-rw------- 1 root root 1023908031 Apr 10 09:16 en-vi_step_9000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1S26AN4rHUAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618062731690,"user_tz":-420,"elapsed":22098126,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bf1988f3-6aa7-4b27-ac38-2908ff5433c8"},"source":["!onmt_translate -model model/en-vi_step_30000.pt -src en_test -tgt vi_test -output predict.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-10 12:51:42,494 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [10, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-10 13:50:10,975 INFO] PRED AVG SCORE: -0.4277, PRED PPL: 1.5338\n","[2021-04-10 13:50:10,975 INFO] GOLD AVG SCORE: -3.6347, GOLD PPL: 37.8907\n","[2021-04-10 13:50:10,999 INFO] Translating shard 1.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [3, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-10 13:52:10,572 INFO] PRED AVG SCORE: -0.4353, PRED PPL: 1.5454\n","[2021-04-10 13:52:10,572 INFO] GOLD AVG SCORE: -3.5991, GOLD PPL: 36.5663\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPGsYFs_XpAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618062731692,"user_tz":-420,"elapsed":22098126,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"1a78be11-bccd-413f-f3b2-33815ffb827a"},"source":["!tail vi_test"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Cà vạt thì loè loẹt .\n","và lí do là bởi vì có 2 lí do , theo tôi nghĩ\n","Ông thích nói về thiên tài tâm linh của lứa tuổi .\n","Chúng tôi đều là người Triều Tiên , nhưng đã trở nên rất khác nhau do hậu quả của 67 năm bị chia cắt .\n","Đó là cách bạn xử lý một vấn đề khi bạn nhìn thấy chúng và đó không chỉ là việc than phiền về vấn đề đó .\n","Tham vọng của các bạn được thoã mãn , nó rất đẹp .\n","Không có thứ nào trong những điều trên thực sự hữu ích bởi vì bạn đang điều trị những triệu chứng chứ không phải nguyên nhân của các vấn đề cơ bản ở Phi Châu .\n","Nhưng hiện nay nhiều người sống đến 90 hay 100 tuổi , trừ khi họ bắt tay quá nhiều hay làm những điều đại loại thế .\n","Nhưng quý vị phải có những công cụ đúng .\n","Những điều này là một phần cuộc đời ông và là những gì ông còn nhớ\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rf5W-T8MzRK3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618062752021,"user_tz":-420,"elapsed":22118453,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"58d54219-bfdd-4a28-9e96-e6f2320825b9"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 29, done.\u001b[K\n","remote: Counting objects: 100% (29/29), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 17114 (delta 7), reused 8 (delta 4), pack-reused 17085\u001b[K\n","Receiving objects: 100% (17114/17114), 273.05 MiB | 20.02 MiB/s, done.\n","Resolving deltas: 100% (12323/12323), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Iw_Gma2gz0PK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618062752022,"user_tz":-420,"elapsed":22118453,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"33f11764-8f08-4363-ea0b-8d2fadf11fbf"},"source":["!ls -al"],"execution_count":12,"outputs":[{"output_type":"stream","text":["total 93793\n","drwx------  2 root root     4096 Apr 10 07:44 data_bin\n","-rw-------  1 root root   996149 Apr 10 07:32 en_test\n","-rw-------  1 root root  8024744 Apr 10 07:32 en_train\n","-rw-------  1 root root  8204550 Apr 10 07:32 en_train_EM_0.8\n","-rw-------  1 root root  8103017 Apr 10 07:32 en_train_EM_0.85\n","-rw-------  1 root root  8053185 Apr 10 07:32 en_train_EM_0.9\n","-rw-------  1 root root  8031617 Apr 10 07:32 en_train_EM_0.95\n","-rw-------  1 root root  3323998 Apr 10 07:32 en_train_EM_factor_0.8\n","-rw-------  1 root root  3281394 Apr 10 07:32 en_train_EM_factor_0.85\n","-rw-------  1 root root  3260286 Apr 10 07:32 en_train_EM_factor_0.9\n","-rw-------  1 root root  3250950 Apr 10 07:32 en_train_EM_factor_0.95\n","-rw-------  1 root root   860286 Apr 10 07:32 en_train_EM_score_0.8\n","-rw-------  1 root root   860286 Apr 10 07:32 en_train_EM_score_0.85\n","-rw-------  1 root root   860286 Apr 10 07:32 en_train_EM_score_0.9\n","-rw-------  1 root root   860286 Apr 10 07:32 en_train_EM_score_0.95\n","-rw-------  1 root root  1000856 Apr 10 07:32 en_valid\n","-rw-------  1 root root 22512408 Apr 10 07:44 en_vi_iwslt_sent2vec.tar.gz\n","drwx------  2 root root     4096 Apr 10 12:51 model\n","drwx------ 11 root root     4096 Apr 10 13:52 OpenNMT-py\n","drwx------  2 root root     4096 Apr 10 07:44 output\n","-rw-------  1 root root  1207941 Apr 10 13:52 predict.txt\n","-rw-------  1 root root  1327417 Apr 10 07:32 vi_test\n","-rw-------  1 root root 10671354 Apr 10 07:32 vi_train\n","-rw-------  1 root root  1330789 Apr 10 07:32 vi_valid\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7T7xCaDdR469","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618062754235,"user_tz":-420,"elapsed":22120664,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a6cae65e-afb2-484a-b2b5-238952d21809"},"source":["!perl OpenNMT-py/tools/multi-bleu.perl vi_test < predict.txt"],"execution_count":13,"outputs":[{"output_type":"stream","text":["BLEU = 23.74, 58.9/33.0/19.4/11.8 (BP=0.919, ratio=0.923, hyp_len=225300, ref_len=244219)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uLGJCSK_Qih1","executionInfo":{"status":"ok","timestamp":1618062754236,"user_tz":-420,"elapsed":22120663,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}}},"source":[""],"execution_count":13,"outputs":[]}]}