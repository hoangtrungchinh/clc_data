{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OPUS-OpenNMT-sent2vec-0.9-20210412-0416 BLEU 16.60.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267170848,"user_tz":-420,"elapsed":17896,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"95fe805b-98f2-4d44-d8e1-ddc1f39a1f56"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267177174,"user_tz":-420,"elapsed":1813,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"3f18778a-6bb8-4ace-f170-2b0c2c412d94"},"source":["# import os\n","# path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"OPUS-OpenNMT-sent2vec-0.9-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-sent2vec-0.9-20210412-0416'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-sent2vec-0.9-20210412-0416\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267178995,"user_tz":-420,"elapsed":771,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"8ad21456-de62-487a-e965-dcc6dc025a28"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mon Apr 12 22:39:38 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267194776,"user_tz":-420,"elapsed":13928,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"0e76017d-b9fb-4802-92fc-8bc8507df6a3"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n","\u001b[?25hCollecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Collecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n","\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/63/17c6ac0d8a0cfa5ff7257e52edb6759d12dc266392f6c97f5c65c0c7238c/pyonmttok-1.25.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 15.9MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.3)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.8.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=1cbfc8f1ce82c3c856974ccbb23c0eb8c570ce92baf6cafff326fbd91121e485\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: torchtext, configargparse, waitress, pyonmttok, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.25.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 102.8MB/s \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618201003568,"user_tz":-420,"elapsed":56521,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bdbc25fd-4548-406f-e6b4-1d1cbd606ca2"},"source":["!wget https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_sent2vec.tar.gz\n","!mkdir data_bin\n","!tar -xvf 'opus_sent2vec.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-12 04:16:32--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_sent2vec.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 94255466 (90M) [application/octet-stream]\n","Saving to: ‘opus_sent2vec.tar.gz’\n","\n","opus_sent2vec.tar.g 100%[===================>]  89.89M  59.6MB/s    in 1.5s    \n","\n","2021-04-12 04:16:38 (59.6 MB/s) - ‘opus_sent2vec.tar.gz’ saved [94255466/94255466]\n","\n","en_train_EM_score_0.95\n","vi_valid\n","en_train_EM_0.95\n","en_train_EM_factor_0.85\n","en_train_EM_score_0.8\n","vi_train\n","en_train_EM_factor_0.8\n","en_train_EM_0.8\n","en_valid\n","en_train_EM_factor_0.95\n","en_train\n","en_train_EM_score_0.85\n","vi_test\n","en_train_EM_0.85\n","en_train_EM_score_0.9\n","en_test\n","en_train_EM_factor_0.9\n","en_train_EM_0.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618201067998,"user_tz":-420,"elapsed":120949,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"f9fb4f1b-6b9b-4517-93a6-9983371dd71b"},"source":["!mkdir -p output\n","!onmt_preprocess -train_src 'en_train_EM_0.9' \\\\\n","-train_tgt 'vi_train' \\\\\n","-valid_src 'en_valid' \\\\\n","-valid_tgt 'vi_valid' \\\\\n","-save_data 'output/en-vi' "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-12 04:16:46,666 INFO] Extracting features...\n","[2021-04-12 04:16:46,672 INFO]  * number of source features: 0.\n","[2021-04-12 04:16:46,673 INFO]  * number of target features: 0.\n","[2021-04-12 04:16:46,673 INFO] Building `Fields` object...\n","[2021-04-12 04:16:46,673 INFO] Building & saving training data...\n","[2021-04-12 04:16:47,972 INFO] Building shard 0.\n","[2021-04-12 04:17:21,451 INFO]  * saving 0th train data shard to output/en-vi.train.0.pt.\n","[2021-04-12 04:17:40,293 INFO]  * tgt vocab size: 50004.\n","[2021-04-12 04:17:40,639 INFO]  * src vocab size: 50002.\n","[2021-04-12 04:17:41,307 INFO] Building & saving validation data...\n","[2021-04-12 04:17:41,900 INFO] Building shard 0.\n","[2021-04-12 04:17:44,249 INFO]  * saving 0th valid data shard to output/en-vi.valid.0.pt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYyQECmi0TX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618226032349,"user_tz":-420,"elapsed":13807074,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"f62d6595-be00-43ea-c573-36c2441e10d4"},"source":["!mkdir -p model\n","!onmt_train -data 'output/en-vi' \\\\\n","-save_model 'model/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30000  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 1000 -save_checkpoint_steps 1000 \\\\\n","-report_every 1000 -world_size 1 -gpu_ranks 0"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-12 04:17:50,988 INFO]  * src vocab size = 50002\n","[2021-04-12 04:17:50,988 INFO]  * tgt vocab size = 50004\n","[2021-04-12 04:17:50,988 INFO] Building model...\n","[2021-04-12 04:17:59,355 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50002, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50004, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=50004, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-12 04:17:59,377 INFO] encoder: 44516352\n","[2021-04-12 04:17:59,377 INFO] decoder: 76479316\n","[2021-04-12 04:17:59,377 INFO] * number of parameters: 120995668\n","[2021-04-12 04:17:59,383 INFO] Starting training on GPU: [0]\n","[2021-04-12 04:17:59,383 INFO] Start training loop and validate every 1000 steps...\n","[2021-04-12 04:17:59,383 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:18:18,636 INFO] number of examples: 802830\n","[2021-04-12 04:29:34,098 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:29:57,105 INFO] number of examples: 802830\n","[2021-04-12 04:30:20,160 INFO] Step 1000/30000; acc:  13.53; ppl: 555.34; xent: 6.32; lr: 0.00012; 7385/9488 tok/s;    741 sec\n","[2021-04-12 04:30:20,161 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:30:21,642 INFO] number of examples: 100400\n","[2021-04-12 04:31:57,355 INFO] Validation perplexity: 278.662\n","[2021-04-12 04:31:57,356 INFO] Validation accuracy: 18.9657\n","[2021-04-12 04:31:58,903 INFO] Saving checkpoint model/en-vi_step_1000.pt\n","[2021-04-12 04:43:04,015 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:43:38,089 INFO] number of examples: 802830\n","[2021-04-12 04:44:17,893 INFO] Step 2000/30000; acc:  27.49; ppl: 70.33; xent: 4.25; lr: 0.00025; 6532/8381 tok/s;   1579 sec\n","[2021-04-12 04:44:17,895 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:44:19,439 INFO] number of examples: 100400\n","[2021-04-12 04:45:54,476 INFO] Validation perplexity: 58.225\n","[2021-04-12 04:45:54,476 INFO] Validation accuracy: 35.6497\n","[2021-04-12 04:45:56,016 INFO] Saving checkpoint model/en-vi_step_2000.pt\n","[2021-04-12 04:56:40,107 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 04:57:06,391 INFO] number of examples: 802830\n","[2021-04-12 04:58:01,074 INFO] Step 3000/30000; acc:  38.81; ppl: 26.18; xent: 3.26; lr: 0.00037; 6647/8527 tok/s;   2402 sec\n","[2021-04-12 04:58:01,075 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 04:58:02,611 INFO] number of examples: 100400\n","[2021-04-12 04:59:37,724 INFO] Validation perplexity: 28.1332\n","[2021-04-12 04:59:37,724 INFO] Validation accuracy: 42.6388\n","[2021-04-12 04:59:39,255 INFO] Saving checkpoint model/en-vi_step_3000.pt\n","[2021-04-12 05:10:07,947 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:10:34,964 INFO] number of examples: 802830\n","[2021-04-12 05:11:45,672 INFO] Step 4000/30000; acc:  43.90; ppl: 16.48; xent: 2.80; lr: 0.00049; 6627/8525 tok/s;   3226 sec\n","[2021-04-12 05:11:45,673 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:11:47,254 INFO] number of examples: 100400\n","[2021-04-12 05:13:22,413 INFO] Validation perplexity: 20.8332\n","[2021-04-12 05:13:22,413 INFO] Validation accuracy: 45.7496\n","[2021-04-12 05:13:23,934 INFO] Saving checkpoint model/en-vi_step_4000.pt\n","[2021-04-12 05:23:38,339 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:24:06,276 INFO] number of examples: 802830\n","[2021-04-12 05:25:32,785 INFO] Step 5000/30000; acc:  46.72; ppl: 12.85; xent: 2.55; lr: 0.00062; 6621/8484 tok/s;   4053 sec\n","[2021-04-12 05:25:32,786 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:25:34,341 INFO] number of examples: 100400\n","[2021-04-12 05:27:09,246 INFO] Validation perplexity: 18.6527\n","[2021-04-12 05:27:09,246 INFO] Validation accuracy: 46.859\n","[2021-04-12 05:27:10,784 INFO] Saving checkpoint model/en-vi_step_5000.pt\n","[2021-04-12 05:37:08,298 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:37:36,044 INFO] number of examples: 802830\n","[2021-04-12 05:39:18,662 INFO] Step 6000/30000; acc:  48.55; ppl: 10.99; xent: 2.40; lr: 0.00074; 6621/8512 tok/s;   4879 sec\n","[2021-04-12 05:39:18,664 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:39:20,246 INFO] number of examples: 100400\n","[2021-04-12 05:40:55,267 INFO] Validation perplexity: 17.3912\n","[2021-04-12 05:40:55,267 INFO] Validation accuracy: 47.6684\n","[2021-04-12 05:40:56,808 INFO] Saving checkpoint model/en-vi_step_6000.pt\n","[2021-04-12 05:50:38,177 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 05:51:11,589 INFO] number of examples: 802830\n","[2021-04-12 05:53:09,301 INFO] Step 7000/30000; acc:  49.91; ppl:  9.82; xent: 2.28; lr: 0.00086; 6581/8451 tok/s;   5710 sec\n","[2021-04-12 05:53:09,303 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 05:53:10,912 INFO] number of examples: 100400\n","[2021-04-12 05:54:45,867 INFO] Validation perplexity: 17.278\n","[2021-04-12 05:54:45,867 INFO] Validation accuracy: 47.7701\n","[2021-04-12 05:54:47,410 INFO] Saving checkpoint model/en-vi_step_7000.pt\n","[2021-04-12 06:04:13,802 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:04:37,194 INFO] number of examples: 802830\n","[2021-04-12 06:06:50,735 INFO] Step 8000/30000; acc:  51.16; ppl:  8.92; xent: 2.19; lr: 0.00099; 6654/8552 tok/s;   6531 sec\n","[2021-04-12 06:06:50,736 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:06:56,696 INFO] number of examples: 100400\n","[2021-04-12 06:08:31,620 INFO] Validation perplexity: 16.6143\n","[2021-04-12 06:08:31,620 INFO] Validation accuracy: 48.3311\n","[2021-04-12 06:08:33,182 INFO] Saving checkpoint model/en-vi_step_8000.pt\n","[2021-04-12 06:17:45,012 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:18:13,617 INFO] number of examples: 802830\n","[2021-04-12 06:20:43,026 INFO] Step 9000/30000; acc:  52.66; ppl:  8.04; xent: 2.09; lr: 0.00093; 6576/8444 tok/s;   7364 sec\n","[2021-04-12 06:20:43,027 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:20:44,610 INFO] number of examples: 100400\n","[2021-04-12 06:22:19,706 INFO] Validation perplexity: 16.2262\n","[2021-04-12 06:22:19,706 INFO] Validation accuracy: 48.8565\n","[2021-04-12 06:22:21,241 INFO] Saving checkpoint model/en-vi_step_9000.pt\n","[2021-04-12 06:31:15,954 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:31:43,432 INFO] number of examples: 802830\n","[2021-04-12 06:34:28,659 INFO] Step 10000/30000; acc:  54.61; ppl:  7.11; xent: 1.96; lr: 0.00088; 6611/8501 tok/s;   8189 sec\n","[2021-04-12 06:34:28,661 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:34:36,329 INFO] number of examples: 100400\n","[2021-04-12 06:36:11,504 INFO] Validation perplexity: 16.0293\n","[2021-04-12 06:36:11,505 INFO] Validation accuracy: 49.2192\n","[2021-04-12 06:36:13,060 INFO] Saving checkpoint model/en-vi_step_10000.pt\n","[2021-04-12 06:44:52,595 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:45:28,608 INFO] number of examples: 802830\n","[2021-04-12 06:48:29,814 INFO] Step 11000/30000; acc:  56.43; ppl:  6.39; xent: 1.86; lr: 0.00084; 6501/8349 tok/s;   9030 sec\n","[2021-04-12 06:48:29,815 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 06:48:31,404 INFO] number of examples: 100400\n","[2021-04-12 06:50:06,435 INFO] Validation perplexity: 15.8214\n","[2021-04-12 06:50:06,436 INFO] Validation accuracy: 49.6035\n","[2021-04-12 06:50:07,958 INFO] Saving checkpoint model/en-vi_step_11000.pt\n","[2021-04-12 06:58:30,998 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 06:58:58,443 INFO] number of examples: 802830\n","[2021-04-12 07:02:21,171 INFO] Step 12000/30000; acc:  58.09; ppl:  5.83; xent: 1.76; lr: 0.00081; 6577/8454 tok/s;   9862 sec\n","[2021-04-12 07:02:21,172 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:02:22,775 INFO] number of examples: 100400\n","[2021-04-12 07:03:57,653 INFO] Validation perplexity: 16.1003\n","[2021-04-12 07:03:57,653 INFO] Validation accuracy: 49.6818\n","[2021-04-12 07:03:59,182 INFO] Saving checkpoint model/en-vi_step_12000.pt\n","[2021-04-12 07:12:06,796 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:12:42,295 INFO] number of examples: 802830\n","[2021-04-12 07:16:15,168 INFO] Step 13000/30000; acc:  59.67; ppl:  5.36; xent: 1.68; lr: 0.00078; 6550/8427 tok/s;  10696 sec\n","[2021-04-12 07:16:15,169 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:16:22,994 INFO] number of examples: 100400\n","[2021-04-12 07:17:58,122 INFO] Validation perplexity: 16.4589\n","[2021-04-12 07:17:58,122 INFO] Validation accuracy: 50.0763\n","[2021-04-12 07:17:59,674 INFO] Saving checkpoint model/en-vi_step_13000.pt\n","[2021-04-12 07:25:51,038 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:26:27,955 INFO] number of examples: 802830\n","[2021-04-12 07:30:16,626 INFO] Step 14000/30000; acc:  61.06; ppl:  4.99; xent: 1.61; lr: 0.00075; 6486/8359 tok/s;  11537 sec\n","[2021-04-12 07:30:16,627 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:30:18,233 INFO] number of examples: 100400\n","[2021-04-12 07:31:52,866 INFO] Validation perplexity: 16.316\n","[2021-04-12 07:31:52,866 INFO] Validation accuracy: 50.0503\n","[2021-04-12 07:31:54,401 INFO] Saving checkpoint model/en-vi_step_14000.pt\n","[2021-04-12 07:39:29,513 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:39:58,750 INFO] number of examples: 802830\n","[2021-04-12 07:44:02,304 INFO] Step 15000/30000; acc:  62.39; ppl:  4.66; xent: 1.54; lr: 0.00072; 6625/8495 tok/s;  12363 sec\n","[2021-04-12 07:44:02,305 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:44:03,908 INFO] number of examples: 100400\n","[2021-04-12 07:45:38,749 INFO] Validation perplexity: 16.8653\n","[2021-04-12 07:45:38,749 INFO] Validation accuracy: 50.0968\n","[2021-04-12 07:45:40,245 INFO] Saving checkpoint model/en-vi_step_15000.pt\n","[2021-04-12 07:53:00,277 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 07:53:29,065 INFO] number of examples: 802830\n","[2021-04-12 07:57:48,480 INFO] Step 16000/30000; acc:  63.58; ppl:  4.40; xent: 1.48; lr: 0.00070; 6621/8496 tok/s;  13189 sec\n","[2021-04-12 07:57:48,481 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 07:57:50,081 INFO] number of examples: 100400\n","[2021-04-12 07:59:25,153 INFO] Validation perplexity: 16.8609\n","[2021-04-12 07:59:25,153 INFO] Validation accuracy: 50.3202\n","[2021-04-12 07:59:26,678 INFO] Saving checkpoint model/en-vi_step_16000.pt\n","[2021-04-12 08:06:32,207 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:07:00,281 INFO] number of examples: 802830\n","[2021-04-12 08:11:35,188 INFO] Step 17000/30000; acc:  64.67; ppl:  4.18; xent: 1.43; lr: 0.00068; 6614/8494 tok/s;  14016 sec\n","[2021-04-12 08:11:35,190 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:11:42,983 INFO] number of examples: 100400\n","[2021-04-12 08:13:17,957 INFO] Validation perplexity: 17.5866\n","[2021-04-12 08:13:17,957 INFO] Validation accuracy: 50.3125\n","[2021-04-12 08:13:19,503 INFO] Saving checkpoint model/en-vi_step_17000.pt\n","[2021-04-12 08:20:08,459 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:20:45,116 INFO] number of examples: 802830\n","[2021-04-12 08:25:35,909 INFO] Step 18000/30000; acc:  65.66; ppl:  3.98; xent: 1.38; lr: 0.00066; 6499/8353 tok/s;  14857 sec\n","[2021-04-12 08:25:35,910 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:25:37,513 INFO] number of examples: 100400\n","[2021-04-12 08:27:12,460 INFO] Validation perplexity: 18.167\n","[2021-04-12 08:27:12,460 INFO] Validation accuracy: 50.28\n","[2021-04-12 08:27:13,968 INFO] Saving checkpoint model/en-vi_step_18000.pt\n","[2021-04-12 08:33:47,136 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:34:16,302 INFO] number of examples: 802830\n","[2021-04-12 08:39:22,748 INFO] Step 19000/30000; acc:  66.61; ppl:  3.81; xent: 1.34; lr: 0.00064; 6612/8506 tok/s;  15683 sec\n","[2021-04-12 08:39:22,749 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:39:24,349 INFO] number of examples: 100400\n","[2021-04-12 08:40:59,313 INFO] Validation perplexity: 18.0566\n","[2021-04-12 08:40:59,313 INFO] Validation accuracy: 50.3645\n","[2021-04-12 08:41:00,828 INFO] Saving checkpoint model/en-vi_step_19000.pt\n","[2021-04-12 08:47:18,088 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 08:47:46,664 INFO] number of examples: 802830\n","[2021-04-12 08:53:08,818 INFO] Step 20000/30000; acc:  67.52; ppl:  3.66; xent: 1.30; lr: 0.00062; 6619/8497 tok/s;  16509 sec\n","[2021-04-12 08:53:08,820 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 08:53:16,824 INFO] number of examples: 100400\n","[2021-04-12 08:54:51,859 INFO] Validation perplexity: 18.8018\n","[2021-04-12 08:54:51,859 INFO] Validation accuracy: 50.5285\n","[2021-04-12 08:54:53,385 INFO] Saving checkpoint model/en-vi_step_20000.pt\n","[2021-04-12 09:00:55,526 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 09:01:32,882 INFO] number of examples: 802830\n","[2021-04-12 09:07:11,266 INFO] Step 21000/30000; acc:  68.31; ppl:  3.53; xent: 1.26; lr: 0.00061; 6499/8332 tok/s;  17352 sec\n","[2021-04-12 09:07:11,268 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 09:07:12,861 INFO] number of examples: 100400\n","[2021-04-12 09:08:47,764 INFO] Validation perplexity: 19.0044\n","[2021-04-12 09:08:47,764 INFO] Validation accuracy: 50.5305\n","[2021-04-12 09:08:49,298 INFO] Saving checkpoint model/en-vi_step_21000.pt\n","[2021-04-12 09:14:35,512 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 09:15:05,612 INFO] number of examples: 802830\n","[2021-04-12 09:20:59,350 INFO] Step 22000/30000; acc:  69.06; ppl:  3.41; xent: 1.23; lr: 0.00060; 6601/8485 tok/s;  18180 sec\n","[2021-04-12 09:20:59,351 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 09:21:00,942 INFO] number of examples: 100400\n","[2021-04-12 09:22:36,149 INFO] Validation perplexity: 19.5418\n","[2021-04-12 09:22:36,149 INFO] Validation accuracy: 50.3559\n","[2021-04-12 09:22:37,702 INFO] Saving checkpoint model/en-vi_step_22000.pt\n","[2021-04-12 09:28:07,867 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 09:28:31,762 INFO] number of examples: 802830\n","[2021-04-12 09:34:41,653 INFO] Step 23000/30000; acc:  69.83; ppl:  3.30; xent: 1.19; lr: 0.00058; 6641/8547 tok/s;  19002 sec\n","[2021-04-12 09:34:41,654 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 09:34:49,405 INFO] number of examples: 100400\n","[2021-04-12 09:36:24,356 INFO] Validation perplexity: 19.1564\n","[2021-04-12 09:36:24,356 INFO] Validation accuracy: 50.6378\n","[2021-04-12 09:36:25,882 INFO] Saving checkpoint model/en-vi_step_23000.pt\n","[2021-04-12 09:41:40,482 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 09:42:17,121 INFO] number of examples: 802830\n","[2021-04-12 09:48:42,695 INFO] Step 24000/30000; acc:  70.53; ppl:  3.20; xent: 1.16; lr: 0.00057; 6501/8346 tok/s;  19843 sec\n","[2021-04-12 09:48:42,696 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 09:48:44,302 INFO] number of examples: 100400\n","[2021-04-12 09:50:19,319 INFO] Validation perplexity: 20.2257\n","[2021-04-12 09:50:19,319 INFO] Validation accuracy: 50.5112\n","[2021-04-12 09:50:20,850 INFO] Saving checkpoint model/en-vi_step_24000.pt\n","[2021-04-12 09:55:19,851 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 09:55:48,253 INFO] number of examples: 802830\n","[2021-04-12 10:02:29,246 INFO] Step 25000/30000; acc:  71.17; ppl:  3.12; xent: 1.14; lr: 0.00056; 6622/8501 tok/s;  20670 sec\n","[2021-04-12 10:02:29,247 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 10:02:37,260 INFO] number of examples: 100400\n","[2021-04-12 10:04:11,957 INFO] Validation perplexity: 19.8691\n","[2021-04-12 10:04:11,957 INFO] Validation accuracy: 50.6385\n","[2021-04-12 10:04:13,511 INFO] Saving checkpoint model/en-vi_step_25000.pt\n","[2021-04-12 10:08:57,772 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 10:09:35,298 INFO] number of examples: 802830\n","[2021-04-12 10:16:31,934 INFO] Step 26000/30000; acc:  71.80; ppl:  3.04; xent: 1.11; lr: 0.00055; 6488/8330 tok/s;  21513 sec\n","[2021-04-12 10:16:31,935 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 10:16:33,553 INFO] number of examples: 100400\n","[2021-04-12 10:18:08,508 INFO] Validation perplexity: 20.9065\n","[2021-04-12 10:18:08,508 INFO] Validation accuracy: 50.6127\n","[2021-04-12 10:18:10,063 INFO] Saving checkpoint model/en-vi_step_26000.pt\n","[2021-04-12 10:22:38,051 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 10:23:01,413 INFO] number of examples: 802830\n","[2021-04-12 10:30:19,457 INFO] Step 27000/30000; acc:  72.37; ppl:  2.97; xent: 1.09; lr: 0.00054; 6601/8499 tok/s;  22340 sec\n","[2021-04-12 10:30:19,458 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 10:30:21,084 INFO] number of examples: 100400\n","[2021-04-12 10:31:56,204 INFO] Validation perplexity: 20.5931\n","[2021-04-12 10:31:56,204 INFO] Validation accuracy: 50.6109\n","[2021-04-12 10:31:57,741 INFO] Saving checkpoint model/en-vi_step_27000.pt\n","[2021-04-12 10:36:09,778 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 10:36:45,342 INFO] number of examples: 802830\n","[2021-04-12 10:44:12,807 INFO] Step 28000/30000; acc:  72.96; ppl:  2.89; xent: 1.06; lr: 0.00053; 6560/8418 tok/s;  23173 sec\n","[2021-04-12 10:44:12,808 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 10:44:20,689 INFO] number of examples: 100400\n","[2021-04-12 10:45:55,827 INFO] Validation perplexity: 22.4528\n","[2021-04-12 10:45:55,827 INFO] Validation accuracy: 50.4501\n","[2021-04-12 10:45:57,367 INFO] Saving checkpoint model/en-vi_step_28000.pt\n","[2021-04-12 10:49:54,113 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 10:50:31,036 INFO] number of examples: 802830\n","[2021-04-12 10:58:13,991 INFO] Step 29000/30000; acc:  73.48; ppl:  2.83; xent: 1.04; lr: 0.00052; 6498/8356 tok/s;  24015 sec\n","[2021-04-12 10:58:13,993 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 10:58:15,600 INFO] number of examples: 100400\n","[2021-04-12 10:59:50,678 INFO] Validation perplexity: 21.724\n","[2021-04-12 10:59:50,679 INFO] Validation accuracy: 50.5605\n","[2021-04-12 10:59:52,208 INFO] Saving checkpoint model/en-vi_step_29000.pt\n","[2021-04-12 11:03:34,105 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-12 11:04:03,675 INFO] number of examples: 802830\n","[2021-04-12 11:12:02,314 INFO] Step 30000/30000; acc:  74.00; ppl:  2.77; xent: 1.02; lr: 0.00051; 6603/8475 tok/s;  24843 sec\n","[2021-04-12 11:12:02,316 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-12 11:12:03,934 INFO] number of examples: 100400\n","[2021-04-12 11:13:38,811 INFO] Validation perplexity: 22.5893\n","[2021-04-12 11:13:38,811 INFO] Validation accuracy: 50.5446\n","[2021-04-12 11:13:40,326 INFO] Saving checkpoint model/en-vi_step_30000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XOEz-j6IbNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618226032350,"user_tz":-420,"elapsed":36,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"da28aa86-7122-432c-8aca-2035dd7087da"},"source":["!ls -al model model/"],"execution_count":8,"outputs":[{"output_type":"stream","text":["model:\n","total 43399635\n","-rw------- 1 root root 1481374079 Apr 12 06:36 en-vi_step_10000.pt\n","-rw------- 1 root root 1481374079 Apr 12 04:32 en-vi_step_1000.pt\n","-rw------- 1 root root 1481374079 Apr 12 06:50 en-vi_step_11000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:04 en-vi_step_12000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:18 en-vi_step_13000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:32 en-vi_step_14000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:45 en-vi_step_15000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:59 en-vi_step_16000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:13 en-vi_step_17000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:27 en-vi_step_18000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:41 en-vi_step_19000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:54 en-vi_step_20000.pt\n","-rw------- 1 root root 1481374079 Apr 12 04:46 en-vi_step_2000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:08 en-vi_step_21000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:22 en-vi_step_22000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:36 en-vi_step_23000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:50 en-vi_step_24000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:04 en-vi_step_25000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:18 en-vi_step_26000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:32 en-vi_step_27000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:46 en-vi_step_28000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:59 en-vi_step_29000.pt\n","-rw------- 1 root root 1481374079 Apr 12 11:13 en-vi_step_30000.pt\n","-rw------- 1 root root 1481374079 Apr 12 04:59 en-vi_step_3000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:13 en-vi_step_4000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:27 en-vi_step_5000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:41 en-vi_step_6000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:54 en-vi_step_7000.pt\n","-rw------- 1 root root 1481374079 Apr 12 06:08 en-vi_step_8000.pt\n","-rw------- 1 root root 1481374079 Apr 12 06:22 en-vi_step_9000.pt\n","\n","model/:\n","total 43399635\n","-rw------- 1 root root 1481374079 Apr 12 06:36 en-vi_step_10000.pt\n","-rw------- 1 root root 1481374079 Apr 12 04:32 en-vi_step_1000.pt\n","-rw------- 1 root root 1481374079 Apr 12 06:50 en-vi_step_11000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:04 en-vi_step_12000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:18 en-vi_step_13000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:32 en-vi_step_14000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:45 en-vi_step_15000.pt\n","-rw------- 1 root root 1481374079 Apr 12 07:59 en-vi_step_16000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:13 en-vi_step_17000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:27 en-vi_step_18000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:41 en-vi_step_19000.pt\n","-rw------- 1 root root 1481374079 Apr 12 08:54 en-vi_step_20000.pt\n","-rw------- 1 root root 1481374079 Apr 12 04:46 en-vi_step_2000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:08 en-vi_step_21000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:22 en-vi_step_22000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:36 en-vi_step_23000.pt\n","-rw------- 1 root root 1481374079 Apr 12 09:50 en-vi_step_24000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:04 en-vi_step_25000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:18 en-vi_step_26000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:32 en-vi_step_27000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:46 en-vi_step_28000.pt\n","-rw------- 1 root root 1481374079 Apr 12 10:59 en-vi_step_29000.pt\n","-rw------- 1 root root 1481374079 Apr 12 11:13 en-vi_step_30000.pt\n","-rw------- 1 root root 1481374079 Apr 12 04:59 en-vi_step_3000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:13 en-vi_step_4000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:27 en-vi_step_5000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:41 en-vi_step_6000.pt\n","-rw------- 1 root root 1481374079 Apr 12 05:54 en-vi_step_7000.pt\n","-rw------- 1 root root 1481374079 Apr 12 06:08 en-vi_step_8000.pt\n","-rw------- 1 root root 1481374079 Apr 12 06:22 en-vi_step_9000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1S26AN4rHUAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618237773953,"user_tz":-420,"elapsed":11741626,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"01906c20-a41e-4ae8-84bc-ef108b240202"},"source":["!onmt_translate -model model/en-vi_step_30000.pt -src en_test -tgt vi_test -output predict.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-12 11:14:01,180 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [10, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-12 11:33:31,083 INFO] PRED AVG SCORE: -0.6330, PRED PPL: 1.8832\n","[2021-04-12 11:33:31,083 INFO] GOLD AVG SCORE: -3.1135, GOLD PPL: 22.4986\n","[2021-04-12 11:33:31,107 INFO] Translating shard 1.\n","tcmalloc: large alloc 1518125056 bytes == 0x5570da3f8000 @  0x7fc640fbbb6b 0x7fc640fdb379 0x7fc5ed71425e 0x7fc5ed7159d2 0x7fc62a3f88e6 0x7fc62a85add9 0x7fc62ad6577a 0x7fc62ad30ef9 0x7fc62ace7657 0x7fc62ab8b929 0x7fc62a6a3516 0x7fc62ad667af 0x7fc62ab15846 0x7fc62ab1ae6f 0x7fc62c3febcc 0x7fc62c3ff13f 0x7fc62af67a86 0x7fc62af6bcaf 0x7fc62a69516a 0x7fc62a695b3a 0x7fc62ae7a7f8 0x7fc62ae7a83f 0x7fc62ab15846 0x7fc62ab1b22f 0x7fc62a6790b1 0x7fc62ae794c0 0x7fc62ae9c05d 0x7fc62ac6ca59 0x7fc63c18a8de 0x55705dde1050 0x55705dde0de0\n","tcmalloc: large alloc 1518125056 bytes == 0x557134bc4000 @  0x7fc640fbbb6b 0x7fc640fdb379 0x7fc5ed71425e 0x7fc5ed7159d2 0x7fc62a3f88e6 0x7fc62a85add9 0x7fc62ad6577a 0x7fc62ad30ef9 0x7fc62ace7657 0x7fc62ab8b929 0x7fc62a8646a2 0x7fc62a7f45c5 0x7fc62ad66573 0x7fc62acd6904 0x7fc62ab40f09 0x7fc62c388444 0x7fc62c388783 0x7fc62acd6904 0x7fc62ab40f09 0x7fc62a7ed5d0 0x7fc62ae7a0e0 0x7fc62ae7a132 0x7fc62ace4054 0x7fc62af88735 0x7fc63bee7f4f 0x55705dde1050 0x55705ded299d 0x55705de54fe9 0x55705de4fb0e 0x55705dde277a 0x55705de5186a\n","[2021-04-12 11:52:51,314 INFO] PRED AVG SCORE: -0.6317, PRED PPL: 1.8807\n","[2021-04-12 11:52:51,314 INFO] GOLD AVG SCORE: -3.1211, GOLD PPL: 22.6722\n","[2021-04-12 11:52:51,335 INFO] Translating shard 2.\n","[2021-04-12 12:12:30,038 INFO] PRED AVG SCORE: -0.6294, PRED PPL: 1.8765\n","[2021-04-12 12:12:30,038 INFO] GOLD AVG SCORE: -3.1226, GOLD PPL: 22.7045\n","[2021-04-12 12:12:30,060 INFO] Translating shard 3.\n","[2021-04-12 12:31:58,776 INFO] PRED AVG SCORE: -0.6329, PRED PPL: 1.8831\n","[2021-04-12 12:31:58,777 INFO] GOLD AVG SCORE: -3.0981, GOLD PPL: 22.1567\n","[2021-04-12 12:31:58,797 INFO] Translating shard 4.\n","[2021-04-12 12:51:43,659 INFO] PRED AVG SCORE: -0.6321, PRED PPL: 1.8815\n","[2021-04-12 12:51:43,660 INFO] GOLD AVG SCORE: -3.1210, GOLD PPL: 22.6693\n","[2021-04-12 12:51:43,681 INFO] Translating shard 5.\n","[2021-04-12 13:11:14,470 INFO] PRED AVG SCORE: -0.6332, PRED PPL: 1.8836\n","[2021-04-12 13:11:14,470 INFO] GOLD AVG SCORE: -3.0960, GOLD PPL: 22.1101\n","[2021-04-12 13:11:14,493 INFO] Translating shard 6.\n","[2021-04-12 13:30:37,242 INFO] PRED AVG SCORE: -0.6302, PRED PPL: 1.8779\n","[2021-04-12 13:30:37,242 INFO] GOLD AVG SCORE: -3.0907, GOLD PPL: 21.9922\n","[2021-04-12 13:30:37,265 INFO] Translating shard 7.\n","[2021-04-12 13:49:56,352 INFO] PRED AVG SCORE: -0.6280, PRED PPL: 1.8739\n","[2021-04-12 13:49:56,352 INFO] GOLD AVG SCORE: -3.0764, GOLD PPL: 21.6803\n","[2021-04-12 13:49:56,374 INFO] Translating shard 8.\n","[2021-04-12 14:09:15,741 INFO] PRED AVG SCORE: -0.6335, PRED PPL: 1.8842\n","[2021-04-12 14:09:15,742 INFO] GOLD AVG SCORE: -3.1238, GOLD PPL: 22.7337\n","[2021-04-12 14:09:15,763 INFO] Translating shard 9.\n","[2021-04-12 14:28:44,348 INFO] PRED AVG SCORE: -0.6319, PRED PPL: 1.8813\n","[2021-04-12 14:28:44,348 INFO] GOLD AVG SCORE: -3.1045, GOLD PPL: 22.2983\n","[2021-04-12 14:28:44,369 INFO] Translating shard 10.\n","[2021-04-12 14:29:32,006 INFO] PRED AVG SCORE: -0.6438, PRED PPL: 1.9036\n","[2021-04-12 14:29:32,006 INFO] GOLD AVG SCORE: -2.9999, GOLD PPL: 20.0831\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPGsYFs_XpAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267194776,"user_tz":-420,"elapsed":3562,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bb210067-fac3-4217-ba0c-6b4951d46578"},"source":["!tail vi_test"],"execution_count":5,"outputs":[{"output_type":"stream","text":["And nobody questions him, because they don't want to hear the answer because it's a lie!\n","Kubo?\n","Họ rất vui vẻ, và lúc nào cũng hát với nến.\n","Nghe này, anh không thể nói chuyện bây giờ được.\n","Vậy thì con có thể dùng trí tưởng tượng của mình.\n","Không hề.\n","Tôi đang nhìn hắn ngay lúc này đây.\n","Bác không để tâm chứ?\n","Anh nghĩ cậu ta phản ứng với thuốc?\n","Bị làm sao mà anh lại đi dự lễ đặt tên em bé của Cuddy chứ?\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rf5W-T8MzRK3"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw_Gma2gz0PK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618237796587,"user_tz":-420,"elapsed":18,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"5e0e080d-12fc-4905-c937-338fe87f6ed0"},"source":["!ls -al"],"execution_count":12,"outputs":[{"output_type":"stream","text":["total 355371\n","drwx------  2 root root     4096 Apr 12 04:16 data_bin\n","-rw-------  1 root root  3318349 Apr 12 02:53 en_test\n","-rw-------  1 root root 26563375 Apr 12 02:53 en_train\n","-rw-------  1 root root 31608088 Apr 12 03:46 en_train_EM_0.8\n","-rw-------  1 root root 29845474 Apr 12 03:46 en_train_EM_0.85\n","-rw-------  1 root root 28377219 Apr 12 03:46 en_train_EM_0.9\n","-rw-------  1 root root 27245135 Apr 12 03:46 en_train_EM_0.95\n","-rw-------  1 root root 11930822 Apr 12 03:46 en_train_EM_factor_0.8\n","-rw-------  1 root root 11270762 Apr 12 03:46 en_train_EM_factor_0.85\n","-rw-------  1 root root 10719696 Apr 12 03:46 en_train_EM_factor_0.9\n","-rw-------  1 root root 10293086 Apr 12 03:46 en_train_EM_factor_0.95\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.8\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.85\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.9\n","-rw-------  1 root root  6897191 Apr 12 03:46 en_train_EM_score_0.95\n","-rw-------  1 root root  3328557 Apr 12 02:53 en_valid\n","drwx------  2 root root     4096 Apr 12 11:13 model\n","drwx------ 11 root root     4096 Apr 12 14:29 OpenNMT-py\n","-rw-------  1 root root 94255466 Apr 12 04:16 opus_sent2vec.tar.gz\n","drwx------  2 root root     4096 Apr 12 04:17 output\n","-rw-------  1 root root  3764655 Apr 12 14:29 predict.txt\n","-rw-------  1 root root  4365722 Apr 12 02:53 vi_test\n","-rw-------  1 root root 35019161 Apr 12 02:53 vi_train\n","-rw-------  1 root root  4382771 Apr 12 02:53 vi_valid\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7T7xCaDdR469","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618267207801,"user_tz":-420,"elapsed":10451,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"405e8770-c646-4a08-a608-83e7fdea498f"},"source":["!perl OpenNMT-py/tools/multi-bleu.perl vi_test < predict.txt"],"execution_count":6,"outputs":[{"output_type":"stream","text":["BLEU = 16.60, 42.0/23.4/14.4/9.7 (BP=0.863, ratio=0.871, hyp_len=660812, ref_len=758454)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uLGJCSK_Qih1","executionInfo":{"status":"ok","timestamp":1618237803932,"user_tz":-420,"elapsed":11,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}}},"source":[""],"execution_count":13,"outputs":[]}]}