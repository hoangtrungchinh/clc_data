{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OPUS-OpenNMT-bert-0.95 BLEU 17.49.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619582722552,"user_tz":-420,"elapsed":29222,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a6729826-e76c-46fa-a090-fd92db4cde45"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC"},"source":["# import os\n","# path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"OPUS-OpenNMT-bert-0.9-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v5-Faiss/OPUS-OpenNMT-bert-0.9-20210412-0158'\n","os.chdir(path)\n","!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619582745144,"user_tz":-420,"elapsed":1076,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2e0dccde-ad77-494c-86d4-292edada61b1"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Wed Apr 28 04:05:42 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619582776335,"user_tz":-420,"elapsed":17064,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"8fa5e429-3e1d-4c55-da82-4c82659ae65c"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 20.0MB/s \n","\u001b[?25hCollecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (56.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=75589f5125fff4a345c57afd95a0402c4ba86dda7d3934a965c011f22c9fe9fc\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 7.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619582786064,"user_tz":-420,"elapsed":26242,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"f56596f8-fcd5-42c7-83eb-7acef23cceea"},"source":["!wget https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_bert.tar.gz\n","!mkdir data_bin\n","!tar -xvf 'opus_bert.tar.gz'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2021-04-28 04:06:14--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset/opus_bert.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 100661014 (96M) [application/octet-stream]\n","Saving to: ‘opus_bert.tar.gz’\n","\n","opus_bert.tar.gz    100%[===================>]  96.00M   112MB/s    in 0.9s    \n","\n","2021-04-28 04:06:20 (112 MB/s) - ‘opus_bert.tar.gz’ saved [100661014/100661014]\n","\n","en_train_EM_score_0.95\n","vi_valid\n","en_train_EM_0.95\n","en_train_EM_factor_0.85\n","en_train_EM_score_0.8\n","vi_train\n","en_train_EM_factor_0.8\n","en_train_EM_0.8\n","en_valid\n","en_train_EM_factor_0.95\n","en_train\n","en_train_EM_score_0.85\n","vi_test\n","en_train_EM_0.85\n","en_train_EM_score_0.9\n","en_test\n","en_train_EM_factor_0.9\n","en_train_EM_0.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619582849506,"user_tz":-420,"elapsed":58992,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"d6aeb52d-76b2-48bd-bd99-a92c980bc15c"},"source":["!mkdir -p output\n","!onmt_preprocess -train_src 'en_train_EM_0.95' \\\\\n","-train_tgt 'vi_train' \\\\\n","-valid_src 'en_valid' \\\\\n","-valid_tgt 'vi_valid' \\\\\n","-save_data 'output/en-vi' "],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-28 04:06:32,825 INFO] Extracting features...\n","[2021-04-28 04:06:32,825 INFO]  * number of source features: 0.\n","[2021-04-28 04:06:32,825 INFO]  * number of target features: 0.\n","[2021-04-28 04:06:32,825 INFO] Building `Fields` object...\n","[2021-04-28 04:06:32,825 INFO] Building & saving training data...\n","[2021-04-28 04:06:34,008 INFO] Building shard 0.\n","[2021-04-28 04:07:03,448 INFO]  * saving 0th train data shard to output/en-vi.train.0.pt.\n","[2021-04-28 04:07:20,527 INFO]  * tgt vocab size: 50004.\n","[2021-04-28 04:07:20,934 INFO]  * src vocab size: 50002.\n","[2021-04-28 04:07:21,618 INFO] Building & saving validation data...\n","[2021-04-28 04:07:22,274 INFO] Building shard 0.\n","[2021-04-28 04:07:24,428 INFO]  * saving 0th valid data shard to output/en-vi.valid.0.pt.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYyQECmi0TX5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619597875134,"user_tz":-420,"elapsed":15081101,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"b383ae77-e1b3-4a8d-8140-b3d06fe8ee96"},"source":["!mkdir -p model\n","!onmt_train -data 'output/en-vi' \\\\\n","-save_model 'model/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30000  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 1000 -save_checkpoint_steps 1000 \\\\\n","-report_every 1000 -world_size 1 -gpu_ranks 0"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-28 04:07:30,699 INFO]  * src vocab size = 50002\n","[2021-04-28 04:07:30,699 INFO]  * tgt vocab size = 50004\n","[2021-04-28 04:07:30,699 INFO] Building model...\n","[2021-04-28 04:07:39,414 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50002, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(50004, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=50004, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-28 04:07:39,467 INFO] encoder: 44516352\n","[2021-04-28 04:07:39,467 INFO] decoder: 76479316\n","[2021-04-28 04:07:39,467 INFO] * number of parameters: 120995668\n","[2021-04-28 04:07:39,471 INFO] Starting training on GPU: [0]\n","[2021-04-28 04:07:39,471 INFO] Start training loop and validate every 1000 steps...\n","[2021-04-28 04:07:39,471 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:07:56,821 INFO] number of examples: 802833\n","[2021-04-28 04:14:22,951 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:14:43,903 INFO] number of examples: 802833\n","[2021-04-28 04:15:03,242 INFO] Step 1000/30000; acc:  13.58; ppl: 554.48; xent: 6.32; lr: 0.00012; 12211/15973 tok/s;    444 sec\n","[2021-04-28 04:15:03,243 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 04:15:04,589 INFO] number of examples: 100400\n","[2021-04-28 04:16:07,635 INFO] Validation perplexity: 268.643\n","[2021-04-28 04:16:07,635 INFO] Validation accuracy: 19.0596\n","[2021-04-28 04:16:09,281 INFO] Saving checkpoint model/en-vi_step_1000.pt\n","[2021-04-28 04:22:29,418 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:23:03,463 INFO] number of examples: 802833\n","[2021-04-28 04:23:35,457 INFO] Step 2000/30000; acc:  28.07; ppl: 67.28; xent: 4.21; lr: 0.00025; 10580/13817 tok/s;    956 sec\n","[2021-04-28 04:23:35,458 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 04:23:36,832 INFO] number of examples: 100400\n","[2021-04-28 04:24:40,001 INFO] Validation perplexity: 55.16\n","[2021-04-28 04:24:40,001 INFO] Validation accuracy: 36.0562\n","[2021-04-28 04:24:41,641 INFO] Saving checkpoint model/en-vi_step_2000.pt\n","[2021-04-28 04:30:44,221 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:31:08,802 INFO] number of examples: 802833\n","[2021-04-28 04:31:52,666 INFO] Step 3000/30000; acc:  39.05; ppl: 25.61; xent: 3.24; lr: 0.00037; 10910/14242 tok/s;   1453 sec\n","[2021-04-28 04:31:52,667 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 04:31:54,035 INFO] number of examples: 100400\n","[2021-04-28 04:32:56,286 INFO] Validation perplexity: 27.1323\n","[2021-04-28 04:32:56,287 INFO] Validation accuracy: 43.1175\n","[2021-04-28 04:32:57,927 INFO] Saving checkpoint model/en-vi_step_3000.pt\n","[2021-04-28 04:38:48,203 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:39:13,195 INFO] number of examples: 802833\n","[2021-04-28 04:40:09,550 INFO] Step 4000/30000; acc:  44.05; ppl: 16.28; xent: 2.79; lr: 0.00049; 10892/14274 tok/s;   1950 sec\n","[2021-04-28 04:40:09,551 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 04:40:10,947 INFO] number of examples: 100400\n","[2021-04-28 04:41:13,154 INFO] Validation perplexity: 21.0646\n","[2021-04-28 04:41:13,154 INFO] Validation accuracy: 45.5383\n","[2021-04-28 04:41:14,758 INFO] Saving checkpoint model/en-vi_step_4000.pt\n","[2021-04-28 04:46:53,028 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:47:18,484 INFO] number of examples: 802833\n","[2021-04-28 04:48:26,964 INFO] Step 5000/30000; acc:  46.86; ppl: 12.71; xent: 2.54; lr: 0.00062; 10906/14257 tok/s;   2447 sec\n","[2021-04-28 04:48:26,965 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 04:48:28,360 INFO] number of examples: 100400\n","[2021-04-28 04:49:30,341 INFO] Validation perplexity: 18.069\n","[2021-04-28 04:49:30,341 INFO] Validation accuracy: 47.1333\n","[2021-04-28 04:49:31,981 INFO] Saving checkpoint model/en-vi_step_5000.pt\n","[2021-04-28 04:54:57,757 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 04:55:22,485 INFO] number of examples: 802833\n","[2021-04-28 04:56:42,799 INFO] Step 6000/30000; acc:  48.62; ppl: 10.92; xent: 2.39; lr: 0.00074; 10930/14278 tok/s;   2943 sec\n","[2021-04-28 04:56:42,800 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 04:56:44,244 INFO] number of examples: 100400\n","[2021-04-28 04:57:46,483 INFO] Validation perplexity: 17.7275\n","[2021-04-28 04:57:46,483 INFO] Validation accuracy: 47.3243\n","[2021-04-28 04:57:48,082 INFO] Saving checkpoint model/en-vi_step_6000.pt\n","[2021-04-28 05:03:01,954 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:03:32,569 INFO] number of examples: 802833\n","[2021-04-28 05:05:05,334 INFO] Step 7000/30000; acc:  50.10; ppl:  9.71; xent: 2.27; lr: 0.00086; 10794/14100 tok/s;   3446 sec\n","[2021-04-28 05:05:05,335 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:05:06,736 INFO] number of examples: 100400\n","[2021-04-28 05:06:08,750 INFO] Validation perplexity: 17.2287\n","[2021-04-28 05:06:08,750 INFO] Validation accuracy: 47.5861\n","[2021-04-28 05:06:10,368 INFO] Saving checkpoint model/en-vi_step_7000.pt\n","[2021-04-28 05:11:12,387 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:11:33,605 INFO] number of examples: 802833\n","[2021-04-28 05:13:18,787 INFO] Step 8000/30000; acc:  51.33; ppl:  8.85; xent: 2.18; lr: 0.00099; 10979/14352 tok/s;   3939 sec\n","[2021-04-28 05:13:18,788 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:13:24,391 INFO] number of examples: 100400\n","[2021-04-28 05:14:26,145 INFO] Validation perplexity: 16.8447\n","[2021-04-28 05:14:26,145 INFO] Validation accuracy: 48.2133\n","[2021-04-28 05:14:27,762 INFO] Saving checkpoint model/en-vi_step_8000.pt\n","[2021-04-28 05:19:16,902 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:19:42,896 INFO] number of examples: 802833\n","[2021-04-28 05:21:40,292 INFO] Step 9000/30000; acc:  52.82; ppl:  7.97; xent: 2.08; lr: 0.00093; 10818/14130 tok/s;   4441 sec\n","[2021-04-28 05:21:40,293 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:21:41,711 INFO] number of examples: 100400\n","[2021-04-28 05:22:43,454 INFO] Validation perplexity: 16.279\n","[2021-04-28 05:22:43,454 INFO] Validation accuracy: 48.9329\n","[2021-04-28 05:22:45,044 INFO] Saving checkpoint model/en-vi_step_9000.pt\n","[2021-04-28 05:27:21,916 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:27:47,433 INFO] number of examples: 802833\n","[2021-04-28 05:29:56,927 INFO] Step 10000/30000; acc:  54.85; ppl:  7.03; xent: 1.95; lr: 0.00088; 10901/14259 tok/s;   4937 sec\n","[2021-04-28 05:29:56,929 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:30:05,661 INFO] number of examples: 100400\n","[2021-04-28 05:31:07,539 INFO] Validation perplexity: 16.0256\n","[2021-04-28 05:31:07,539 INFO] Validation accuracy: 49.1189\n","[2021-04-28 05:31:09,179 INFO] Saving checkpoint model/en-vi_step_10000.pt\n","[2021-04-28 05:35:33,763 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:36:11,542 INFO] number of examples: 802833\n","[2021-04-28 05:38:32,628 INFO] Step 11000/30000; acc:  56.75; ppl:  6.29; xent: 1.84; lr: 0.00084; 10515/13749 tok/s;   5453 sec\n","[2021-04-28 05:38:32,629 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:38:33,997 INFO] number of examples: 100400\n","[2021-04-28 05:39:35,122 INFO] Validation perplexity: 15.4964\n","[2021-04-28 05:39:35,122 INFO] Validation accuracy: 49.7614\n","[2021-04-28 05:39:36,693 INFO] Saving checkpoint model/en-vi_step_11000.pt\n","[2021-04-28 05:43:47,583 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:44:13,731 INFO] number of examples: 802833\n","[2021-04-28 05:46:46,364 INFO] Step 12000/30000; acc:  58.47; ppl:  5.72; xent: 1.74; lr: 0.00081; 10987/14331 tok/s;   5947 sec\n","[2021-04-28 05:46:46,365 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:46:47,759 INFO] number of examples: 100400\n","[2021-04-28 05:47:47,989 INFO] Validation perplexity: 16.1806\n","[2021-04-28 05:47:47,989 INFO] Validation accuracy: 49.785\n","[2021-04-28 05:47:49,552 INFO] Saving checkpoint model/en-vi_step_12000.pt\n","[2021-04-28 05:51:48,756 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 05:52:15,090 INFO] number of examples: 802833\n","[2021-04-28 05:54:59,371 INFO] Step 13000/30000; acc:  60.05; ppl:  5.26; xent: 1.66; lr: 0.00078; 11014/14355 tok/s;   6440 sec\n","[2021-04-28 05:54:59,372 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 05:55:00,728 INFO] number of examples: 100400\n","[2021-04-28 05:55:59,994 INFO] Validation perplexity: 16.4576\n","[2021-04-28 05:55:59,994 INFO] Validation accuracy: 49.9129\n","[2021-04-28 05:56:01,469 INFO] Saving checkpoint model/en-vi_step_13000.pt\n","[2021-04-28 05:59:49,100 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:00:15,582 INFO] number of examples: 802833\n","[2021-04-28 06:03:12,403 INFO] Step 14000/30000; acc:  61.47; ppl:  4.89; xent: 1.59; lr: 0.00075; 10971/14357 tok/s;   6933 sec\n","[2021-04-28 06:03:12,404 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:03:13,818 INFO] number of examples: 100400\n","[2021-04-28 06:04:14,182 INFO] Validation perplexity: 16.6806\n","[2021-04-28 06:04:14,182 INFO] Validation accuracy: 49.9117\n","[2021-04-28 06:04:15,801 INFO] Saving checkpoint model/en-vi_step_14000.pt\n","[2021-04-28 06:07:51,317 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:08:18,191 INFO] number of examples: 802833\n","[2021-04-28 06:11:27,649 INFO] Step 15000/30000; acc:  62.78; ppl:  4.57; xent: 1.52; lr: 0.00072; 10958/14305 tok/s;   7428 sec\n","[2021-04-28 06:11:27,650 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:11:29,119 INFO] number of examples: 100400\n","[2021-04-28 06:12:31,491 INFO] Validation perplexity: 17.1106\n","[2021-04-28 06:12:31,491 INFO] Validation accuracy: 50.1135\n","[2021-04-28 06:12:33,115 INFO] Saving checkpoint model/en-vi_step_15000.pt\n","[2021-04-28 06:15:57,707 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:16:25,071 INFO] number of examples: 802833\n","[2021-04-28 06:19:46,990 INFO] Step 16000/30000; acc:  64.01; ppl:  4.31; xent: 1.46; lr: 0.00070; 10859/14171 tok/s;   7928 sec\n","[2021-04-28 06:19:46,991 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:19:48,385 INFO] number of examples: 100400\n","[2021-04-28 06:20:48,349 INFO] Validation perplexity: 17.48\n","[2021-04-28 06:20:48,349 INFO] Validation accuracy: 50.0631\n","[2021-04-28 06:20:49,907 INFO] Saving checkpoint model/en-vi_step_16000.pt\n","[2021-04-28 06:24:01,304 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:24:28,167 INFO] number of examples: 802833\n","[2021-04-28 06:28:01,531 INFO] Step 17000/30000; acc:  65.04; ppl:  4.10; xent: 1.41; lr: 0.00068; 10943/14335 tok/s;   8422 sec\n","[2021-04-28 06:28:01,532 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:28:02,908 INFO] number of examples: 100400\n","[2021-04-28 06:29:02,638 INFO] Validation perplexity: 17.3969\n","[2021-04-28 06:29:02,638 INFO] Validation accuracy: 50.2069\n","[2021-04-28 06:29:04,145 INFO] Saving checkpoint model/en-vi_step_17000.pt\n","[2021-04-28 06:32:02,755 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:32:28,886 INFO] number of examples: 802833\n","[2021-04-28 06:36:14,874 INFO] Step 18000/30000; acc:  66.14; ppl:  3.89; xent: 1.36; lr: 0.00066; 11001/14342 tok/s;   8915 sec\n","[2021-04-28 06:36:14,875 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:36:16,334 INFO] number of examples: 100400\n","[2021-04-28 06:37:18,565 INFO] Validation perplexity: 18.0471\n","[2021-04-28 06:37:18,565 INFO] Validation accuracy: 50.2207\n","[2021-04-28 06:37:20,137 INFO] Saving checkpoint model/en-vi_step_18000.pt\n","[2021-04-28 06:40:07,680 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:40:34,078 INFO] number of examples: 802833\n","[2021-04-28 06:44:32,376 INFO] Step 19000/30000; acc:  67.05; ppl:  3.73; xent: 1.32; lr: 0.00064; 10893/14241 tok/s;   9413 sec\n","[2021-04-28 06:44:32,378 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:44:33,813 INFO] number of examples: 100400\n","[2021-04-28 06:45:35,757 INFO] Validation perplexity: 18.0883\n","[2021-04-28 06:45:35,757 INFO] Validation accuracy: 50.4809\n","[2021-04-28 06:45:37,347 INFO] Saving checkpoint model/en-vi_step_19000.pt\n","[2021-04-28 06:48:13,376 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:48:38,616 INFO] number of examples: 802833\n","[2021-04-28 06:52:48,675 INFO] Step 20000/30000; acc:  67.98; ppl:  3.58; xent: 1.27; lr: 0.00062; 10935/14243 tok/s;   9909 sec\n","[2021-04-28 06:52:48,676 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 06:52:57,071 INFO] number of examples: 100400\n","[2021-04-28 06:53:58,683 INFO] Validation perplexity: 19.002\n","[2021-04-28 06:53:58,683 INFO] Validation accuracy: 50.3721\n","[2021-04-28 06:54:00,253 INFO] Saving checkpoint model/en-vi_step_20000.pt\n","[2021-04-28 06:56:23,556 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 06:56:59,645 INFO] number of examples: 802833\n","[2021-04-28 07:01:22,410 INFO] Step 21000/30000; acc:  68.73; ppl:  3.46; xent: 1.24; lr: 0.00061; 10535/13804 tok/s;  10423 sec\n","[2021-04-28 07:01:22,411 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:01:23,866 INFO] number of examples: 100400\n","[2021-04-28 07:02:25,488 INFO] Validation perplexity: 18.9206\n","[2021-04-28 07:02:25,488 INFO] Validation accuracy: 50.5585\n","[2021-04-28 07:02:27,062 INFO] Saving checkpoint model/en-vi_step_21000.pt\n","[2021-04-28 07:04:38,277 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:05:03,397 INFO] number of examples: 802833\n","[2021-04-28 07:09:44,403 INFO] Step 22000/30000; acc:  69.54; ppl:  3.34; xent: 1.21; lr: 0.00060; 10807/14099 tok/s;  10925 sec\n","[2021-04-28 07:09:44,404 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:09:45,862 INFO] number of examples: 100400\n","[2021-04-28 07:10:47,369 INFO] Validation perplexity: 19.6641\n","[2021-04-28 07:10:47,369 INFO] Validation accuracy: 50.33\n","[2021-04-28 07:10:48,944 INFO] Saving checkpoint model/en-vi_step_22000.pt\n","[2021-04-28 07:12:48,591 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:13:24,509 INFO] number of examples: 802833\n","[2021-04-28 07:18:10,872 INFO] Step 23000/30000; acc:  70.27; ppl:  3.24; xent: 1.17; lr: 0.00058; 10694/14008 tok/s;  11431 sec\n","[2021-04-28 07:18:10,874 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:18:19,429 INFO] number of examples: 100400\n","[2021-04-28 07:19:20,456 INFO] Validation perplexity: 19.3479\n","[2021-04-28 07:19:20,456 INFO] Validation accuracy: 50.721\n","[2021-04-28 07:19:22,013 INFO] Saving checkpoint model/en-vi_step_23000.pt\n","[2021-04-28 07:21:09,135 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:21:45,978 INFO] number of examples: 802833\n","[2021-04-28 07:26:44,295 INFO] Step 24000/30000; acc:  70.98; ppl:  3.14; xent: 1.14; lr: 0.00057; 10552/13791 tok/s;  11945 sec\n","[2021-04-28 07:26:44,296 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:26:45,722 INFO] number of examples: 100400\n","[2021-04-28 07:27:46,985 INFO] Validation perplexity: 21.2147\n","[2021-04-28 07:27:46,985 INFO] Validation accuracy: 50.4924\n","[2021-04-28 07:27:48,554 INFO] Saving checkpoint model/en-vi_step_24000.pt\n","[2021-04-28 07:29:23,716 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:29:50,608 INFO] number of examples: 802833\n","[2021-04-28 07:35:01,784 INFO] Step 25000/30000; acc:  71.64; ppl:  3.05; xent: 1.11; lr: 0.00056; 10899/14227 tok/s;  12442 sec\n","[2021-04-28 07:35:01,785 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:35:03,228 INFO] number of examples: 100400\n","[2021-04-28 07:36:04,763 INFO] Validation perplexity: 20.8268\n","[2021-04-28 07:36:04,763 INFO] Validation accuracy: 50.66\n","[2021-04-28 07:36:06,334 INFO] Saving checkpoint model/en-vi_step_25000.pt\n","[2021-04-28 07:37:28,386 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:37:55,354 INFO] number of examples: 802833\n","[2021-04-28 07:43:18,726 INFO] Step 26000/30000; acc:  72.29; ppl:  2.97; xent: 1.09; lr: 0.00055; 10893/14221 tok/s;  12939 sec\n","[2021-04-28 07:43:18,727 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:43:20,150 INFO] number of examples: 100400\n","[2021-04-28 07:44:21,875 INFO] Validation perplexity: 21.3341\n","[2021-04-28 07:44:21,875 INFO] Validation accuracy: 50.4799\n","[2021-04-28 07:44:23,467 INFO] Saving checkpoint model/en-vi_step_26000.pt\n","[2021-04-28 07:45:33,214 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:45:59,942 INFO] number of examples: 802833\n","[2021-04-28 07:51:35,320 INFO] Step 27000/30000; acc:  72.86; ppl:  2.90; xent: 1.06; lr: 0.00054; 10913/14237 tok/s;  13436 sec\n","[2021-04-28 07:51:35,321 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:51:36,737 INFO] number of examples: 100400\n","[2021-04-28 07:52:37,713 INFO] Validation perplexity: 21.9018\n","[2021-04-28 07:52:37,714 INFO] Validation accuracy: 50.3908\n","[2021-04-28 07:52:39,283 INFO] Saving checkpoint model/en-vi_step_27000.pt\n","[2021-04-28 07:53:36,782 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 07:54:03,146 INFO] number of examples: 802833\n","[2021-04-28 07:59:50,877 INFO] Step 28000/30000; acc:  73.41; ppl:  2.83; xent: 1.04; lr: 0.00053; 10934/14270 tok/s;  13931 sec\n","[2021-04-28 07:59:50,878 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 07:59:52,308 INFO] number of examples: 100400\n","[2021-04-28 08:00:53,455 INFO] Validation perplexity: 22.4412\n","[2021-04-28 08:00:53,455 INFO] Validation accuracy: 50.6259\n","[2021-04-28 08:00:55,027 INFO] Saving checkpoint model/en-vi_step_28000.pt\n","[2021-04-28 08:01:40,602 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 08:02:06,182 INFO] number of examples: 802833\n","[2021-04-28 08:08:05,265 INFO] Step 29000/30000; acc:  73.93; ppl:  2.77; xent: 1.02; lr: 0.00052; 10933/14309 tok/s;  14426 sec\n","[2021-04-28 08:08:05,267 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 08:08:13,782 INFO] number of examples: 100400\n","[2021-04-28 08:09:16,680 INFO] Validation perplexity: 22.5671\n","[2021-04-28 08:09:16,680 INFO] Validation accuracy: 50.6362\n","[2021-04-28 08:09:18,352 INFO] Saving checkpoint model/en-vi_step_29000.pt\n","[2021-04-28 08:09:51,367 INFO] Loading dataset from output/en-vi.train.0.pt\n","[2021-04-28 08:10:28,196 INFO] number of examples: 802833\n","[2021-04-28 08:16:39,870 INFO] Step 30000/30000; acc:  74.41; ppl:  2.72; xent: 1.00; lr: 0.00051; 10516/13740 tok/s;  14940 sec\n","[2021-04-28 08:16:39,871 INFO] Loading dataset from output/en-vi.valid.0.pt\n","[2021-04-28 08:16:41,336 INFO] number of examples: 100400\n","[2021-04-28 08:17:43,603 INFO] Validation perplexity: 22.9714\n","[2021-04-28 08:17:43,604 INFO] Validation accuracy: 50.4484\n","[2021-04-28 08:17:45,233 INFO] Saving checkpoint model/en-vi_step_30000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XOEz-j6IbNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619597875135,"user_tz":-420,"elapsed":15078351,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"b717fb1d-0f8a-45d4-e80a-c7f39d60febf"},"source":["!ls -al model model/"],"execution_count":8,"outputs":[{"output_type":"stream","text":["model:\n","total 43398608\n","drwxr-xr-x 2 root root       4096 Apr 28 08:17 .\n","drwxr-xr-x 1 root root       4096 Apr 28 04:07 ..\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:31 en-vi_step_10000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:16 en-vi_step_1000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:39 en-vi_step_11000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:47 en-vi_step_12000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:56 en-vi_step_13000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:04 en-vi_step_14000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:12 en-vi_step_15000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:20 en-vi_step_16000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:29 en-vi_step_17000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:37 en-vi_step_18000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:45 en-vi_step_19000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:54 en-vi_step_20000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:24 en-vi_step_2000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:02 en-vi_step_21000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:10 en-vi_step_22000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:19 en-vi_step_23000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:27 en-vi_step_24000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:36 en-vi_step_25000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:44 en-vi_step_26000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:52 en-vi_step_27000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 08:01 en-vi_step_28000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 08:09 en-vi_step_29000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 08:17 en-vi_step_30000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:33 en-vi_step_3000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:41 en-vi_step_4000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:49 en-vi_step_5000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:57 en-vi_step_6000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:06 en-vi_step_7000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:14 en-vi_step_8000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:22 en-vi_step_9000.pt\n","\n","model/:\n","total 43398608\n","drwxr-xr-x 2 root root       4096 Apr 28 08:17 .\n","drwxr-xr-x 1 root root       4096 Apr 28 04:07 ..\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:31 en-vi_step_10000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:16 en-vi_step_1000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:39 en-vi_step_11000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:47 en-vi_step_12000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:56 en-vi_step_13000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:04 en-vi_step_14000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:12 en-vi_step_15000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:20 en-vi_step_16000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:29 en-vi_step_17000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:37 en-vi_step_18000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:45 en-vi_step_19000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 06:54 en-vi_step_20000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:24 en-vi_step_2000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:02 en-vi_step_21000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:10 en-vi_step_22000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:19 en-vi_step_23000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:27 en-vi_step_24000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:36 en-vi_step_25000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:44 en-vi_step_26000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 07:52 en-vi_step_27000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 08:01 en-vi_step_28000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 08:09 en-vi_step_29000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 08:17 en-vi_step_30000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:33 en-vi_step_3000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:41 en-vi_step_4000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:49 en-vi_step_5000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 04:57 en-vi_step_6000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:06 en-vi_step_7000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:14 en-vi_step_8000.pt\n","-rw-r--r-- 1 root root 1481332543 Apr 28 05:22 en-vi_step_9000.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1S26AN4rHUAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619608967410,"user_tz":-420,"elapsed":26169936,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2d2f3f52-fd39-4017-d383-b06797a0d552"},"source":["!onmt_translate -model model/en-vi_step_30000.pt -src en_test -tgt vi_test -output predict.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-28 08:18:00,016 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [10, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-28 08:36:31,929 INFO] PRED AVG SCORE: -0.6171, PRED PPL: 1.8536\n","[2021-04-28 08:36:31,929 INFO] GOLD AVG SCORE: -3.1235, GOLD PPL: 22.7253\n","[2021-04-28 08:36:31,953 INFO] Translating shard 1.\n","tcmalloc: large alloc 1518125056 bytes == 0x562e81188000 @  0x7f4c8e5f4b6b 0x7f4c8e614379 0x7f4c3ad4d25e 0x7f4c3ad4e9d2 0x7f4c77a318e6 0x7f4c77e93dd9 0x7f4c7839e77a 0x7f4c78369ef9 0x7f4c78320657 0x7f4c781c4929 0x7f4c77cdc516 0x7f4c7839f7af 0x7f4c7814e846 0x7f4c78153e6f 0x7f4c79a37bcc 0x7f4c79a3813f 0x7f4c785a0a86 0x7f4c785a4caf 0x7f4c77cce16a 0x7f4c77cceb3a 0x7f4c784b37f8 0x7f4c784b383f 0x7f4c7814e846 0x7f4c7815422f 0x7f4c77cb20b1 0x7f4c784b24c0 0x7f4c784d505d 0x7f4c782a5a59 0x7f4c897c38de 0x562e0597b050 0x562e0597ade0\n","tcmalloc: large alloc 1518125056 bytes == 0x562edb954000 @  0x7f4c8e5f4b6b 0x7f4c8e614379 0x7f4c3ad4d25e 0x7f4c3ad4e9d2 0x7f4c77a318e6 0x7f4c77e93dd9 0x7f4c7839e77a 0x7f4c78369ef9 0x7f4c78320657 0x7f4c781c4929 0x7f4c77e9d6a2 0x7f4c77e2d5c5 0x7f4c7839f573 0x7f4c7830f904 0x7f4c78179f09 0x7f4c799c1444 0x7f4c799c1783 0x7f4c7830f904 0x7f4c78179f09 0x7f4c77e265d0 0x7f4c784b30e0 0x7f4c784b3132 0x7f4c7831d054 0x7f4c785c1735 0x7f4c89520f4f 0x562e0597b050 0x562e05a6c99d 0x562e059eefe9 0x562e059e9b0e 0x562e0597c77a 0x562e059eb86a\n","[2021-04-28 08:55:03,051 INFO] PRED AVG SCORE: -0.6164, PRED PPL: 1.8523\n","[2021-04-28 08:55:03,051 INFO] GOLD AVG SCORE: -3.1441, GOLD PPL: 23.1980\n","[2021-04-28 08:55:03,083 INFO] Translating shard 2.\n","[2021-04-28 09:13:52,050 INFO] PRED AVG SCORE: -0.6171, PRED PPL: 1.8535\n","[2021-04-28 09:13:52,050 INFO] GOLD AVG SCORE: -3.1248, GOLD PPL: 22.7558\n","[2021-04-28 09:13:52,071 INFO] Translating shard 3.\n","[2021-04-28 09:32:19,102 INFO] PRED AVG SCORE: -0.6204, PRED PPL: 1.8597\n","[2021-04-28 09:32:19,102 INFO] GOLD AVG SCORE: -3.1085, GOLD PPL: 22.3879\n","[2021-04-28 09:32:19,124 INFO] Translating shard 4.\n","[2021-04-28 09:51:05,870 INFO] PRED AVG SCORE: -0.6190, PRED PPL: 1.8571\n","[2021-04-28 09:51:05,870 INFO] GOLD AVG SCORE: -3.1448, GOLD PPL: 23.2160\n","[2021-04-28 09:51:05,894 INFO] Translating shard 5.\n","[2021-04-28 10:09:26,022 INFO] PRED AVG SCORE: -0.6191, PRED PPL: 1.8572\n","[2021-04-28 10:09:26,022 INFO] GOLD AVG SCORE: -3.1164, GOLD PPL: 22.5640\n","[2021-04-28 10:09:26,052 INFO] Translating shard 6.\n","[2021-04-28 10:27:44,537 INFO] PRED AVG SCORE: -0.6179, PRED PPL: 1.8550\n","[2021-04-28 10:27:44,538 INFO] GOLD AVG SCORE: -3.0905, GOLD PPL: 21.9890\n","[2021-04-28 10:27:44,565 INFO] Translating shard 7.\n","[2021-04-28 10:45:50,064 INFO] PRED AVG SCORE: -0.6145, PRED PPL: 1.8488\n","[2021-04-28 10:45:50,064 INFO] GOLD AVG SCORE: -3.0832, GOLD PPL: 21.8273\n","[2021-04-28 10:45:50,087 INFO] Translating shard 8.\n","[2021-04-28 11:03:45,851 INFO] PRED AVG SCORE: -0.6207, PRED PPL: 1.8602\n","[2021-04-28 11:03:45,852 INFO] GOLD AVG SCORE: -3.1362, GOLD PPL: 23.0152\n","[2021-04-28 11:03:45,878 INFO] Translating shard 9.\n","[2021-04-28 11:22:02,580 INFO] PRED AVG SCORE: -0.6185, PRED PPL: 1.8562\n","[2021-04-28 11:22:02,581 INFO] GOLD AVG SCORE: -3.1179, GOLD PPL: 22.5991\n","[2021-04-28 11:22:02,600 INFO] Translating shard 10.\n","[2021-04-28 11:22:46,173 INFO] PRED AVG SCORE: -0.6289, PRED PPL: 1.8755\n","[2021-04-28 11:22:46,173 INFO] GOLD AVG SCORE: -2.9988, GOLD PPL: 20.0614\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPGsYFs_XpAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619608968210,"user_tz":-420,"elapsed":26169903,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"903db4d3-2bd1-4879-b56a-2e38a9cdd442"},"source":["!tail vi_test"],"execution_count":10,"outputs":[{"output_type":"stream","text":["And nobody questions him, because they don't want to hear the answer because it's a lie!\n","Kubo?\n","Họ rất vui vẻ, và lúc nào cũng hát với nến.\n","Nghe này, anh không thể nói chuyện bây giờ được.\n","Vậy thì con có thể dùng trí tưởng tượng của mình.\n","Không hề.\n","Tôi đang nhìn hắn ngay lúc này đây.\n","Bác không để tâm chứ?\n","Anh nghĩ cậu ta phản ứng với thuốc?\n","Bị làm sao mà anh lại đi dự lễ đặt tên em bé của Cuddy chứ?\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rf5W-T8MzRK3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619608981516,"user_tz":-420,"elapsed":26182819,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"c59a0402-4332-4202-f4a8-a4714b5a95cb"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17287, done.\u001b[K\n","remote: Counting objects: 100% (246/246), done.\u001b[K\n","remote: Compressing objects: 100% (167/167), done.\u001b[K\n","remote: Total 17287 (delta 148), reused 135 (delta 77), pack-reused 17041\u001b[K\n","Receiving objects: 100% (17287/17287), 273.51 MiB | 26.83 MiB/s, done.\n","Resolving deltas: 100% (12456/12456), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Iw_Gma2gz0PK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619608981516,"user_tz":-420,"elapsed":26182395,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a1dd48b0-f75a-46fe-c00b-99479a6f8980"},"source":["!ls -al"],"execution_count":12,"outputs":[{"output_type":"stream","text":["total 376852\n","drwxr-xr-x  1 root root      4096 Apr 28 11:22 .\n","drwxr-xr-x  1 root root      4096 Apr 28 04:03 ..\n","drwxr-xr-x  4 root root      4096 Apr 21 13:38 .config\n","drwxr-xr-x  2 root root      4096 Apr 28 04:06 data_bin\n","drwx------  6 root root      4096 Apr 28 04:05 drive\n","-rw-rw-r--  1 1000 1000   3318349 Apr 12 01:48 en_test\n","-rw-rw-r--  1 1000 1000  26563375 Apr 12 01:48 en_train\n","-rw-rw-r--  1 1000 1000  34866334 Apr 12 01:07 en_train_EM_0.8\n","-rw-rw-r--  1 1000 1000  32006284 Apr 12 01:07 en_train_EM_0.85\n","-rw-rw-r--  1 1000 1000  29696900 Apr 12 01:07 en_train_EM_0.9\n","-rw-rw-r--  1 1000 1000  27849828 Apr 12 01:07 en_train_EM_0.95\n","-rw-rw-r--  1 1000 1000  13223858 Apr 12 01:07 en_train_EM_factor_0.8\n","-rw-rw-r--  1 1000 1000  12138268 Apr 12 01:07 en_train_EM_factor_0.85\n","-rw-rw-r--  1 1000 1000  11254364 Apr 12 01:07 en_train_EM_factor_0.9\n","-rw-rw-r--  1 1000 1000  10539008 Apr 12 01:07 en_train_EM_factor_0.95\n","-rw-rw-r--  1 1000 1000   8152745 Apr 12 01:07 en_train_EM_score_0.8\n","-rw-rw-r--  1 1000 1000   8152745 Apr 12 01:07 en_train_EM_score_0.85\n","-rw-rw-r--  1 1000 1000   8152745 Apr 12 01:07 en_train_EM_score_0.9\n","-rw-rw-r--  1 1000 1000   8152745 Apr 12 01:07 en_train_EM_score_0.95\n","-rw-rw-r--  1 1000 1000   3328557 Apr 12 01:48 en_valid\n","drwxr-xr-x  2 root root      4096 Apr 28 08:17 model\n","drwxr-xr-x 11 root root      4096 Apr 28 11:23 OpenNMT-py\n","-rw-r--r--  1 root root 100661014 Apr 28 04:06 opus_bert.tar.gz\n","drwxr-xr-x  2 root root      4096 Apr 28 04:07 output\n","-rw-r--r--  1 root root   3981580 Apr 28 11:22 predict.txt\n","drwxr-xr-x  1 root root      4096 Apr 21 13:39 sample_data\n","-rw-rw-r--  1 1000 1000   4365722 Apr 12 01:48 vi_test\n","-rw-rw-r--  1 1000 1000  35019161 Apr 12 01:48 vi_train\n","-rw-rw-r--  1 1000 1000   4382771 Apr 12 01:48 vi_valid\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7T7xCaDdR469","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619608987869,"user_tz":-420,"elapsed":26188151,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"f825d3eb-da70-4178-809f-c4e56d1c4b52"},"source":["!perl OpenNMT-py/tools/multi-bleu.perl vi_test < predict.txt"],"execution_count":13,"outputs":[{"output_type":"stream","text":["BLEU = 17.49, 41.5/23.1/14.2/9.5 (BP=0.923, ratio=0.926, hyp_len=701996, ref_len=758454)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uLGJCSK_Qih1"},"source":[""],"execution_count":null,"outputs":[]}]}