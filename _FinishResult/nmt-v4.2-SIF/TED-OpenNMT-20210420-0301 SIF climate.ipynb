{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-20210420-0301 SIF climate.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888345008,"user_tz":-420,"elapsed":20055,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e4f9935c-4bec-4e7e-eab7-bf8940b73d1a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888352932,"user_tz":-420,"elapsed":2521,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"8dccdff6-d4de-4d36-bd07-f03a20a5b84b"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888369728,"user_tz":-420,"elapsed":801,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"02eeb39a-0b26-46b1-be15-824372db85ce"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:12:49 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888389623,"user_tz":-420,"elapsed":18437,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"23b7d1fc-d011-46f5-ea32-87ad25b681a4"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 6.0MB/s \n","\u001b[?25hCollecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 514kB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Collecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=9c0f02612ef89fea1edb7bd127fc23beec9649734aadd2cbbb4063d78b3489ac\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: configargparse, torchtext, pyonmttok, waitress, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618887841008,"user_tz":-420,"elapsed":1781,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2d9dcd2-26ed-49c2-a523-9ad4d474e26a"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","!tar -xvf 'SIF-finetune.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 03:03:59--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 97919 (96K) [application/octet-stream]\n","Saving to: ‘SIF-finetune.tar.gz’\n","\n","\rSIF-finetune.tar.gz   0%[                    ]       0  --.-KB/s               \rSIF-finetune.tar.gz 100%[===================>]  95.62K  --.-KB/s    in 0.008s  \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 03:03:59 (12.0 MB/s) - ‘SIF-finetune.tar.gz’ saved [97919/97919]\n","\n","finetune/\n","finetune/en_finetune_law_50\n","finetune/en_finetune_law_95\n","finetune/en_finetune_buddhism_50\n","finetune/en_finetune_buddhism_100\n","finetune/en_finetune_buddhism_150\n","finetune/en_finetune_buddhism_193\n","finetune/en_finetune_climate_50\n","finetune/en_finetune_climate_100\n","finetune/en_finetune_climate_120\n","finetune/en_finetune_catechism_50\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_150\n","finetune/en_finetune_catechism_152\n","finetune/vi_finetune_law_50\n","finetune/vi_finetune_law_95\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_buddhism_193\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_100\n","finetune/vi_finetune_climate_120\n","finetune/vi_finetune_catechism_50\n","finetune/vi_finetune_catechism_100\n","finetune/vi_finetune_catechism_150\n","finetune/vi_finetune_catechism_152\n","finetune/test_law.vi\n","finetune/test_buddhism.vi\n","finetune/test_climate.vi\n","finetune/test_catechism.vi\n","finetune/test_law.en\n","finetune/test_buddhism.en\n","finetune/test_climate.en\n","finetune/test_catechism.en\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618887850544,"user_tz":-420,"elapsed":872,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"38a22dd4-2b96-4702-b250-82d5a7368a6a"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 491\n","-rw------- 1 root root  9813 Apr 20 02:12 en_finetune_buddhism_100\n","-rw------- 1 root root 14812 Apr 20 02:12 en_finetune_buddhism_150\n","-rw------- 1 root root 19723 Apr 20 02:12 en_finetune_buddhism_193\n","-rw------- 1 root root  5246 Apr 20 02:12 en_finetune_buddhism_50\n","-rw------- 1 root root 13614 Apr 20 02:12 en_finetune_catechism_100\n","-rw------- 1 root root 20281 Apr 20 02:12 en_finetune_catechism_150\n","-rw------- 1 root root 20539 Apr 20 02:12 en_finetune_catechism_152\n","-rw------- 1 root root  7151 Apr 20 02:12 en_finetune_catechism_50\n","-rw------- 1 root root 13647 Apr 20 02:12 en_finetune_climate_100\n","-rw------- 1 root root 16493 Apr 20 02:12 en_finetune_climate_120\n","-rw------- 1 root root  6498 Apr 20 02:12 en_finetune_climate_50\n","-rw------- 1 root root  6664 Apr 20 02:12 en_finetune_law_50\n","-rw------- 1 root root 12797 Apr 20 02:12 en_finetune_law_95\n","-rw------- 1 root root  7711 Apr 20 02:12 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 02:12 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 02:12 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 02:12 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 02:12 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 02:12 test_climate.vi\n","-rw------- 1 root root 11267 Apr 20 02:12 test_law.en\n","-rw------- 1 root root 15336 Apr 20 02:12 test_law.vi\n","-rw------- 1 root root 13723 Apr 20 02:12 vi_finetune_buddhism_100\n","-rw------- 1 root root 21466 Apr 20 02:12 vi_finetune_buddhism_150\n","-rw------- 1 root root 28181 Apr 20 02:12 vi_finetune_buddhism_193\n","-rw------- 1 root root  7370 Apr 20 02:12 vi_finetune_buddhism_50\n","-rw------- 1 root root 18512 Apr 20 02:12 vi_finetune_catechism_100\n","-rw------- 1 root root 27677 Apr 20 02:12 vi_finetune_catechism_150\n","-rw------- 1 root root 28052 Apr 20 02:12 vi_finetune_catechism_152\n","-rw------- 1 root root  9740 Apr 20 02:12 vi_finetune_catechism_50\n","-rw------- 1 root root 18253 Apr 20 02:12 vi_finetune_climate_100\n","-rw------- 1 root root 22006 Apr 20 02:12 vi_finetune_climate_120\n","-rw------- 1 root root  8917 Apr 20 02:12 vi_finetune_climate_50\n","-rw------- 1 root root  8413 Apr 20 02:12 vi_finetune_law_50\n","-rw------- 1 root root 16118 Apr 20 02:12 vi_finetune_law_95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618887872728,"user_tz":-420,"elapsed":19319,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"dea9bd89-6e12-4441-eb1e-6a1eaeb208c4"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 24.10 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888676511,"user_tz":-420,"elapsed":207479,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a82d01ba-3619-4b87-a67d-dde6ed6fc78a"},"source":["!mkdir -p sif_climate/output_climate_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_50' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_50' \\\\\n","-save_data 'sif_climate/output_climate_50/en-vi' \n","\n","!mkdir -p sif_climate/model_climate_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_climate/output_climate_50/en-vi' \\\\\n","-save_model 'sif_climate/model_climate_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_climate/model_climate_50/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_climate/model_climate_50/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_climate/model_climate_50/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_climate/model_climate_50/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":5,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:14:32,881 INFO] Extracting features...\n","[2021-04-20 03:14:33,444 INFO]  * number of source features: 0.\n","[2021-04-20 03:14:33,445 INFO]  * number of target features: 0.\n","[2021-04-20 03:14:33,445 INFO] Building `Fields` object...\n","[2021-04-20 03:14:33,445 INFO] Building & saving training data...\n","[2021-04-20 03:14:33,458 INFO] Building shard 0.\n","[2021-04-20 03:14:33,461 INFO]  * saving 0th train data shard to sif_climate/output_climate_50/en-vi.train.0.pt.\n","[2021-04-20 03:14:33,557 INFO]  * tgt vocab size: 550.\n","[2021-04-20 03:14:33,557 INFO]  * src vocab size: 475.\n","[2021-04-20 03:14:47,587 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:14:49,182 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:14:49,186 INFO]  * src vocab size = 39660\n","[2021-04-20 03:14:49,186 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:14:49,186 INFO] Building model...\n","[2021-04-20 03:14:56,164 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:14:56,181 INFO] encoder: 39221248\n","[2021-04-20 03:14:56,181 INFO] decoder: 43931466\n","[2021-04-20 03:14:56,181 INFO] * number of parameters: 83152714\n","[2021-04-20 03:14:56,710 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:14:56,710 INFO] Start training loop without validation...\n","[2021-04-20 03:14:56,710 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:56,713 INFO] number of examples: 45\n","[2021-04-20 03:14:56,721 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:56,724 INFO] number of examples: 45\n","[2021-04-20 03:14:57,180 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:57,184 INFO] number of examples: 45\n","[2021-04-20 03:14:57,187 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:57,190 INFO] number of examples: 45\n","[2021-04-20 03:14:57,573 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:57,576 INFO] number of examples: 45\n","[2021-04-20 03:14:57,579 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:57,582 INFO] number of examples: 45\n","[2021-04-20 03:14:57,966 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:57,970 INFO] number of examples: 45\n","[2021-04-20 03:14:57,973 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:57,976 INFO] number of examples: 45\n","[2021-04-20 03:14:58,362 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:58,365 INFO] number of examples: 45\n","[2021-04-20 03:14:58,368 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:14:58,372 INFO] number of examples: 45\n","[2021-04-20 03:14:58,758 INFO] Step 30005/30020; acc:  49.65; ppl: 29.71; xent: 3.39; lr: 0.00051; 3988/5833 tok/s;      2 sec\n","[2021-04-20 03:14:58,970 INFO] Saving checkpoint sif_climate/model_climate_50/en-vi_step_30005.pt\n","[2021-04-20 03:15:03,286 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:03,289 INFO] number of examples: 45\n","[2021-04-20 03:15:03,292 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:03,295 INFO] number of examples: 45\n","[2021-04-20 03:15:03,693 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:03,696 INFO] number of examples: 45\n","[2021-04-20 03:15:03,700 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:03,702 INFO] number of examples: 45\n","[2021-04-20 03:15:04,086 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:04,090 INFO] number of examples: 45\n","[2021-04-20 03:15:04,094 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:04,097 INFO] number of examples: 45\n","[2021-04-20 03:15:04,479 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:04,483 INFO] number of examples: 45\n","[2021-04-20 03:15:04,486 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:04,489 INFO] number of examples: 45\n","[2021-04-20 03:15:04,877 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:04,881 INFO] number of examples: 45\n","[2021-04-20 03:15:04,884 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:04,887 INFO] number of examples: 45\n","[2021-04-20 03:15:05,270 INFO] Step 30010/30020; acc:  85.72; ppl:  2.80; xent: 1.03; lr: 0.00051; 1255/1835 tok/s;      9 sec\n","[2021-04-20 03:15:05,471 INFO] Saving checkpoint sif_climate/model_climate_50/en-vi_step_30010.pt\n","[2021-04-20 03:15:09,558 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:09,561 INFO] number of examples: 45\n","[2021-04-20 03:15:09,565 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:09,568 INFO] number of examples: 45\n","[2021-04-20 03:15:09,951 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:09,954 INFO] number of examples: 45\n","[2021-04-20 03:15:09,957 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:09,960 INFO] number of examples: 45\n","[2021-04-20 03:15:10,342 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:10,345 INFO] number of examples: 45\n","[2021-04-20 03:15:10,348 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:10,351 INFO] number of examples: 45\n","[2021-04-20 03:15:10,734 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:10,738 INFO] number of examples: 45\n","[2021-04-20 03:15:10,741 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:10,744 INFO] number of examples: 45\n","[2021-04-20 03:15:11,126 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:11,129 INFO] number of examples: 45\n","[2021-04-20 03:15:11,132 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:11,135 INFO] number of examples: 45\n","[2021-04-20 03:15:11,517 INFO] Step 30015/30020; acc:  93.76; ppl:  1.54; xent: 0.43; lr: 0.00051; 1308/1913 tok/s;     15 sec\n","[2021-04-20 03:15:11,820 INFO] Saving checkpoint sif_climate/model_climate_50/en-vi_step_30015.pt\n","[2021-04-20 03:15:20,899 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:20,906 INFO] number of examples: 45\n","[2021-04-20 03:15:20,913 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:20,917 INFO] number of examples: 45\n","[2021-04-20 03:15:21,328 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:21,332 INFO] number of examples: 45\n","[2021-04-20 03:15:21,337 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:21,341 INFO] number of examples: 45\n","[2021-04-20 03:15:21,795 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:21,799 INFO] number of examples: 45\n","[2021-04-20 03:15:21,802 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:21,808 INFO] number of examples: 45\n","[2021-04-20 03:15:22,221 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:22,224 INFO] number of examples: 45\n","[2021-04-20 03:15:22,228 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:22,232 INFO] number of examples: 45\n","[2021-04-20 03:15:22,635 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:22,639 INFO] number of examples: 45\n","[2021-04-20 03:15:22,643 INFO] Loading dataset from sif_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:15:22,646 INFO] number of examples: 45\n","[2021-04-20 03:15:23,041 INFO] Step 30020/30020; acc:  98.74; ppl:  1.20; xent: 0.18; lr: 0.00051; 709/1037 tok/s;     26 sec\n","[2021-04-20 03:15:23,276 INFO] Saving checkpoint sif_climate/model_climate_50/en-vi_step_30020.pt\n","[2021-04-20 03:15:39,059 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:16:22,541 INFO] PRED AVG SCORE: -0.8138, PRED PPL: 2.2565\n","[2021-04-20 03:16:22,542 INFO] GOLD AVG SCORE: -4.8471, GOLD PPL: 127.3742\n","BLEU = 12.91, 37.7/18.0/8.9/4.6 (BP=1.000, ratio=1.041, hyp_len=2714, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:16:29,137 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:17:00,157 INFO] PRED AVG SCORE: -1.2163, PRED PPL: 3.3746\n","[2021-04-20 03:17:00,157 INFO] GOLD AVG SCORE: -6.0164, GOLD PPL: 410.0893\n","BLEU = 6.83, 30.7/13.1/5.9/2.8 (BP=0.755, ratio=0.781, hyp_len=2035, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:17:04,475 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:17:27,770 INFO] PRED AVG SCORE: -0.5736, PRED PPL: 1.7747\n","[2021-04-20 03:17:27,770 INFO] GOLD AVG SCORE: -6.2886, GOLD PPL: 538.4099\n","BLEU = 2.12, 18.0/6.3/2.6/0.9 (BP=0.529, ratio=0.611, hyp_len=1593, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:17:32,118 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:17:55,559 INFO] PRED AVG SCORE: -0.9122, PRED PPL: 2.4899\n","[2021-04-20 03:17:55,559 INFO] GOLD AVG SCORE: -5.4901, GOLD PPL: 242.2726\n","BLEU = 3.15, 23.9/9.0/4.1/1.7 (BP=0.508, ratio=0.596, hyp_len=1554, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBsEa57JRgsD","executionInfo":{"status":"ok","timestamp":1618889321302,"user_tz":-420,"elapsed":215796,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"f57746e1-14d1-49f8-bb74-a0ff7c2f7b4d"},"source":["!mkdir -p sif_climate/output_climate_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_100' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_100' \\\\\n","-save_data 'sif_climate/output_climate_100/en-vi' \n","\n","!mkdir -p sif_climate/model_climate_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_climate/output_climate_100/en-vi' \\\\\n","-save_model 'sif_climate/model_climate_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_climate/model_climate_100/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_climate/model_climate_100/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_climate/model_climate_100/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_climate/model_climate_100/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:25:07,201 INFO] Extracting features...\n","[2021-04-20 03:25:07,771 INFO]  * number of source features: 0.\n","[2021-04-20 03:25:07,771 INFO]  * number of target features: 0.\n","[2021-04-20 03:25:07,771 INFO] Building `Fields` object...\n","[2021-04-20 03:25:07,771 INFO] Building & saving training data...\n","[2021-04-20 03:25:07,785 INFO] Building shard 0.\n","[2021-04-20 03:25:07,790 INFO]  * saving 0th train data shard to sif_climate/output_climate_100/en-vi.train.0.pt.\n","[2021-04-20 03:25:07,886 INFO]  * tgt vocab size: 864.\n","[2021-04-20 03:25:07,887 INFO]  * src vocab size: 875.\n","[2021-04-20 03:25:11,175 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:25:12,903 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:25:12,908 INFO]  * src vocab size = 39660\n","[2021-04-20 03:25:12,908 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:25:12,908 INFO] Building model...\n","[2021-04-20 03:25:16,586 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:25:16,683 INFO] encoder: 39221248\n","[2021-04-20 03:25:16,683 INFO] decoder: 43931466\n","[2021-04-20 03:25:16,683 INFO] * number of parameters: 83152714\n","[2021-04-20 03:25:17,234 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:25:17,234 INFO] Start training loop without validation...\n","[2021-04-20 03:25:17,235 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:17,239 INFO] number of examples: 90\n","[2021-04-20 03:25:17,660 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:17,665 INFO] number of examples: 90\n","[2021-04-20 03:25:18,058 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:18,063 INFO] number of examples: 90\n","[2021-04-20 03:25:18,452 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:18,457 INFO] number of examples: 90\n","[2021-04-20 03:25:18,850 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:18,857 INFO] number of examples: 90\n","[2021-04-20 03:25:19,244 INFO] Step 30005/30020; acc:  41.59; ppl: 50.72; xent: 3.93; lr: 0.00051; 4285/6178 tok/s;      2 sec\n","[2021-04-20 03:25:19,462 INFO] Saving checkpoint sif_climate/model_climate_100/en-vi_step_30005.pt\n","[2021-04-20 03:25:24,664 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:24,669 INFO] number of examples: 90\n","[2021-04-20 03:25:25,065 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:25,071 INFO] number of examples: 90\n","[2021-04-20 03:25:25,457 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:25,462 INFO] number of examples: 90\n","[2021-04-20 03:25:25,848 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:25,853 INFO] number of examples: 90\n","[2021-04-20 03:25:26,236 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:26,241 INFO] number of examples: 90\n","[2021-04-20 03:25:26,630 INFO] Step 30010/30020; acc:  74.46; ppl:  4.82; xent: 1.57; lr: 0.00051; 1166/1681 tok/s;      9 sec\n","[2021-04-20 03:25:26,858 INFO] Saving checkpoint sif_climate/model_climate_100/en-vi_step_30010.pt\n","[2021-04-20 03:25:31,206 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:31,883 INFO] number of examples: 90\n","[2021-04-20 03:25:32,285 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:32,291 INFO] number of examples: 90\n","[2021-04-20 03:25:32,674 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:32,680 INFO] number of examples: 90\n","[2021-04-20 03:25:33,062 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:33,068 INFO] number of examples: 90\n","[2021-04-20 03:25:33,453 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:33,458 INFO] number of examples: 90\n","[2021-04-20 03:25:33,844 INFO] Step 30015/30020; acc:  89.19; ppl:  1.95; xent: 0.67; lr: 0.00051; 1194/1721 tok/s;     17 sec\n","[2021-04-20 03:25:34,055 INFO] Saving checkpoint sif_climate/model_climate_100/en-vi_step_30015.pt\n","[2021-04-20 03:25:39,494 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:39,518 INFO] number of examples: 90\n","[2021-04-20 03:25:39,943 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:39,949 INFO] number of examples: 90\n","[2021-04-20 03:25:40,369 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:40,376 INFO] number of examples: 90\n","[2021-04-20 03:25:40,816 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:40,823 INFO] number of examples: 90\n","[2021-04-20 03:25:41,255 INFO] Loading dataset from sif_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:25:41,263 INFO] number of examples: 90\n","[2021-04-20 03:25:41,688 INFO] Step 30020/30020; acc:  96.83; ppl:  1.29; xent: 0.25; lr: 0.00051; 1098/1583 tok/s;     24 sec\n","[2021-04-20 03:25:41,937 INFO] Saving checkpoint sif_climate/model_climate_100/en-vi_step_30020.pt\n","[2021-04-20 03:26:11,132 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:26:57,731 INFO] PRED AVG SCORE: -0.8067, PRED PPL: 2.2405\n","[2021-04-20 03:26:57,731 INFO] GOLD AVG SCORE: -4.5646, GOLD PPL: 96.0218\n","BLEU = 13.17, 37.7/17.8/8.9/5.0 (BP=1.000, ratio=1.083, hyp_len=2822, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:27:02,272 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:27:39,025 INFO] PRED AVG SCORE: -0.9039, PRED PPL: 2.4693\n","[2021-04-20 03:27:39,026 INFO] GOLD AVG SCORE: -4.8199, GOLD PPL: 123.9538\n","BLEU = 11.26, 36.7/17.3/8.2/4.3 (BP=0.924, ratio=0.927, hyp_len=2415, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:27:43,468 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:28:09,492 INFO] PRED AVG SCORE: -0.8844, PRED PPL: 2.4215\n","[2021-04-20 03:28:09,492 INFO] GOLD AVG SCORE: -5.4583, GOLD PPL: 234.6868\n","BLEU = 4.79, 29.9/13.0/5.6/2.6 (BP=0.554, ratio=0.629, hyp_len=1638, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:28:13,890 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:28:40,314 INFO] PRED AVG SCORE: -0.8059, PRED PPL: 2.2388\n","[2021-04-20 03:28:40,314 INFO] GOLD AVG SCORE: -5.3108, GOLD PPL: 202.5058\n","BLEU = 6.45, 30.0/13.2/6.0/3.1 (BP=0.695, ratio=0.733, hyp_len=1911, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMljWicBRswk","executionInfo":{"status":"ok","timestamp":1618889528777,"user_tz":-420,"elapsed":207455,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"1b436468-2332-485a-d395-96e2f95a1ef4"},"source":["!mkdir -p sif_climate/output_climate_120\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_120' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_120' \\\\\n","-save_data 'sif_climate/output_climate_120/en-vi' \n","\n","!mkdir -p sif_climate/model_climate_120\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_climate/output_climate_120/en-vi' \\\\\n","-save_model 'sif_climate/model_climate_120/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_climate/model_climate_120/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_120/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_120/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_climate/model_climate_120/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_120/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_120/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_climate/model_climate_120/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_120/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_120/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_climate/model_climate_120/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/model_climate_120/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/model_climate_120/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:28:43,260 INFO] Extracting features...\n","[2021-04-20 03:28:43,262 INFO]  * number of source features: 0.\n","[2021-04-20 03:28:43,262 INFO]  * number of target features: 0.\n","[2021-04-20 03:28:43,262 INFO] Building `Fields` object...\n","[2021-04-20 03:28:43,262 INFO] Building & saving training data...\n","[2021-04-20 03:28:43,263 WARNING] Shards for corpus train already exist, won't be overwritten, pass the `-overwrite` option if you want to.\n","[2021-04-20 03:28:46,638 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:28:48,312 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:28:48,317 INFO]  * src vocab size = 39660\n","[2021-04-20 03:28:48,317 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:28:48,317 INFO] Building model...\n","[2021-04-20 03:28:51,931 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:28:51,961 INFO] encoder: 39221248\n","[2021-04-20 03:28:51,961 INFO] decoder: 43931466\n","[2021-04-20 03:28:51,961 INFO] * number of parameters: 83152714\n","[2021-04-20 03:28:52,538 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:28:52,538 INFO] Start training loop without validation...\n","[2021-04-20 03:28:52,539 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:28:52,548 INFO] number of examples: 108\n","[2021-04-20 03:28:52,996 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:28:53,001 INFO] number of examples: 108\n","[2021-04-20 03:28:53,389 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:28:53,394 INFO] number of examples: 108\n","[2021-04-20 03:28:53,788 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:28:53,794 INFO] number of examples: 108\n","[2021-04-20 03:28:54,185 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:28:54,192 INFO] number of examples: 108\n","[2021-04-20 03:28:54,585 INFO] Step 30005/30020; acc:  39.60; ppl: 60.19; xent: 4.10; lr: 0.00051; 5093/7331 tok/s;      2 sec\n","[2021-04-20 03:28:54,778 INFO] Saving checkpoint sif_climate/model_climate_120/en-vi_step_30005.pt\n","[2021-04-20 03:29:00,385 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:00,394 INFO] number of examples: 108\n","[2021-04-20 03:29:00,802 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:00,808 INFO] number of examples: 108\n","[2021-04-20 03:29:01,200 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:01,206 INFO] number of examples: 108\n","[2021-04-20 03:29:01,594 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:01,599 INFO] number of examples: 108\n","[2021-04-20 03:29:01,986 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:01,992 INFO] number of examples: 108\n","[2021-04-20 03:29:02,391 INFO] Step 30010/30020; acc:  70.18; ppl:  6.11; xent: 1.81; lr: 0.00051; 1336/1922 tok/s;     10 sec\n","[2021-04-20 03:29:02,597 INFO] Saving checkpoint sif_climate/model_climate_120/en-vi_step_30010.pt\n","[2021-04-20 03:29:07,087 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:07,097 INFO] number of examples: 108\n","[2021-04-20 03:29:07,488 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:07,494 INFO] number of examples: 108\n","[2021-04-20 03:29:07,887 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:07,893 INFO] number of examples: 108\n","[2021-04-20 03:29:08,286 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:08,292 INFO] number of examples: 108\n","[2021-04-20 03:29:08,684 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:08,690 INFO] number of examples: 108\n","[2021-04-20 03:29:09,084 INFO] Step 30015/30020; acc:  86.62; ppl:  2.20; xent: 0.79; lr: 0.00051; 1558/2242 tok/s;     17 sec\n","[2021-04-20 03:29:09,280 INFO] Saving checkpoint sif_climate/model_climate_120/en-vi_step_30015.pt\n","[2021-04-20 03:29:16,666 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:16,785 INFO] number of examples: 108\n","[2021-04-20 03:29:17,243 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:17,253 INFO] number of examples: 108\n","[2021-04-20 03:29:17,704 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:17,713 INFO] number of examples: 108\n","[2021-04-20 03:29:18,168 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:18,176 INFO] number of examples: 108\n","[2021-04-20 03:29:18,619 INFO] Loading dataset from sif_climate/output_climate_120/en-vi.train.0.pt\n","[2021-04-20 03:29:18,627 INFO] number of examples: 108\n","[2021-04-20 03:29:19,093 INFO] Step 30020/30020; acc:  95.52; ppl:  1.35; xent: 0.30; lr: 0.00051; 1042/1499 tok/s;     27 sec\n","[2021-04-20 03:29:19,408 INFO] Saving checkpoint sif_climate/model_climate_120/en-vi_step_30020.pt\n","[2021-04-20 03:29:42,420 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:30:30,337 INFO] PRED AVG SCORE: -0.8394, PRED PPL: 2.3150\n","[2021-04-20 03:30:30,337 INFO] GOLD AVG SCORE: -4.5063, GOLD PPL: 90.5847\n","BLEU = 14.03, 39.4/19.3/9.5/5.3 (BP=1.000, ratio=1.062, hyp_len=2768, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:30:34,636 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:31:07,577 INFO] PRED AVG SCORE: -0.8483, PRED PPL: 2.3356\n","[2021-04-20 03:31:07,577 INFO] GOLD AVG SCORE: -4.5289, GOLD PPL: 92.6589\n","BLEU = 13.12, 39.8/19.3/10.0/5.8 (BP=0.900, ratio=0.905, hyp_len=2358, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:31:12,000 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:31:37,262 INFO] PRED AVG SCORE: -0.8255, PRED PPL: 2.2830\n","[2021-04-20 03:31:37,262 INFO] GOLD AVG SCORE: -5.1731, GOLD PPL: 176.4602\n","BLEU = 5.58, 29.6/13.7/6.5/3.5 (BP=0.570, ratio=0.640, hyp_len=1668, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:31:41,569 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:32:07,790 INFO] PRED AVG SCORE: -0.7646, PRED PPL: 2.1480\n","[2021-04-20 03:32:07,791 INFO] GOLD AVG SCORE: -5.1832, GOLD PPL: 178.2567\n","BLEU = 5.80, 30.7/13.3/5.3/2.5 (BP=0.677, ratio=0.719, hyp_len=1875, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889565028,"user_tz":-420,"elapsed":243695,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"03f2512e-e0b1-45e9-dab7-3a84a1a0577f"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_climate.en -tgt finetune/test_climate.vi -output sif_climate/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < sif_climate/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":10,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:32:14,800 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:32:44,106 INFO] PRED AVG SCORE: -1.0597, PRED PPL: 2.8854\n","[2021-04-20 03:32:44,106 INFO] GOLD AVG SCORE: -6.4587, GOLD PPL: 638.2229\n","BLEU = 12.20, 36.0/18.2/9.9/6.1 (BP=0.866, ratio=0.875, hyp_len=2279, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}