{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-20210420-0301 SIF catechism.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892195477,"user_tz":-420,"elapsed":22578,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2def3d49-5b0b-4db8-8520-9ad5e07fefb4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892197762,"user_tz":-420,"elapsed":2265,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ede755e9-6497-45e3-94d1-a03b073411d7"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888369728,"user_tz":-420,"elapsed":801,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"02eeb39a-0b26-46b1-be15-824372db85ce"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:12:49 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892213024,"user_tz":-420,"elapsed":17513,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"d6a0aca9-5218-4181-e3e3-5ae6f329fb54"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 26.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 24.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Collecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=23ece1694b031faf4ed242e0da131eb858dcf99a53976491503081b6e2c0730a\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: configargparse, waitress, torchtext, pyonmttok, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618887841008,"user_tz":-420,"elapsed":1781,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2d9dcd2-26ed-49c2-a523-9ad4d474e26a"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","!tar -xvf 'SIF-finetune.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 03:03:59--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 97919 (96K) [application/octet-stream]\n","Saving to: ‘SIF-finetune.tar.gz’\n","\n","\rSIF-finetune.tar.gz   0%[                    ]       0  --.-KB/s               \rSIF-finetune.tar.gz 100%[===================>]  95.62K  --.-KB/s    in 0.008s  \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 03:03:59 (12.0 MB/s) - ‘SIF-finetune.tar.gz’ saved [97919/97919]\n","\n","finetune/\n","finetune/en_finetune_law_50\n","finetune/en_finetune_law_95\n","finetune/en_finetune_buddhism_50\n","finetune/en_finetune_buddhism_100\n","finetune/en_finetune_buddhism_150\n","finetune/en_finetune_buddhism_193\n","finetune/en_finetune_climate_50\n","finetune/en_finetune_climate_100\n","finetune/en_finetune_climate_120\n","finetune/en_finetune_catechism_50\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_150\n","finetune/en_finetune_catechism_152\n","finetune/vi_finetune_law_50\n","finetune/vi_finetune_law_95\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_buddhism_193\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_100\n","finetune/vi_finetune_climate_120\n","finetune/vi_finetune_catechism_50\n","finetune/vi_finetune_catechism_100\n","finetune/vi_finetune_catechism_150\n","finetune/vi_finetune_catechism_152\n","finetune/test_law.vi\n","finetune/test_buddhism.vi\n","finetune/test_climate.vi\n","finetune/test_catechism.vi\n","finetune/test_law.en\n","finetune/test_buddhism.en\n","finetune/test_climate.en\n","finetune/test_catechism.en\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618887850544,"user_tz":-420,"elapsed":872,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"38a22dd4-2b96-4702-b250-82d5a7368a6a"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 491\n","-rw------- 1 root root  9813 Apr 20 02:12 en_finetune_buddhism_100\n","-rw------- 1 root root 14812 Apr 20 02:12 en_finetune_buddhism_150\n","-rw------- 1 root root 19723 Apr 20 02:12 en_finetune_buddhism_193\n","-rw------- 1 root root  5246 Apr 20 02:12 en_finetune_buddhism_50\n","-rw------- 1 root root 13614 Apr 20 02:12 en_finetune_catechism_100\n","-rw------- 1 root root 20281 Apr 20 02:12 en_finetune_catechism_150\n","-rw------- 1 root root 20539 Apr 20 02:12 en_finetune_catechism_152\n","-rw------- 1 root root  7151 Apr 20 02:12 en_finetune_catechism_50\n","-rw------- 1 root root 13647 Apr 20 02:12 en_finetune_climate_100\n","-rw------- 1 root root 16493 Apr 20 02:12 en_finetune_climate_120\n","-rw------- 1 root root  6498 Apr 20 02:12 en_finetune_climate_50\n","-rw------- 1 root root  6664 Apr 20 02:12 en_finetune_law_50\n","-rw------- 1 root root 12797 Apr 20 02:12 en_finetune_law_95\n","-rw------- 1 root root  7711 Apr 20 02:12 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 02:12 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 02:12 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 02:12 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 02:12 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 02:12 test_climate.vi\n","-rw------- 1 root root 11267 Apr 20 02:12 test_law.en\n","-rw------- 1 root root 15336 Apr 20 02:12 test_law.vi\n","-rw------- 1 root root 13723 Apr 20 02:12 vi_finetune_buddhism_100\n","-rw------- 1 root root 21466 Apr 20 02:12 vi_finetune_buddhism_150\n","-rw------- 1 root root 28181 Apr 20 02:12 vi_finetune_buddhism_193\n","-rw------- 1 root root  7370 Apr 20 02:12 vi_finetune_buddhism_50\n","-rw------- 1 root root 18512 Apr 20 02:12 vi_finetune_catechism_100\n","-rw------- 1 root root 27677 Apr 20 02:12 vi_finetune_catechism_150\n","-rw------- 1 root root 28052 Apr 20 02:12 vi_finetune_catechism_152\n","-rw------- 1 root root  9740 Apr 20 02:12 vi_finetune_catechism_50\n","-rw------- 1 root root 18253 Apr 20 02:12 vi_finetune_climate_100\n","-rw------- 1 root root 22006 Apr 20 02:12 vi_finetune_climate_120\n","-rw------- 1 root root  8917 Apr 20 02:12 vi_finetune_climate_50\n","-rw------- 1 root root  8413 Apr 20 02:12 vi_finetune_law_50\n","-rw------- 1 root root 16118 Apr 20 02:12 vi_finetune_law_95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618887872728,"user_tz":-420,"elapsed":19319,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"dea9bd89-6e12-4441-eb1e-6a1eaeb208c4"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 24.10 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618893774333,"user_tz":-420,"elapsed":174832,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"c9b73885-474e-47eb-9dc1-d0da06ae6cb4"},"source":["!mkdir -p sif_catechism/output_catechism_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_50' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_50' \\\\\n","-save_data 'sif_catechism/output_catechism_50/en-vi' \n","\n","!mkdir -p sif_catechism/model_catechism_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_catechism/output_catechism_50/en-vi' \\\\\n","-save_model 'sif_catechism/model_catechism_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_catechism/model_catechism_50/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_50/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_50/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_50/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:40:01,025 INFO] Extracting features...\n","[2021-04-20 04:40:01,028 INFO]  * number of source features: 0.\n","[2021-04-20 04:40:01,028 INFO]  * number of target features: 0.\n","[2021-04-20 04:40:01,028 INFO] Building `Fields` object...\n","[2021-04-20 04:40:01,028 INFO] Building & saving training data...\n","[2021-04-20 04:40:01,029 WARNING] Shards for corpus train already exist, won't be overwritten, pass the `-overwrite` option if you want to.\n","[2021-04-20 04:40:03,788 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:40:05,158 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:40:05,161 INFO]  * src vocab size = 39660\n","[2021-04-20 04:40:05,161 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:40:05,161 INFO] Building model...\n","[2021-04-20 04:40:09,551 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:40:09,643 INFO] encoder: 39221248\n","[2021-04-20 04:40:09,643 INFO] decoder: 43931466\n","[2021-04-20 04:40:09,643 INFO] * number of parameters: 83152714\n","[2021-04-20 04:40:10,216 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:40:10,216 INFO] Start training loop without validation...\n","[2021-04-20 04:40:10,216 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,219 INFO] number of examples: 42\n","[2021-04-20 04:40:10,225 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,229 INFO] number of examples: 42\n","[2021-04-20 04:40:10,480 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,483 INFO] number of examples: 42\n","[2021-04-20 04:40:10,486 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,488 INFO] number of examples: 42\n","[2021-04-20 04:40:10,706 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,709 INFO] number of examples: 42\n","[2021-04-20 04:40:10,712 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,715 INFO] number of examples: 42\n","[2021-04-20 04:40:10,932 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,935 INFO] number of examples: 42\n","[2021-04-20 04:40:10,938 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:10,941 INFO] number of examples: 42\n","[2021-04-20 04:40:11,156 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:11,159 INFO] number of examples: 42\n","[2021-04-20 04:40:11,162 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:11,164 INFO] number of examples: 42\n","[2021-04-20 04:40:11,379 INFO] Step 30005/30020; acc:  25.80; ppl: 204.18; xent: 5.32; lr: 0.00051; 7432/9617 tok/s;      1 sec\n","[2021-04-20 04:40:11,569 INFO] Saving checkpoint sif_catechism/model_catechism_50/en-vi_step_30005.pt\n","[2021-04-20 04:40:15,984 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:15,988 INFO] number of examples: 42\n","[2021-04-20 04:40:15,992 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:15,995 INFO] number of examples: 42\n","[2021-04-20 04:40:16,226 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,230 INFO] number of examples: 42\n","[2021-04-20 04:40:16,233 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,236 INFO] number of examples: 42\n","[2021-04-20 04:40:16,456 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,459 INFO] number of examples: 42\n","[2021-04-20 04:40:16,462 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,464 INFO] number of examples: 42\n","[2021-04-20 04:40:16,683 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,686 INFO] number of examples: 42\n","[2021-04-20 04:40:16,689 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,692 INFO] number of examples: 42\n","[2021-04-20 04:40:16,909 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,912 INFO] number of examples: 42\n","[2021-04-20 04:40:16,914 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:16,917 INFO] number of examples: 42\n","[2021-04-20 04:40:17,134 INFO] Step 30010/30020; acc:  67.34; ppl:  7.40; xent: 2.00; lr: 0.00051; 1501/1943 tok/s;      7 sec\n","[2021-04-20 04:40:17,328 INFO] Saving checkpoint sif_catechism/model_catechism_50/en-vi_step_30010.pt\n","[2021-04-20 04:40:21,942 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:21,946 INFO] number of examples: 42\n","[2021-04-20 04:40:21,950 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:21,954 INFO] number of examples: 42\n","[2021-04-20 04:40:22,190 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,194 INFO] number of examples: 42\n","[2021-04-20 04:40:22,198 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,201 INFO] number of examples: 42\n","[2021-04-20 04:40:22,441 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,444 INFO] number of examples: 42\n","[2021-04-20 04:40:22,447 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,450 INFO] number of examples: 42\n","[2021-04-20 04:40:22,677 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,680 INFO] number of examples: 42\n","[2021-04-20 04:40:22,683 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,686 INFO] number of examples: 42\n","[2021-04-20 04:40:22,913 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,916 INFO] number of examples: 42\n","[2021-04-20 04:40:22,919 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:22,922 INFO] number of examples: 42\n","[2021-04-20 04:40:23,157 INFO] Step 30015/30020; acc:  88.05; ppl:  1.95; xent: 0.67; lr: 0.00051; 1435/1856 tok/s;     13 sec\n","[2021-04-20 04:40:23,371 INFO] Saving checkpoint sif_catechism/model_catechism_50/en-vi_step_30015.pt\n","[2021-04-20 04:40:35,170 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:35,537 INFO] number of examples: 42\n","[2021-04-20 04:40:35,541 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:35,544 INFO] number of examples: 42\n","[2021-04-20 04:40:35,774 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:35,781 INFO] number of examples: 42\n","[2021-04-20 04:40:35,785 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:35,788 INFO] number of examples: 42\n","[2021-04-20 04:40:36,015 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:36,018 INFO] number of examples: 42\n","[2021-04-20 04:40:36,021 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:36,024 INFO] number of examples: 42\n","[2021-04-20 04:40:36,250 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:36,254 INFO] number of examples: 42\n","[2021-04-20 04:40:36,258 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:36,261 INFO] number of examples: 42\n","[2021-04-20 04:40:36,489 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:36,492 INFO] number of examples: 42\n","[2021-04-20 04:40:36,495 INFO] Loading dataset from sif_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:40:36,498 INFO] number of examples: 42\n","[2021-04-20 04:40:36,717 INFO] Step 30020/30020; acc:  96.82; ppl:  1.28; xent: 0.25; lr: 0.00051; 637/824 tok/s;     27 sec\n","[2021-04-20 04:40:36,923 INFO] Saving checkpoint sif_catechism/model_catechism_50/en-vi_step_30020.pt\n","[2021-04-20 04:40:52,872 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:41:15,363 INFO] PRED AVG SCORE: -1.1305, PRED PPL: 3.0972\n","[2021-04-20 04:41:15,363 INFO] GOLD AVG SCORE: -6.4551, GOLD PPL: 635.9096\n","BLEU = 2.72, 25.6/6.2/2.2/0.6 (BP=0.719, ratio=0.752, hyp_len=1736, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:41:19,579 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:42:05,612 INFO] PRED AVG SCORE: -1.0069, PRED PPL: 2.7372\n","[2021-04-20 04:42:05,612 INFO] GOLD AVG SCORE: -7.3348, GOLD PPL: 1532.7924\n","BLEU = 1.96, 14.0/4.0/1.0/0.3 (BP=1.000, ratio=1.459, hyp_len=3368, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:42:09,539 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:42:28,350 INFO] PRED AVG SCORE: -0.5756, PRED PPL: 1.7781\n","[2021-04-20 04:42:28,350 INFO] GOLD AVG SCORE: -7.5839, GOLD PPL: 1966.3471\n","Use of uninitialized value in division (/) at OpenNMT-py/tools/multi-bleu.perl line 139, <STDIN> line 80.\n","BLEU = 0.00, 12.3/2.4/0.5/0.0 (BP=0.584, ratio=0.650, hyp_len=1500, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:42:32,203 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:42:53,431 INFO] PRED AVG SCORE: -0.6817, PRED PPL: 1.9772\n","[2021-04-20 04:42:53,432 INFO] GOLD AVG SCORE: -7.3300, GOLD PPL: 1525.3388\n","BLEU = 1.15, 14.4/3.1/0.8/0.1 (BP=0.794, ratio=0.813, hyp_len=1876, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1s0xD4M-nCFU","executionInfo":{"status":"ok","timestamp":1618894146433,"user_tz":-420,"elapsed":1797,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ed42e169-6b9d-432b-95ab-74ff9a299995","colab":{"base_uri":"https://localhost:8080/"}},"source":["# !onmt_translate -model sif_catechism/model_catechism_50/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","# !onmt_translate -model sif_catechism/model_catechism_50/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":10,"outputs":[{"output_type":"stream","text":["Use of uninitialized value in division (/) at OpenNMT-py/tools/multi-bleu.perl line 139, <STDIN> line 80.\n","BLEU = 0.00, 12.3/2.4/0.5/0.0 (BP=0.584, ratio=0.650, hyp_len=1500, ref_len=2308)\n"," ===15==^======= \n","BLEU = 1.15, 14.4/3.1/0.8/0.1 (BP=0.794, ratio=0.813, hyp_len=1876, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XBsEa57JRgsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892410611,"user_tz":-420,"elapsed":169445,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"5e6530b4-1849-44f6-b7a9-7b09842b1b18"},"source":["!mkdir -p sif_catechism/output_catechism_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_100' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_100' \\\\\n","-save_data 'sif_catechism/output_catechism_100/en-vi' \n","\n","!mkdir -p sif_catechism/model_catechism_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_catechism/output_catechism_100/en-vi' \\\\\n","-save_model 'sif_catechism/model_catechism_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_catechism/model_catechism_100/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_100/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_100/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_100/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":4,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:17:25,007 INFO] Extracting features...\n","[2021-04-20 04:17:25,820 INFO]  * number of source features: 0.\n","[2021-04-20 04:17:25,820 INFO]  * number of target features: 0.\n","[2021-04-20 04:17:25,820 INFO] Building `Fields` object...\n","[2021-04-20 04:17:25,820 INFO] Building & saving training data...\n","[2021-04-20 04:17:25,831 INFO] Building shard 0.\n","[2021-04-20 04:17:25,835 INFO]  * saving 0th train data shard to sif_catechism/output_catechism_100/en-vi.train.0.pt.\n","[2021-04-20 04:17:25,931 INFO]  * tgt vocab size: 847.\n","[2021-04-20 04:17:25,932 INFO]  * src vocab size: 769.\n","[2021-04-20 04:17:44,311 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:17:45,560 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:17:45,564 INFO]  * src vocab size = 39660\n","[2021-04-20 04:17:45,564 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:17:45,564 INFO] Building model...\n","[2021-04-20 04:17:52,798 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:17:52,886 INFO] encoder: 39221248\n","[2021-04-20 04:17:52,886 INFO] decoder: 43931466\n","[2021-04-20 04:17:52,886 INFO] * number of parameters: 83152714\n","[2021-04-20 04:17:53,443 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:17:53,443 INFO] Start training loop without validation...\n","[2021-04-20 04:17:53,443 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:53,448 INFO] number of examples: 87\n","[2021-04-20 04:17:53,784 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:53,788 INFO] number of examples: 87\n","[2021-04-20 04:17:54,032 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:54,037 INFO] number of examples: 87\n","[2021-04-20 04:17:54,278 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:54,283 INFO] number of examples: 87\n","[2021-04-20 04:17:54,525 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:54,529 INFO] number of examples: 87\n","[2021-04-20 04:17:54,765 INFO] Step 30005/30020; acc:  22.41; ppl: 261.81; xent: 5.57; lr: 0.00051; 6667/8680 tok/s;      1 sec\n","[2021-04-20 04:17:54,955 INFO] Saving checkpoint sif_catechism/model_catechism_100/en-vi_step_30005.pt\n","[2021-04-20 04:17:58,616 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:58,620 INFO] number of examples: 87\n","[2021-04-20 04:17:58,877 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:58,882 INFO] number of examples: 87\n","[2021-04-20 04:17:59,131 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:59,135 INFO] number of examples: 87\n","[2021-04-20 04:17:59,377 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:59,382 INFO] number of examples: 87\n","[2021-04-20 04:17:59,625 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:17:59,629 INFO] number of examples: 87\n","[2021-04-20 04:17:59,867 INFO] Step 30010/30020; acc:  52.01; ppl: 15.56; xent: 2.74; lr: 0.00051; 1727/2248 tok/s;      6 sec\n","[2021-04-20 04:18:00,059 INFO] Saving checkpoint sif_catechism/model_catechism_100/en-vi_step_30010.pt\n","[2021-04-20 04:18:03,818 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:03,822 INFO] number of examples: 87\n","[2021-04-20 04:18:04,066 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:04,070 INFO] number of examples: 87\n","[2021-04-20 04:18:04,311 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:04,316 INFO] number of examples: 87\n","[2021-04-20 04:18:04,557 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:04,562 INFO] number of examples: 87\n","[2021-04-20 04:18:04,803 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:04,807 INFO] number of examples: 87\n","[2021-04-20 04:18:05,054 INFO] Step 30015/30020; acc:  78.95; ppl:  2.96; xent: 1.09; lr: 0.00051; 1699/2211 tok/s;     12 sec\n","[2021-04-20 04:18:05,258 INFO] Saving checkpoint sif_catechism/model_catechism_100/en-vi_step_30015.pt\n","[2021-04-20 04:18:14,562 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:14,568 INFO] number of examples: 87\n","[2021-04-20 04:18:14,839 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:14,843 INFO] number of examples: 87\n","[2021-04-20 04:18:15,099 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:15,103 INFO] number of examples: 87\n","[2021-04-20 04:18:15,352 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:15,357 INFO] number of examples: 87\n","[2021-04-20 04:18:15,608 INFO] Loading dataset from sif_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:18:15,612 INFO] number of examples: 87\n","[2021-04-20 04:18:15,871 INFO] Step 30020/30020; acc:  93.10; ppl:  1.44; xent: 0.37; lr: 0.00051; 814/1060 tok/s;     22 sec\n","[2021-04-20 04:18:16,086 INFO] Saving checkpoint sif_catechism/model_catechism_100/en-vi_step_30020.pt\n","[2021-04-20 04:18:34,550 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:18:54,921 INFO] PRED AVG SCORE: -1.1843, PRED PPL: 3.2685\n","[2021-04-20 04:18:54,921 INFO] GOLD AVG SCORE: -6.1549, GOLD PPL: 471.0146\n","BLEU = 1.33, 29.4/7.6/2.2/0.6 (BP=0.323, ratio=0.470, hyp_len=1084, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:18:59,634 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:19:23,290 INFO] PRED AVG SCORE: -0.9127, PRED PPL: 2.4911\n","[2021-04-20 04:19:23,290 INFO] GOLD AVG SCORE: -6.1837, GOLD PPL: 484.7651\n","BLEU = 3.59, 24.6/6.2/1.9/0.8 (BP=0.920, ratio=0.923, hyp_len=2130, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:19:26,959 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:19:46,027 INFO] PRED AVG SCORE: -0.9363, PRED PPL: 2.5506\n","[2021-04-20 04:19:46,027 INFO] GOLD AVG SCORE: -6.7444, GOLD PPL: 849.2622\n","BLEU = 2.92, 20.4/6.4/2.4/0.9 (BP=0.717, ratio=0.750, hyp_len=1732, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:19:49,698 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:20:09,702 INFO] PRED AVG SCORE: -0.7416, PRED PPL: 2.0993\n","[2021-04-20 04:20:09,702 INFO] GOLD AVG SCORE: -7.2755, GOLD PPL: 1444.5445\n","BLEU = 2.47, 17.9/5.2/1.7/0.5 (BP=0.809, ratio=0.825, hyp_len=1904, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMljWicBRswk","executionInfo":{"status":"ok","timestamp":1618892735034,"user_tz":-420,"elapsed":160963,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"556f0b89-aa24-4103-c58f-45a8138e610b"},"source":["!mkdir -p sif_catechism/output_catechism_150\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_150' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_150' \\\\\n","-save_data 'sif_catechism/output_catechism_150/en-vi' \n","\n","!mkdir -p sif_catechism/model_catechism_150\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_catechism/output_catechism_150/en-vi' \\\\\n","-save_model 'sif_catechism/model_catechism_150/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_catechism/model_catechism_150/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_150/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_150/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_150/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_150/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_150/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_150/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_150/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_150/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_150/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_150/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_150/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:22:56,119 INFO] Extracting features...\n","[2021-04-20 04:22:56,799 INFO]  * number of source features: 0.\n","[2021-04-20 04:22:56,799 INFO]  * number of target features: 0.\n","[2021-04-20 04:22:56,799 INFO] Building `Fields` object...\n","[2021-04-20 04:22:56,799 INFO] Building & saving training data...\n","[2021-04-20 04:22:56,811 INFO] Building shard 0.\n","[2021-04-20 04:22:56,817 INFO]  * saving 0th train data shard to sif_catechism/output_catechism_150/en-vi.train.0.pt.\n","[2021-04-20 04:22:56,912 INFO]  * tgt vocab size: 1085.\n","[2021-04-20 04:22:56,913 INFO]  * src vocab size: 1059.\n","[2021-04-20 04:22:59,626 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:23:00,977 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:23:00,982 INFO]  * src vocab size = 39660\n","[2021-04-20 04:23:00,983 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:23:00,983 INFO] Building model...\n","[2021-04-20 04:23:05,262 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:23:05,361 INFO] encoder: 39221248\n","[2021-04-20 04:23:05,361 INFO] decoder: 43931466\n","[2021-04-20 04:23:05,362 INFO] * number of parameters: 83152714\n","[2021-04-20 04:23:05,933 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:23:05,933 INFO] Start training loop without validation...\n","[2021-04-20 04:23:05,933 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:05,943 INFO] number of examples: 132\n","[2021-04-20 04:23:06,241 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:06,247 INFO] number of examples: 132\n","[2021-04-20 04:23:06,500 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:06,505 INFO] number of examples: 132\n","[2021-04-20 04:23:06,754 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:06,760 INFO] number of examples: 132\n","[2021-04-20 04:23:07,019 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:07,025 INFO] number of examples: 132\n","[2021-04-20 04:23:07,278 INFO] Step 30005/30020; acc:  19.80; ppl: 315.53; xent: 5.75; lr: 0.00051; 10030/13060 tok/s;      1 sec\n","[2021-04-20 04:23:07,465 INFO] Saving checkpoint sif_catechism/model_catechism_150/en-vi_step_30005.pt\n","[2021-04-20 04:23:11,848 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:11,854 INFO] number of examples: 132\n","[2021-04-20 04:23:12,121 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:12,127 INFO] number of examples: 132\n","[2021-04-20 04:23:12,382 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:12,388 INFO] number of examples: 132\n","[2021-04-20 04:23:12,644 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:12,650 INFO] number of examples: 132\n","[2021-04-20 04:23:12,906 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:12,912 INFO] number of examples: 132\n","[2021-04-20 04:23:13,165 INFO] Step 30010/30020; acc:  42.81; ppl: 26.08; xent: 3.26; lr: 0.00051; 2291/2984 tok/s;      7 sec\n","[2021-04-20 04:23:13,352 INFO] Saving checkpoint sif_catechism/model_catechism_150/en-vi_step_30010.pt\n","[2021-04-20 04:23:20,863 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:20,870 INFO] number of examples: 132\n","[2021-04-20 04:23:21,122 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:21,127 INFO] number of examples: 132\n","[2021-04-20 04:23:21,400 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:21,408 INFO] number of examples: 132\n","[2021-04-20 04:23:21,684 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:21,690 INFO] number of examples: 132\n","[2021-04-20 04:23:21,946 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:21,952 INFO] number of examples: 132\n","[2021-04-20 04:23:22,207 INFO] Step 30015/30020; acc:  68.85; ppl:  4.72; xent: 1.55; lr: 0.00051; 1492/1943 tok/s;     16 sec\n","[2021-04-20 04:23:22,438 INFO] Saving checkpoint sif_catechism/model_catechism_150/en-vi_step_30015.pt\n","[2021-04-20 04:23:28,284 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:33,498 INFO] number of examples: 132\n","[2021-04-20 04:23:33,761 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:33,768 INFO] number of examples: 132\n","[2021-04-20 04:23:34,045 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:34,052 INFO] number of examples: 132\n","[2021-04-20 04:23:34,336 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:34,345 INFO] number of examples: 132\n","[2021-04-20 04:23:34,628 INFO] Loading dataset from sif_catechism/output_catechism_150/en-vi.train.0.pt\n","[2021-04-20 04:23:34,636 INFO] number of examples: 132\n","[2021-04-20 04:23:34,923 INFO] Step 30020/30020; acc:  87.97; ppl:  1.72; xent: 0.54; lr: 0.00051; 1061/1381 tok/s;     29 sec\n","[2021-04-20 04:23:35,174 INFO] Saving checkpoint sif_catechism/model_catechism_150/en-vi_step_30020.pt\n","[2021-04-20 04:23:58,570 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:24:17,015 INFO] PRED AVG SCORE: -1.2789, PRED PPL: 3.5927\n","[2021-04-20 04:24:17,015 INFO] GOLD AVG SCORE: -6.0198, GOLD PPL: 411.5094\n","BLEU = 2.04, 31.6/8.7/2.6/0.9 (BP=0.402, ratio=0.523, hyp_len=1207, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:24:20,773 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:24:42,472 INFO] PRED AVG SCORE: -0.9732, PRED PPL: 2.6463\n","[2021-04-20 04:24:42,473 INFO] GOLD AVG SCORE: -5.6506, GOLD PPL: 284.4690\n","BLEU = 3.88, 28.8/7.5/2.3/0.8 (BP=0.875, ratio=0.883, hyp_len=2037, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:24:46,273 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:25:07,296 INFO] PRED AVG SCORE: -0.9497, PRED PPL: 2.5850\n","[2021-04-20 04:25:07,296 INFO] GOLD AVG SCORE: -6.0978, GOLD PPL: 444.8559\n","BLEU = 3.32, 25.7/7.3/2.2/0.8 (BP=0.778, ratio=0.799, hyp_len=1844, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:25:18,344 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:25:34,052 INFO] PRED AVG SCORE: -0.6421, PRED PPL: 1.9004\n","[2021-04-20 04:25:34,052 INFO] GOLD AVG SCORE: -5.4830, GOLD PPL: 240.5741\n","BLEU = 7.37, 35.0/16.3/7.9/3.5 (BP=0.658, ratio=0.705, hyp_len=1331, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2ffz_YpTl3i","executionInfo":{"status":"ok","timestamp":1618892909753,"user_tz":-420,"elapsed":174709,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"323c5c10-4a27-4f28-dde3-6f13f4592c3f"},"source":["!mkdir -p sif_catechism/output_catechism_152\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_152' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_152' \\\\\n","-save_data 'sif_catechism/output_catechism_152/en-vi' \n","\n","!mkdir -p sif_catechism/model_catechism_152\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_catechism/output_catechism_152/en-vi' \\\\\n","-save_model 'sif_catechism/model_catechism_152/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_catechism/model_catechism_152/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_152/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_152/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_152/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_152/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_152/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_152/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_152/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_152/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_catechism/model_catechism_152/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/model_catechism_152/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/model_catechism_152/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:25:37,276 INFO] Extracting features...\n","[2021-04-20 04:25:38,215 INFO]  * number of source features: 0.\n","[2021-04-20 04:25:38,216 INFO]  * number of target features: 0.\n","[2021-04-20 04:25:38,216 INFO] Building `Fields` object...\n","[2021-04-20 04:25:38,216 INFO] Building & saving training data...\n","[2021-04-20 04:25:38,229 INFO] Building shard 0.\n","[2021-04-20 04:25:38,235 INFO]  * saving 0th train data shard to sif_catechism/output_catechism_152/en-vi.train.0.pt.\n","[2021-04-20 04:25:38,329 INFO]  * tgt vocab size: 1094.\n","[2021-04-20 04:25:38,330 INFO]  * src vocab size: 1075.\n","[2021-04-20 04:25:41,806 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:25:43,250 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:25:43,256 INFO]  * src vocab size = 39660\n","[2021-04-20 04:25:43,256 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:25:43,256 INFO] Building model...\n","[2021-04-20 04:25:47,718 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:25:47,798 INFO] encoder: 39221248\n","[2021-04-20 04:25:47,798 INFO] decoder: 43931466\n","[2021-04-20 04:25:47,798 INFO] * number of parameters: 83152714\n","[2021-04-20 04:25:48,390 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:25:48,391 INFO] Start training loop without validation...\n","[2021-04-20 04:25:48,391 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:48,401 INFO] number of examples: 134\n","[2021-04-20 04:25:48,686 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:48,692 INFO] number of examples: 134\n","[2021-04-20 04:25:48,949 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:48,957 INFO] number of examples: 134\n","[2021-04-20 04:25:49,215 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:49,221 INFO] number of examples: 134\n","[2021-04-20 04:25:49,476 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:49,482 INFO] number of examples: 134\n","[2021-04-20 04:25:49,737 INFO] Step 30005/30020; acc:  20.11; ppl: 312.43; xent: 5.74; lr: 0.00051; 10195/13277 tok/s;      1 sec\n","[2021-04-20 04:25:49,941 INFO] Saving checkpoint sif_catechism/model_catechism_152/en-vi_step_30005.pt\n","[2021-04-20 04:25:54,712 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:54,719 INFO] number of examples: 134\n","[2021-04-20 04:25:54,995 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:55,002 INFO] number of examples: 134\n","[2021-04-20 04:25:55,265 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:55,272 INFO] number of examples: 134\n","[2021-04-20 04:25:55,528 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:55,535 INFO] number of examples: 134\n","[2021-04-20 04:25:55,794 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:25:55,800 INFO] number of examples: 134\n","[2021-04-20 04:25:56,057 INFO] Step 30010/30020; acc:  42.43; ppl: 26.15; xent: 3.26; lr: 0.00051; 2172/2828 tok/s;      8 sec\n","[2021-04-20 04:25:56,254 INFO] Saving checkpoint sif_catechism/model_catechism_152/en-vi_step_30010.pt\n","[2021-04-20 04:26:00,991 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:00,999 INFO] number of examples: 134\n","[2021-04-20 04:26:01,260 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:01,269 INFO] number of examples: 134\n","[2021-04-20 04:26:01,555 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:01,872 INFO] number of examples: 134\n","[2021-04-20 04:26:02,132 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:02,139 INFO] number of examples: 134\n","[2021-04-20 04:26:02,406 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:02,413 INFO] number of examples: 134\n","[2021-04-20 04:26:02,683 INFO] Step 30015/30020; acc:  68.49; ppl:  4.74; xent: 1.56; lr: 0.00051; 2071/2698 tok/s;     14 sec\n","[2021-04-20 04:26:02,895 INFO] Saving checkpoint sif_catechism/model_catechism_152/en-vi_step_30015.pt\n","[2021-04-20 04:26:13,332 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:13,341 INFO] number of examples: 134\n","[2021-04-20 04:26:13,632 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:13,641 INFO] number of examples: 134\n","[2021-04-20 04:26:13,934 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:13,942 INFO] number of examples: 134\n","[2021-04-20 04:26:14,234 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:14,242 INFO] number of examples: 134\n","[2021-04-20 04:26:14,531 INFO] Loading dataset from sif_catechism/output_catechism_152/en-vi.train.0.pt\n","[2021-04-20 04:26:14,539 INFO] number of examples: 134\n","[2021-04-20 04:26:14,832 INFO] Step 30020/30020; acc:  87.83; ppl:  1.75; xent: 0.56; lr: 0.00051; 1130/1471 tok/s;     26 sec\n","[2021-04-20 04:26:15,111 INFO] Saving checkpoint sif_catechism/model_catechism_152/en-vi_step_30020.pt\n","[2021-04-20 04:26:44,511 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:27:02,116 INFO] PRED AVG SCORE: -1.2979, PRED PPL: 3.6616\n","[2021-04-20 04:27:02,116 INFO] GOLD AVG SCORE: -6.0432, GOLD PPL: 421.2481\n","BLEU = 2.05, 29.7/7.8/2.8/1.2 (BP=0.391, ratio=0.516, hyp_len=1190, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:27:10,640 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:27:32,615 INFO] PRED AVG SCORE: -0.9504, PRED PPL: 2.5867\n","[2021-04-20 04:27:32,616 INFO] GOLD AVG SCORE: -5.6905, GOLD PPL: 296.0513\n","BLEU = 4.12, 29.5/7.9/2.6/0.9 (BP=0.853, ratio=0.863, hyp_len=1992, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:27:41,906 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:28:03,782 INFO] PRED AVG SCORE: -0.9766, PRED PPL: 2.6555\n","[2021-04-20 04:28:03,782 INFO] GOLD AVG SCORE: -6.0983, GOLD PPL: 445.0981\n","BLEU = 3.09, 23.7/6.5/2.0/0.6 (BP=0.851, ratio=0.861, hyp_len=1987, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:28:07,716 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:28:28,931 INFO] PRED AVG SCORE: -0.7355, PRED PPL: 2.0865\n","[2021-04-20 04:28:28,931 INFO] GOLD AVG SCORE: -6.6799, GOLD PPL: 796.2168\n","BLEU = 3.17, 20.4/6.3/2.0/0.8 (BP=0.830, ratio=0.843, hyp_len=1945, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892944607,"user_tz":-420,"elapsed":34843,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"667bf73a-1558-41bc-80fc-9dd313d0c291"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output sif_catechism/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < sif_catechism/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:28:39,604 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:29:03,768 INFO] PRED AVG SCORE: -1.0610, PRED PPL: 2.8893\n","[2021-04-20 04:29:03,768 INFO] GOLD AVG SCORE: -8.7200, GOLD PPL: 6124.4530\n","BLEU = 3.31, 22.2/5.5/1.8/0.6 (BP=0.973, ratio=0.973, hyp_len=2246, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}