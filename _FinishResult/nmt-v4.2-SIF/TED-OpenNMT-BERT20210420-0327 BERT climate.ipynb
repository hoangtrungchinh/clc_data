{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-BERT20210420-0327 BERT climate.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889693859,"user_tz":-420,"elapsed":22671,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a4475ba6-5205-419c-d2a9-ab1530fc811b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889712758,"user_tz":-420,"elapsed":1581,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bb4bb83b-ab0d-4046-8330-30beffbb524d"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-BERT\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889714977,"user_tz":-420,"elapsed":1072,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"1fc17175-14e2-4598-d9f0-75815a673834"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:35:14 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889731761,"user_tz":-420,"elapsed":15418,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"597123d5-994a-4677-c123-05a2c7c34206"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 26.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 27.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 16.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 16.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 17.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 17.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 17.0MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 22.1MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=ad09a5401218807c1214621e5f0957ed47374be5b1d291762432959b58455da4\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAscbACoOygX","executionInfo":{"status":"ok","timestamp":1618887835865,"user_tz":-420,"elapsed":1201,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2c865aae-0635-4032-c4e5-9b4226bfb7c4"},"source":["!rm -rf SIF-finetune.tar.gz\n","!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889442257,"user_tz":-420,"elapsed":1687,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"12a7c093-4a7c-4137-8c78-b7085a63b06e"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","!tar -xvf 'SIF-finetune-bert.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 03:30:40--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 180907 (177K) [application/octet-stream]\n","Saving to: ‘SIF-finetune-bert.tar.gz’\n","\n","\rSIF-finetune-bert.t   0%[                    ]       0  --.-KB/s               \rSIF-finetune-bert.t 100%[===================>] 176.67K  --.-KB/s    in 0.03s   \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 03:30:41 (6.05 MB/s) - ‘SIF-finetune-bert.tar.gz’ saved [180907/180907]\n","\n","finetune/\n","finetune/en_finetune_climate_150\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_150\n","finetune/en_finetune_climate_100\n","finetune/vi_finetune_buddhism_162\n","finetune/en_finetune_law_50\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/test_climate.vi\n","finetune/en_finetune_climate_191\n","finetune/test_catechism.vi\n","finetune/vi_finetune_law_50\n","finetune/en_finetune_law_100\n","finetune/test_law.en\n","finetune/en_finetune_buddhism_162\n","finetune/vi_finetune_law_129\n","finetune/vi_finetune_catechism_50\n","finetune/en_finetune_climate_50\n","finetune/vi_finetune_catechism_100\n","finetune/en_finetune_buddhism_100\n","finetune/test_climate.en\n","finetune/en_finetune_catechism_112\n","finetune/vi_finetune_law_100\n","finetune/test_law.vi\n","finetune/en_finetune_buddhism_150\n","finetune/vi_finetune_catechism_112\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_climate_191\n","finetune/en_finetune_buddhism_50\n","finetune/test_catechism.en\n","finetune/test_buddhism.vi\n","finetune/test_buddhism.en\n","finetune/vi_finetune_climate_100\n","finetune/en_finetune_law_129\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618889447511,"user_tz":-420,"elapsed":1467,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2ab041d5-da8a-4d87-c2e4-0d8fa665e0fe"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 535\n","-rw------- 1 root root  9453 Apr 20 02:00 en_finetune_buddhism_100\n","-rw------- 1 root root 14772 Apr 20 02:00 en_finetune_buddhism_150\n","-rw------- 1 root root 16471 Apr 20 02:00 en_finetune_buddhism_162\n","-rw------- 1 root root  4892 Apr 20 02:00 en_finetune_buddhism_50\n","-rw------- 1 root root 13295 Apr 20 02:00 en_finetune_catechism_100\n","-rw------- 1 root root 14691 Apr 20 02:00 en_finetune_catechism_112\n","-rw------- 1 root root  6722 Apr 20 02:00 en_finetune_catechism_50\n","-rw------- 1 root root 13996 Apr 20 02:00 en_finetune_climate_100\n","-rw------- 1 root root 21088 Apr 20 02:00 en_finetune_climate_150\n","-rw------- 1 root root 27050 Apr 20 02:00 en_finetune_climate_191\n","-rw------- 1 root root  7039 Apr 20 02:00 en_finetune_climate_50\n","-rw------- 1 root root 13779 Apr 20 02:00 en_finetune_law_100\n","-rw------- 1 root root 18325 Apr 20 02:00 en_finetune_law_129\n","-rw------- 1 root root  6255 Apr 20 02:00 en_finetune_law_50\n","-rw------- 1 root root  7711 Apr 20 02:00 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 02:00 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 02:00 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 02:00 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 02:00 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 02:00 test_climate.vi\n","-rw------- 1 root root 11267 Apr 20 02:00 test_law.en\n","-rw------- 1 root root 15336 Apr 20 02:00 test_law.vi\n","-rw------- 1 root root 13704 Apr 20 02:00 vi_finetune_buddhism_100\n","-rw------- 1 root root 21690 Apr 20 02:00 vi_finetune_buddhism_150\n","-rw------- 1 root root 24138 Apr 20 02:00 vi_finetune_buddhism_162\n","-rw------- 1 root root  7050 Apr 20 02:00 vi_finetune_buddhism_50\n","-rw------- 1 root root 18233 Apr 20 02:00 vi_finetune_catechism_100\n","-rw------- 1 root root 20199 Apr 20 02:00 vi_finetune_catechism_112\n","-rw------- 1 root root  8920 Apr 20 02:00 vi_finetune_catechism_50\n","-rw------- 1 root root 18493 Apr 20 02:00 vi_finetune_climate_100\n","-rw------- 1 root root 27537 Apr 20 02:00 vi_finetune_climate_150\n","-rw------- 1 root root 35637 Apr 20 02:00 vi_finetune_climate_191\n","-rw------- 1 root root  9330 Apr 20 02:00 vi_finetune_climate_50\n","-rw------- 1 root root 17161 Apr 20 02:00 vi_finetune_law_100\n","-rw------- 1 root root 22753 Apr 20 02:00 vi_finetune_law_129\n","-rw------- 1 root root  7974 Apr 20 02:00 vi_finetune_law_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618889473587,"user_tz":-420,"elapsed":19250,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2c0ec67-ded8-4c4b-e9c3-760c282cf56c"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 23.94 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889980385,"user_tz":-420,"elapsed":202216,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"fe7e2d28-909f-4cbf-fb61-d7b6dd2d9056"},"source":["!mkdir -p bert_climate/output_climate_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_50' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_50' \\\\\n","-save_data 'bert_climate/output_climate_50/en-vi' \n","\n","!mkdir -p bert_climate/model_climate_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_climate/output_climate_50/en-vi' \\\\\n","-save_model 'bert_climate/model_climate_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_climate/model_climate_50/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_climate/model_climate_50/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_climate/model_climate_50/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_climate/model_climate_50/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":5,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:36:22,011 INFO] Extracting features...\n","[2021-04-20 03:36:23,311 INFO]  * number of source features: 0.\n","[2021-04-20 03:36:23,311 INFO]  * number of target features: 0.\n","[2021-04-20 03:36:23,311 INFO] Building `Fields` object...\n","[2021-04-20 03:36:23,312 INFO] Building & saving training data...\n","[2021-04-20 03:36:23,322 INFO] Building shard 0.\n","[2021-04-20 03:36:23,325 INFO]  * saving 0th train data shard to bert_climate/output_climate_50/en-vi.train.0.pt.\n","[2021-04-20 03:36:23,422 INFO]  * tgt vocab size: 577.\n","[2021-04-20 03:36:23,423 INFO]  * src vocab size: 537.\n","[2021-04-20 03:36:43,735 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:36:45,027 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:36:45,031 INFO]  * src vocab size = 39660\n","[2021-04-20 03:36:45,031 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:36:45,031 INFO] Building model...\n","[2021-04-20 03:36:52,558 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:36:52,633 INFO] encoder: 39221248\n","[2021-04-20 03:36:52,633 INFO] decoder: 43931466\n","[2021-04-20 03:36:52,633 INFO] * number of parameters: 83152714\n","[2021-04-20 03:36:53,188 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:36:53,188 INFO] Start training loop without validation...\n","[2021-04-20 03:36:53,188 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,191 INFO] number of examples: 46\n","[2021-04-20 03:36:53,199 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,202 INFO] number of examples: 46\n","[2021-04-20 03:36:53,513 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,516 INFO] number of examples: 46\n","[2021-04-20 03:36:53,519 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,522 INFO] number of examples: 46\n","[2021-04-20 03:36:53,749 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,752 INFO] number of examples: 46\n","[2021-04-20 03:36:53,755 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,757 INFO] number of examples: 46\n","[2021-04-20 03:36:53,980 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,983 INFO] number of examples: 46\n","[2021-04-20 03:36:53,986 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:53,990 INFO] number of examples: 46\n","[2021-04-20 03:36:54,210 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:54,214 INFO] number of examples: 46\n","[2021-04-20 03:36:54,216 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:54,219 INFO] number of examples: 46\n","[2021-04-20 03:36:54,437 INFO] Step 30005/30020; acc:  49.25; ppl: 30.03; xent: 3.40; lr: 0.00051; 7459/10629 tok/s;      1 sec\n","[2021-04-20 03:36:54,619 INFO] Saving checkpoint bert_climate/model_climate_50/en-vi_step_30005.pt\n","[2021-04-20 03:36:58,758 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:58,761 INFO] number of examples: 46\n","[2021-04-20 03:36:58,764 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:58,767 INFO] number of examples: 46\n","[2021-04-20 03:36:59,006 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,009 INFO] number of examples: 46\n","[2021-04-20 03:36:59,012 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,015 INFO] number of examples: 46\n","[2021-04-20 03:36:59,239 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,242 INFO] number of examples: 46\n","[2021-04-20 03:36:59,245 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,248 INFO] number of examples: 46\n","[2021-04-20 03:36:59,467 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,470 INFO] number of examples: 46\n","[2021-04-20 03:36:59,473 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,476 INFO] number of examples: 46\n","[2021-04-20 03:36:59,701 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,704 INFO] number of examples: 46\n","[2021-04-20 03:36:59,708 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:36:59,711 INFO] number of examples: 46\n","[2021-04-20 03:36:59,931 INFO] Step 30010/30020; acc:  84.57; ppl:  2.90; xent: 1.06; lr: 0.00051; 1696/2417 tok/s;      7 sec\n","[2021-04-20 03:37:00,123 INFO] Saving checkpoint bert_climate/model_climate_50/en-vi_step_30010.pt\n","[2021-04-20 03:37:04,227 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,229 INFO] number of examples: 46\n","[2021-04-20 03:37:04,232 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,235 INFO] number of examples: 46\n","[2021-04-20 03:37:04,454 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,457 INFO] number of examples: 46\n","[2021-04-20 03:37:04,460 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,463 INFO] number of examples: 46\n","[2021-04-20 03:37:04,683 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,686 INFO] number of examples: 46\n","[2021-04-20 03:37:04,689 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,692 INFO] number of examples: 46\n","[2021-04-20 03:37:04,915 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,918 INFO] number of examples: 46\n","[2021-04-20 03:37:04,921 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:04,923 INFO] number of examples: 46\n","[2021-04-20 03:37:05,144 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:05,147 INFO] number of examples: 46\n","[2021-04-20 03:37:05,150 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:05,153 INFO] number of examples: 46\n","[2021-04-20 03:37:05,373 INFO] Step 30015/30020; acc:  93.47; ppl:  1.54; xent: 0.43; lr: 0.00051; 1713/2440 tok/s;     12 sec\n","[2021-04-20 03:37:05,566 INFO] Saving checkpoint bert_climate/model_climate_50/en-vi_step_30015.pt\n","[2021-04-20 03:37:14,720 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:14,724 INFO] number of examples: 46\n","[2021-04-20 03:37:14,727 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:14,730 INFO] number of examples: 46\n","[2021-04-20 03:37:14,977 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:14,980 INFO] number of examples: 46\n","[2021-04-20 03:37:14,984 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:14,987 INFO] number of examples: 46\n","[2021-04-20 03:37:15,232 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:15,236 INFO] number of examples: 46\n","[2021-04-20 03:37:15,241 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:15,245 INFO] number of examples: 46\n","[2021-04-20 03:37:15,499 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:15,502 INFO] number of examples: 46\n","[2021-04-20 03:37:15,506 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:15,509 INFO] number of examples: 46\n","[2021-04-20 03:37:15,756 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:15,760 INFO] number of examples: 46\n","[2021-04-20 03:37:15,764 INFO] Loading dataset from bert_climate/output_climate_50/en-vi.train.0.pt\n","[2021-04-20 03:37:15,767 INFO] number of examples: 46\n","[2021-04-20 03:37:16,016 INFO] Step 30020/30020; acc:  98.62; ppl:  1.20; xent: 0.18; lr: 0.00051; 876/1248 tok/s;     23 sec\n","[2021-04-20 03:37:16,295 INFO] Saving checkpoint bert_climate/model_climate_50/en-vi_step_30020.pt\n","[2021-04-20 03:37:38,299 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:38:18,550 INFO] PRED AVG SCORE: -0.7625, PRED PPL: 2.1436\n","[2021-04-20 03:38:18,551 INFO] GOLD AVG SCORE: -4.8630, GOLD PPL: 129.4142\n","BLEU = 12.87, 38.1/17.8/8.6/4.7 (BP=1.000, ratio=1.079, hyp_len=2813, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:38:23,944 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:38:50,298 INFO] PRED AVG SCORE: -1.1160, PRED PPL: 3.0527\n","[2021-04-20 03:38:50,298 INFO] GOLD AVG SCORE: -5.6858, GOLD PPL: 294.6447\n","BLEU = 9.03, 28.9/13.5/6.9/4.0 (BP=0.884, ratio=0.891, hyp_len=2321, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:38:54,036 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:39:14,633 INFO] PRED AVG SCORE: -0.5998, PRED PPL: 1.8217\n","[2021-04-20 03:39:14,633 INFO] GOLD AVG SCORE: -6.2405, GOLD PPL: 513.1310\n","BLEU = 2.68, 22.8/9.7/4.4/1.9 (BP=0.407, ratio=0.526, hyp_len=1372, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:39:18,348 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:39:39,438 INFO] PRED AVG SCORE: -0.9191, PRED PPL: 2.5070\n","[2021-04-20 03:39:39,438 INFO] GOLD AVG SCORE: -5.3750, GOLD PPL: 215.9398\n","BLEU = 5.09, 28.9/13.0/6.8/4.1 (BP=0.503, ratio=0.593, hyp_len=1545, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8AX0zOuPtfA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890170686,"user_tz":-420,"elapsed":190286,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"fcee2e3b-a9fa-4ca7-fd07-bf81372cc3a1"},"source":["!mkdir -p bert_climate/output_climate_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_100' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_100' \\\\\n","-save_data 'bert_climate/output_climate_100/en-vi' \n","\n","!mkdir -p bert_climate/model_climate_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_climate/output_climate_100/en-vi' \\\\\n","-save_model 'bert_climate/model_climate_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_climate/model_climate_100/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_climate/model_climate_100/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_climate/model_climate_100/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_climate/model_climate_100/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:39:42,399 INFO] Extracting features...\n","[2021-04-20 03:39:43,862 INFO]  * number of source features: 0.\n","[2021-04-20 03:39:43,862 INFO]  * number of target features: 0.\n","[2021-04-20 03:39:43,863 INFO] Building `Fields` object...\n","[2021-04-20 03:39:43,863 INFO] Building & saving training data...\n","[2021-04-20 03:39:43,875 INFO] Building shard 0.\n","[2021-04-20 03:39:43,879 INFO]  * saving 0th train data shard to bert_climate/output_climate_100/en-vi.train.0.pt.\n","[2021-04-20 03:39:43,976 INFO]  * tgt vocab size: 891.\n","[2021-04-20 03:39:43,977 INFO]  * src vocab size: 941.\n","[2021-04-20 03:39:46,695 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:39:48,026 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:39:48,031 INFO]  * src vocab size = 39660\n","[2021-04-20 03:39:48,031 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:39:48,031 INFO] Building model...\n","[2021-04-20 03:39:52,252 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:39:52,348 INFO] encoder: 39221248\n","[2021-04-20 03:39:52,348 INFO] decoder: 43931466\n","[2021-04-20 03:39:52,348 INFO] * number of parameters: 83152714\n","[2021-04-20 03:39:52,894 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:39:52,895 INFO] Start training loop without validation...\n","[2021-04-20 03:39:52,895 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:52,899 INFO] number of examples: 92\n","[2021-04-20 03:39:53,174 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:53,179 INFO] number of examples: 92\n","[2021-04-20 03:39:53,426 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:53,431 INFO] number of examples: 92\n","[2021-04-20 03:39:53,671 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:53,676 INFO] number of examples: 92\n","[2021-04-20 03:39:53,915 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:53,920 INFO] number of examples: 92\n","[2021-04-20 03:39:54,158 INFO] Step 30005/30020; acc:  40.71; ppl: 52.09; xent: 3.95; lr: 0.00051; 7356/10571 tok/s;      1 sec\n","[2021-04-20 03:39:54,348 INFO] Saving checkpoint bert_climate/model_climate_100/en-vi_step_30005.pt\n","[2021-04-20 03:39:58,645 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:58,649 INFO] number of examples: 92\n","[2021-04-20 03:39:58,912 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:58,917 INFO] number of examples: 92\n","[2021-04-20 03:39:59,165 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:59,170 INFO] number of examples: 92\n","[2021-04-20 03:39:59,410 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:59,414 INFO] number of examples: 92\n","[2021-04-20 03:39:59,650 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:39:59,656 INFO] number of examples: 92\n","[2021-04-20 03:39:59,898 INFO] Step 30010/30020; acc:  74.49; ppl:  4.71; xent: 1.55; lr: 0.00051; 1618/2326 tok/s;      7 sec\n","[2021-04-20 03:40:00,094 INFO] Saving checkpoint bert_climate/model_climate_100/en-vi_step_30010.pt\n","[2021-04-20 03:40:07,650 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:07,657 INFO] number of examples: 92\n","[2021-04-20 03:40:07,892 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:07,896 INFO] number of examples: 92\n","[2021-04-20 03:40:08,173 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:08,179 INFO] number of examples: 92\n","[2021-04-20 03:40:08,425 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:08,430 INFO] number of examples: 92\n","[2021-04-20 03:40:08,674 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:08,679 INFO] number of examples: 92\n","[2021-04-20 03:40:08,922 INFO] Step 30015/30020; acc:  89.47; ppl:  1.91; xent: 0.65; lr: 0.00051; 1030/1480 tok/s;     16 sec\n","[2021-04-20 03:40:09,117 INFO] Saving checkpoint bert_climate/model_climate_100/en-vi_step_30015.pt\n","[2021-04-20 03:40:14,496 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:14,521 INFO] number of examples: 92\n","[2021-04-20 03:40:14,766 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:14,771 INFO] number of examples: 92\n","[2021-04-20 03:40:15,029 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:15,036 INFO] number of examples: 92\n","[2021-04-20 03:40:15,307 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:15,313 INFO] number of examples: 92\n","[2021-04-20 03:40:15,578 INFO] Loading dataset from bert_climate/output_climate_100/en-vi.train.0.pt\n","[2021-04-20 03:40:15,583 INFO] number of examples: 92\n","[2021-04-20 03:40:15,841 INFO] Step 30020/30020; acc:  97.11; ppl:  1.28; xent: 0.25; lr: 0.00051; 1343/1929 tok/s;     23 sec\n","[2021-04-20 03:40:16,092 INFO] Saving checkpoint bert_climate/model_climate_100/en-vi_step_30020.pt\n","[2021-04-20 03:40:36,169 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:41:23,352 INFO] PRED AVG SCORE: -0.8002, PRED PPL: 2.2260\n","[2021-04-20 03:41:23,353 INFO] GOLD AVG SCORE: -4.5041, GOLD PPL: 90.3880\n","BLEU = 13.42, 38.3/18.4/9.2/5.0 (BP=1.000, ratio=1.090, hyp_len=2841, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:41:27,198 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:42:00,949 INFO] PRED AVG SCORE: -0.8516, PRED PPL: 2.3433\n","[2021-04-20 03:42:00,949 INFO] GOLD AVG SCORE: -4.6396, GOLD PPL: 103.5035\n","BLEU = 13.91, 38.6/19.0/9.5/5.4 (BP=0.996, ratio=0.996, hyp_len=2596, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:42:04,790 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:42:24,646 INFO] PRED AVG SCORE: -0.8538, PRED PPL: 2.3486\n","[2021-04-20 03:42:24,646 INFO] GOLD AVG SCORE: -5.1069, GOLD PPL: 165.1656\n","BLEU = 5.02, 31.2/14.5/7.1/3.3 (BP=0.494, ratio=0.586, hyp_len=1528, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:42:28,374 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:42:49,854 INFO] PRED AVG SCORE: -0.7834, PRED PPL: 2.1889\n","[2021-04-20 03:42:49,854 INFO] GOLD AVG SCORE: -4.8318, GOLD PPL: 125.4407\n","BLEU = 6.23, 34.6/15.5/7.2/3.2 (BP=0.592, ratio=0.656, hyp_len=1710, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmEAHw2aVdLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890367886,"user_tz":-420,"elapsed":197186,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"482c8eda-ba94-4b7f-fb8f-83295ba4c96a"},"source":["!mkdir -p bert_climate/output_climate_150\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_150' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_150' \\\\\n","-save_data 'bert_climate/output_climate_150/en-vi' \n","\n","!mkdir -p bert_climate/model_climate_150\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_climate/output_climate_150/en-vi' \\\\\n","-save_model 'bert_climate/model_climate_150/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_climate/model_climate_150/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_150/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_150/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_climate/model_climate_150/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_150/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_150/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_climate/model_climate_150/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_150/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_150/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_climate/model_climate_150/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_150/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_150/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:42:52,772 INFO] Extracting features...\n","[2021-04-20 03:42:53,954 INFO]  * number of source features: 0.\n","[2021-04-20 03:42:53,954 INFO]  * number of target features: 0.\n","[2021-04-20 03:42:53,954 INFO] Building `Fields` object...\n","[2021-04-20 03:42:53,954 INFO] Building & saving training data...\n","[2021-04-20 03:42:53,968 INFO] Building shard 0.\n","[2021-04-20 03:42:53,974 INFO]  * saving 0th train data shard to bert_climate/output_climate_150/en-vi.train.0.pt.\n","[2021-04-20 03:42:54,170 INFO]  * tgt vocab size: 1116.\n","[2021-04-20 03:42:54,171 INFO]  * src vocab size: 1259.\n","[2021-04-20 03:42:56,954 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:42:58,263 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:42:58,269 INFO]  * src vocab size = 39660\n","[2021-04-20 03:42:58,269 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:42:58,269 INFO] Building model...\n","[2021-04-20 03:43:02,565 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:43:02,622 INFO] encoder: 39221248\n","[2021-04-20 03:43:02,622 INFO] decoder: 43931466\n","[2021-04-20 03:43:02,622 INFO] * number of parameters: 83152714\n","[2021-04-20 03:43:03,194 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:43:03,195 INFO] Start training loop without validation...\n","[2021-04-20 03:43:03,195 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:03,206 INFO] number of examples: 138\n","[2021-04-20 03:43:03,498 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:03,504 INFO] number of examples: 138\n","[2021-04-20 03:43:03,768 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:03,774 INFO] number of examples: 138\n","[2021-04-20 03:43:04,034 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:04,041 INFO] number of examples: 138\n","[2021-04-20 03:43:04,300 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:04,306 INFO] number of examples: 138\n","[2021-04-20 03:43:04,563 INFO] Step 30005/30020; acc:  38.79; ppl: 59.60; xent: 4.09; lr: 0.00051; 10224/14487 tok/s;      1 sec\n","[2021-04-20 03:43:04,751 INFO] Saving checkpoint bert_climate/model_climate_150/en-vi_step_30005.pt\n","[2021-04-20 03:43:09,007 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:09,469 INFO] number of examples: 138\n","[2021-04-20 03:43:09,762 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:09,774 INFO] number of examples: 138\n","[2021-04-20 03:43:10,034 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:10,040 INFO] number of examples: 138\n","[2021-04-20 03:43:10,297 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:10,304 INFO] number of examples: 138\n","[2021-04-20 03:43:10,562 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:10,569 INFO] number of examples: 138\n","[2021-04-20 03:43:10,824 INFO] Step 30010/30020; acc:  68.48; ppl:  6.51; xent: 1.87; lr: 0.00051; 2236/3168 tok/s;      8 sec\n","[2021-04-20 03:43:11,017 INFO] Saving checkpoint bert_climate/model_climate_150/en-vi_step_30010.pt\n","[2021-04-20 03:43:15,013 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:15,020 INFO] number of examples: 138\n","[2021-04-20 03:43:15,281 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:15,288 INFO] number of examples: 138\n","[2021-04-20 03:43:15,546 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:15,553 INFO] number of examples: 138\n","[2021-04-20 03:43:15,822 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:15,828 INFO] number of examples: 138\n","[2021-04-20 03:43:16,093 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:16,101 INFO] number of examples: 138\n","[2021-04-20 03:43:16,378 INFO] Step 30015/30020; acc:  85.42; ppl:  2.30; xent: 0.83; lr: 0.00051; 2520/3570 tok/s;     13 sec\n","[2021-04-20 03:43:16,600 INFO] Saving checkpoint bert_climate/model_climate_150/en-vi_step_30015.pt\n","[2021-04-20 03:43:27,541 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:27,548 INFO] number of examples: 138\n","[2021-04-20 03:43:27,820 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:27,828 INFO] number of examples: 138\n","[2021-04-20 03:43:28,100 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:28,107 INFO] number of examples: 138\n","[2021-04-20 03:43:28,384 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:28,392 INFO] number of examples: 138\n","[2021-04-20 03:43:28,673 INFO] Loading dataset from bert_climate/output_climate_150/en-vi.train.0.pt\n","[2021-04-20 03:43:28,681 INFO] number of examples: 138\n","[2021-04-20 03:43:28,965 INFO] Step 30020/30020; acc:  95.52; ppl:  1.36; xent: 0.31; lr: 0.00051; 1112/1575 tok/s;     26 sec\n","[2021-04-20 03:43:29,188 INFO] Saving checkpoint bert_climate/model_climate_150/en-vi_step_30020.pt\n","[2021-04-20 03:43:58,632 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:44:39,109 INFO] PRED AVG SCORE: -0.8077, PRED PPL: 2.2427\n","[2021-04-20 03:44:39,109 INFO] GOLD AVG SCORE: -4.4063, GOLD PPL: 81.9690\n","BLEU = 14.38, 40.4/19.7/9.9/5.4 (BP=1.000, ratio=1.041, hyp_len=2712, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:44:42,927 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:45:16,848 INFO] PRED AVG SCORE: -0.8024, PRED PPL: 2.2308\n","[2021-04-20 03:45:16,848 INFO] GOLD AVG SCORE: -4.1835, GOLD PPL: 65.5958\n","BLEU = 15.00, 40.6/20.4/10.5/5.8 (BP=1.000, ratio=1.015, hyp_len=2646, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:45:20,710 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:45:42,233 INFO] PRED AVG SCORE: -0.8384, PRED PPL: 2.3127\n","[2021-04-20 03:45:42,233 INFO] GOLD AVG SCORE: -4.5191, GOLD PPL: 91.7534\n","BLEU = 9.10, 39.5/19.8/10.8/6.0 (BP=0.607, ratio=0.667, hyp_len=1739, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:45:46,040 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:46:07,002 INFO] PRED AVG SCORE: -0.6872, PRED PPL: 1.9882\n","[2021-04-20 03:46:07,002 INFO] GOLD AVG SCORE: -4.5690, GOLD PPL: 96.4493\n","BLEU = 8.65, 38.7/19.5/9.3/4.5 (BP=0.649, ratio=0.698, hyp_len=1819, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSOWqRkDWk_T","executionInfo":{"status":"ok","timestamp":1618890576981,"user_tz":-420,"elapsed":209088,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"de7ea05a-0eb2-45b7-e23c-2b07cdfd5978"},"source":["!mkdir -p bert_climate/output_climate_191\n","!onmt_preprocess -train_src 'finetune/en_finetune_climate_191' \\\\\n","-train_tgt 'finetune/vi_finetune_climate_191' \\\\\n","-save_data 'bert_climate/output_climate_191/en-vi' \n","\n","!mkdir -p bert_climate/model_climate_191\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_climate/output_climate_191/en-vi' \\\\\n","-save_model 'bert_climate/model_climate_191/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_climate/model_climate_191/en-vi_step_30005.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_191/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_191/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_climate/model_climate_191/en-vi_step_30010.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_191/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_191/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_climate/model_climate_191/en-vi_step_30015.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_191/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_191/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_climate/model_climate_191/en-vi_step_30020.pt -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/model_climate_191/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/model_climate_191/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:46:09,974 INFO] Extracting features...\n","[2021-04-20 03:46:11,410 INFO]  * number of source features: 0.\n","[2021-04-20 03:46:11,411 INFO]  * number of target features: 0.\n","[2021-04-20 03:46:11,411 INFO] Building `Fields` object...\n","[2021-04-20 03:46:11,411 INFO] Building & saving training data...\n","[2021-04-20 03:46:11,424 INFO] Building shard 0.\n","[2021-04-20 03:46:11,431 INFO]  * saving 0th train data shard to bert_climate/output_climate_191/en-vi.train.0.pt.\n","[2021-04-20 03:46:11,625 INFO]  * tgt vocab size: 1264.\n","[2021-04-20 03:46:11,627 INFO]  * src vocab size: 1501.\n","[2021-04-20 03:46:15,078 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:46:16,424 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:46:16,429 INFO]  * src vocab size = 39660\n","[2021-04-20 03:46:16,430 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:46:16,430 INFO] Building model...\n","[2021-04-20 03:46:20,795 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:46:20,837 INFO] encoder: 39221248\n","[2021-04-20 03:46:20,837 INFO] decoder: 43931466\n","[2021-04-20 03:46:20,838 INFO] * number of parameters: 83152714\n","[2021-04-20 03:46:21,422 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:46:21,422 INFO] Start training loop without validation...\n","[2021-04-20 03:46:21,422 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:21,434 INFO] number of examples: 174\n","[2021-04-20 03:46:21,786 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:21,794 INFO] number of examples: 174\n","[2021-04-20 03:46:22,114 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:22,123 INFO] number of examples: 174\n","[2021-04-20 03:46:22,441 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:22,449 INFO] number of examples: 174\n","[2021-04-20 03:46:22,767 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:22,774 INFO] number of examples: 174\n","[2021-04-20 03:46:23,094 INFO] Step 30005/30020; acc:  37.24; ppl: 65.78; xent: 4.19; lr: 0.00051; 10657/15150 tok/s;      2 sec\n","[2021-04-20 03:46:23,281 INFO] Saving checkpoint bert_climate/model_climate_191/en-vi_step_30005.pt\n","[2021-04-20 03:46:27,428 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:27,436 INFO] number of examples: 174\n","[2021-04-20 03:46:27,786 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:27,794 INFO] number of examples: 174\n","[2021-04-20 03:46:28,115 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:28,124 INFO] number of examples: 174\n","[2021-04-20 03:46:28,444 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:28,452 INFO] number of examples: 174\n","[2021-04-20 03:46:28,772 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:28,780 INFO] number of examples: 174\n","[2021-04-20 03:46:29,099 INFO] Step 30010/30020; acc:  64.71; ppl:  7.76; xent: 2.05; lr: 0.00051; 2967/4218 tok/s;      8 sec\n","[2021-04-20 03:46:29,289 INFO] Saving checkpoint bert_climate/model_climate_191/en-vi_step_30010.pt\n","[2021-04-20 03:46:36,511 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:36,519 INFO] number of examples: 174\n","[2021-04-20 03:46:36,850 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:36,860 INFO] number of examples: 174\n","[2021-04-20 03:46:37,191 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:37,201 INFO] number of examples: 174\n","[2021-04-20 03:46:37,525 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:37,533 INFO] number of examples: 174\n","[2021-04-20 03:46:37,863 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:37,872 INFO] number of examples: 174\n","[2021-04-20 03:46:38,202 INFO] Step 30015/30020; acc:  82.04; ppl:  2.63; xent: 0.97; lr: 0.00051; 1957/2782 tok/s;     17 sec\n","[2021-04-20 03:46:38,418 INFO] Saving checkpoint bert_climate/model_climate_191/en-vi_step_30015.pt\n","[2021-04-20 03:46:49,045 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:49,054 INFO] number of examples: 174\n","[2021-04-20 03:46:49,398 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:49,407 INFO] number of examples: 174\n","[2021-04-20 03:46:49,743 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:49,754 INFO] number of examples: 174\n","[2021-04-20 03:46:50,091 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:50,100 INFO] number of examples: 174\n","[2021-04-20 03:46:50,435 INFO] Loading dataset from bert_climate/output_climate_191/en-vi.train.0.pt\n","[2021-04-20 03:46:50,444 INFO] number of examples: 174\n","[2021-04-20 03:46:50,774 INFO] Step 30020/30020; acc:  93.87; ppl:  1.44; xent: 0.36; lr: 0.00051; 1417/2014 tok/s;     29 sec\n","[2021-04-20 03:46:51,008 INFO] Saving checkpoint bert_climate/model_climate_191/en-vi_step_30020.pt\n","[2021-04-20 03:47:23,727 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:48:02,516 INFO] PRED AVG SCORE: -0.8075, PRED PPL: 2.2423\n","[2021-04-20 03:48:02,516 INFO] GOLD AVG SCORE: -4.3637, GOLD PPL: 78.5494\n","BLEU = 14.68, 40.3/19.8/10.3/5.6 (BP=1.000, ratio=1.089, hyp_len=2839, ref_len=2606)\n"," ===05==^======= \n","[2021-04-20 03:48:08,431 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:48:42,140 INFO] PRED AVG SCORE: -0.7812, PRED PPL: 2.1841\n","[2021-04-20 03:48:42,140 INFO] GOLD AVG SCORE: -4.0352, GOLD PPL: 56.5527\n","BLEU = 15.83, 42.3/21.2/11.1/6.5 (BP=0.994, ratio=0.994, hyp_len=2591, ref_len=2606)\n"," ===10==^======= \n","[2021-04-20 03:48:47,078 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:49:09,575 INFO] PRED AVG SCORE: -0.8178, PRED PPL: 2.2654\n","[2021-04-20 03:49:09,575 INFO] GOLD AVG SCORE: -4.2660, GOLD PPL: 71.2393\n","BLEU = 11.41, 43.3/23.0/12.3/6.7 (BP=0.673, ratio=0.716, hyp_len=1867, ref_len=2606)\n"," ===15==^======= \n","[2021-04-20 03:49:13,328 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:49:36,196 INFO] PRED AVG SCORE: -0.6632, PRED PPL: 1.9410\n","[2021-04-20 03:49:36,196 INFO] GOLD AVG SCORE: -4.3462, GOLD PPL: 77.1831\n","BLEU = 11.84, 42.2/22.0/11.2/5.8 (BP=0.757, ratio=0.782, hyp_len=2038, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890617605,"user_tz":-420,"elapsed":249709,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"17c082d0-5be7-4e29-b268-bc4bc63db94b"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_climate.en -tgt finetune/test_climate.vi -output bert_climate/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_climate.vi < bert_climate/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:49:51,687 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [95], which does not match the required output shape [19, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:50:16,768 INFO] PRED AVG SCORE: -1.0597, PRED PPL: 2.8854\n","[2021-04-20 03:50:16,768 INFO] GOLD AVG SCORE: -6.4587, GOLD PPL: 638.2224\n","BLEU = 12.20, 36.0/18.2/9.9/6.1 (BP=0.866, ratio=0.875, hyp_len=2279, ref_len=2606)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}