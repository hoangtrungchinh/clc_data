{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-20210420-0301 SIF law.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902385945,"user_tz":-420,"elapsed":21261,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"612b3a59-d924-49b6-f524-9caf4b4d71dc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902402393,"user_tz":-420,"elapsed":3652,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"b185fa1d-ef4b-4090-f6ec-750d529a01b3"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618887722822,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ad093d9c-5838-4dac-d5b5-e7682b1da442"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:02:02 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902419345,"user_tz":-420,"elapsed":14395,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"fe063cbb-4528-41a6-b58e-dd68453266d4"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 8.1MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n","\u001b[?25hCollecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 29.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=ca9cb0802e226d53ff9102215cd40b4d6087d87c71d1464d65445dae4f9f3b19\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: waitress, configargparse, torchtext, pyonmttok, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAscbACoOygX","executionInfo":{"status":"ok","timestamp":1618902426149,"user_tz":-420,"elapsed":1200,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"c148c590-3b35-4ad8-e2bb-a0319eab4546"},"source":["!rm -rf SIF-finetune.tar.gz\n","!rm -rf finetune/\n","!ls -al"],"execution_count":4,"outputs":[{"output_type":"stream","text":["total 996620\n","-rw------- 1 root root 1020517439 Apr 20 03:08 en-vi_step_30000.pt\n","drwx------ 2 root root       4096 Apr 20 03:04 OpenNMT-py\n","drwx------ 2 root root       4096 Apr 20 03:23 sif_buddhism\n","drwx------ 2 root root       4096 Apr 20 04:17 sif_catechism\n","drwx------ 2 root root       4096 Apr 20 03:14 sif_climate\n","drwx------ 2 root root       4096 Apr 20 03:04 sif_law\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902435830,"user_tz":-420,"elapsed":1766,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bf2f885c-0a42-4324-c489-3d5c59286cb4"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","!tar -xvf 'SIF-finetune.tar.gz'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2021-04-20 07:07:14--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 100744 (98K) [application/octet-stream]\n","Saving to: ‘SIF-finetune.tar.gz’\n","\n","SIF-finetune.tar.gz 100%[===================>]  98.38K  --.-KB/s    in 0.01s   \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 07:07:14 (8.06 MB/s) - ‘SIF-finetune.tar.gz’ saved [100744/100744]\n","\n","finetune/\n","finetune/en_finetune_law_50\n","finetune/en_finetune_law_100\n","finetune/en_finetune_law_118\n","finetune/en_finetune_buddhism_50\n","finetune/en_finetune_buddhism_100\n","finetune/en_finetune_buddhism_150\n","finetune/en_finetune_buddhism_193\n","finetune/en_finetune_climate_50\n","finetune/en_finetune_climate_100\n","finetune/en_finetune_climate_120\n","finetune/en_finetune_catechism_50\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_150\n","finetune/en_finetune_catechism_152\n","finetune/vi_finetune_law_50\n","finetune/vi_finetune_law_100\n","finetune/vi_finetune_law_118\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_buddhism_193\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_100\n","finetune/vi_finetune_climate_120\n","finetune/vi_finetune_catechism_50\n","finetune/vi_finetune_catechism_100\n","finetune/vi_finetune_catechism_150\n","finetune/vi_finetune_catechism_152\n","finetune/test_law.vi\n","finetune/test_buddhism.vi\n","finetune/test_climate.vi\n","finetune/test_catechism.vi\n","finetune/test_law.en\n","finetune/test_buddhism.en\n","finetune/test_climate.en\n","finetune/test_catechism.en\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618902442293,"user_tz":-420,"elapsed":832,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a8f28a3b-0765-4806-c7b9-1ae57f68ebba"},"source":["!ls -al finetune"],"execution_count":6,"outputs":[{"output_type":"stream","text":["total 538\n","-rw------- 1 root root  9813 Apr 20 05:25 en_finetune_buddhism_100\n","-rw------- 1 root root 14812 Apr 20 05:25 en_finetune_buddhism_150\n","-rw------- 1 root root 19723 Apr 20 05:25 en_finetune_buddhism_193\n","-rw------- 1 root root  5246 Apr 20 05:25 en_finetune_buddhism_50\n","-rw------- 1 root root 13614 Apr 20 05:25 en_finetune_catechism_100\n","-rw------- 1 root root 20281 Apr 20 05:25 en_finetune_catechism_150\n","-rw------- 1 root root 20539 Apr 20 05:25 en_finetune_catechism_152\n","-rw------- 1 root root  7151 Apr 20 05:25 en_finetune_catechism_50\n","-rw------- 1 root root 13647 Apr 20 05:25 en_finetune_climate_100\n","-rw------- 1 root root 16493 Apr 20 05:25 en_finetune_climate_120\n","-rw------- 1 root root  6498 Apr 20 05:25 en_finetune_climate_50\n","-rw------- 1 root root 16113 Apr 20 05:25 en_finetune_law_100\n","-rw------- 1 root root 18060 Apr 20 05:25 en_finetune_law_118\n","-rw------- 1 root root  7784 Apr 20 05:25 en_finetune_law_50\n","-rw------- 1 root root  7711 Apr 20 05:25 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 05:25 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 05:25 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 05:25 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 05:25 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 05:25 test_climate.vi\n","-rw------- 1 root root 11306 Apr 20 05:25 test_law.en\n","-rw------- 1 root root 13801 Apr 20 05:25 test_law.vi\n","-rw------- 1 root root 13723 Apr 20 05:25 vi_finetune_buddhism_100\n","-rw------- 1 root root 21466 Apr 20 05:25 vi_finetune_buddhism_150\n","-rw------- 1 root root 28181 Apr 20 05:25 vi_finetune_buddhism_193\n","-rw------- 1 root root  7370 Apr 20 05:25 vi_finetune_buddhism_50\n","-rw------- 1 root root 18512 Apr 20 05:25 vi_finetune_catechism_100\n","-rw------- 1 root root 27677 Apr 20 05:25 vi_finetune_catechism_150\n","-rw------- 1 root root 28052 Apr 20 05:25 vi_finetune_catechism_152\n","-rw------- 1 root root  9740 Apr 20 05:25 vi_finetune_catechism_50\n","-rw------- 1 root root 18253 Apr 20 05:25 vi_finetune_climate_100\n","-rw------- 1 root root 22006 Apr 20 05:25 vi_finetune_climate_120\n","-rw------- 1 root root  8917 Apr 20 05:25 vi_finetune_climate_50\n","-rw------- 1 root root 19514 Apr 20 05:25 vi_finetune_law_100\n","-rw------- 1 root root 21908 Apr 20 05:25 vi_finetune_law_118\n","-rw------- 1 root root  9782 Apr 20 05:25 vi_finetune_law_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MhPNq0uGGulY","executionInfo":{"status":"ok","timestamp":1618902464570,"user_tz":-420,"elapsed":1819,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}}},"source":["!rm -rf sif_law"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618887872728,"user_tz":-420,"elapsed":19319,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"dea9bd89-6e12-4441-eb1e-6a1eaeb208c4"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 24.10 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902661682,"user_tz":-420,"elapsed":185039,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"64817885-b2b4-4e5e-fe3c-8b3044480371"},"source":["!mkdir -p sif_law/output_law_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_law_50' \\\\\n","-train_tgt 'finetune/vi_finetune_law_50' \\\\\n","-save_data 'sif_law/output_law_50/en-vi' \n","\n","!mkdir -p sif_law/model_law_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_law/output_law_50/en-vi' \\\\\n","-save_model 'sif_law/model_law_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_law/model_law_50/en-vi_step_30005.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_law/model_law_50/en-vi_step_30010.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_law/model_law_50/en-vi_step_30015.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_law/model_law_50/en-vi_step_30020.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:07:59,998 INFO] Extracting features...\n","[2021-04-20 07:08:00,000 INFO]  * number of source features: 0.\n","[2021-04-20 07:08:00,000 INFO]  * number of target features: 0.\n","[2021-04-20 07:08:00,000 INFO] Building `Fields` object...\n","[2021-04-20 07:08:00,000 INFO] Building & saving training data...\n","[2021-04-20 07:08:00,009 INFO] Building shard 0.\n","[2021-04-20 07:08:00,012 INFO]  * saving 0th train data shard to sif_law/output_law_50/en-vi.train.0.pt.\n","[2021-04-20 07:08:00,110 INFO]  * tgt vocab size: 473.\n","[2021-04-20 07:08:00,110 INFO]  * src vocab size: 405.\n","[2021-04-20 07:08:14,003 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 07:08:15,221 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 07:08:15,224 INFO]  * src vocab size = 39660\n","[2021-04-20 07:08:15,224 INFO]  * tgt vocab size = 18250\n","[2021-04-20 07:08:15,224 INFO] Building model...\n","[2021-04-20 07:08:21,807 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 07:08:21,890 INFO] encoder: 39221248\n","[2021-04-20 07:08:21,890 INFO] decoder: 43931466\n","[2021-04-20 07:08:21,890 INFO] * number of parameters: 83152714\n","[2021-04-20 07:08:22,404 INFO] Starting training on GPU: [0]\n","[2021-04-20 07:08:22,404 INFO] Start training loop without validation...\n","[2021-04-20 07:08:22,404 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:22,406 INFO] number of examples: 41\n","[2021-04-20 07:08:22,414 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:22,416 INFO] number of examples: 41\n","[2021-04-20 07:08:22,866 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:22,868 INFO] number of examples: 41\n","[2021-04-20 07:08:22,871 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:22,873 INFO] number of examples: 41\n","[2021-04-20 07:08:23,258 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:23,260 INFO] number of examples: 41\n","[2021-04-20 07:08:23,263 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:23,265 INFO] number of examples: 41\n","[2021-04-20 07:08:23,650 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:23,653 INFO] number of examples: 41\n","[2021-04-20 07:08:23,656 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:23,658 INFO] number of examples: 41\n","[2021-04-20 07:08:24,045 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:24,048 INFO] number of examples: 41\n","[2021-04-20 07:08:24,050 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:24,052 INFO] number of examples: 41\n","[2021-04-20 07:08:24,438 INFO] Step 30005/30020; acc:  36.18; ppl: 89.99; xent: 4.50; lr: 0.00051; 3491/4804 tok/s;      2 sec\n","[2021-04-20 07:08:24,608 INFO] Saving checkpoint sif_law/model_law_50/en-vi_step_30005.pt\n","[2021-04-20 07:08:28,049 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:28,052 INFO] number of examples: 41\n","[2021-04-20 07:08:28,054 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:28,056 INFO] number of examples: 41\n","[2021-04-20 07:08:28,460 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:28,463 INFO] number of examples: 41\n","[2021-04-20 07:08:28,466 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:28,468 INFO] number of examples: 41\n","[2021-04-20 07:08:28,856 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:28,859 INFO] number of examples: 41\n","[2021-04-20 07:08:28,862 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:28,864 INFO] number of examples: 41\n","[2021-04-20 07:08:29,254 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:29,256 INFO] number of examples: 41\n","[2021-04-20 07:08:29,259 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:29,261 INFO] number of examples: 41\n","[2021-04-20 07:08:29,645 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:29,648 INFO] number of examples: 41\n","[2021-04-20 07:08:29,650 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:29,652 INFO] number of examples: 41\n","[2021-04-20 07:08:30,037 INFO] Step 30010/30020; acc:  78.14; ppl:  4.22; xent: 1.44; lr: 0.00051; 1268/1745 tok/s;      8 sec\n","[2021-04-20 07:08:30,196 INFO] Saving checkpoint sif_law/model_law_50/en-vi_step_30010.pt\n","[2021-04-20 07:08:35,416 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:35,419 INFO] number of examples: 41\n","[2021-04-20 07:08:35,423 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:35,429 INFO] number of examples: 41\n","[2021-04-20 07:08:35,832 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:35,835 INFO] number of examples: 41\n","[2021-04-20 07:08:35,837 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:35,840 INFO] number of examples: 41\n","[2021-04-20 07:08:36,235 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:36,238 INFO] number of examples: 41\n","[2021-04-20 07:08:36,242 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:36,245 INFO] number of examples: 41\n","[2021-04-20 07:08:36,640 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:36,643 INFO] number of examples: 41\n","[2021-04-20 07:08:36,646 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:36,648 INFO] number of examples: 41\n","[2021-04-20 07:08:37,049 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:37,052 INFO] number of examples: 41\n","[2021-04-20 07:08:37,056 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:37,059 INFO] number of examples: 41\n","[2021-04-20 07:08:37,459 INFO] Step 30015/30020; acc:  90.86; ppl:  1.71; xent: 0.53; lr: 0.00051; 957/1316 tok/s;     15 sec\n","[2021-04-20 07:08:37,662 INFO] Saving checkpoint sif_law/model_law_50/en-vi_step_30015.pt\n","[2021-04-20 07:08:45,150 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:45,153 INFO] number of examples: 41\n","[2021-04-20 07:08:45,156 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:45,158 INFO] number of examples: 41\n","[2021-04-20 07:08:45,551 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:45,554 INFO] number of examples: 41\n","[2021-04-20 07:08:45,556 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:45,558 INFO] number of examples: 41\n","[2021-04-20 07:08:45,959 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:45,962 INFO] number of examples: 41\n","[2021-04-20 07:08:45,966 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:45,968 INFO] number of examples: 41\n","[2021-04-20 07:08:46,361 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:46,364 INFO] number of examples: 41\n","[2021-04-20 07:08:46,366 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:46,369 INFO] number of examples: 41\n","[2021-04-20 07:08:46,767 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:46,770 INFO] number of examples: 41\n","[2021-04-20 07:08:46,773 INFO] Loading dataset from sif_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:08:46,776 INFO] number of examples: 41\n","[2021-04-20 07:08:47,170 INFO] Step 30020/30020; acc:  96.96; ppl:  1.27; xent: 0.24; lr: 0.00051; 731/1006 tok/s;     25 sec\n","[2021-04-20 07:08:47,351 INFO] Saving checkpoint sif_law/model_law_50/en-vi_step_30020.pt\n","[2021-04-20 07:09:04,984 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:09:38,936 INFO] PRED AVG SCORE: -0.9962, PRED PPL: 2.7081\n","[2021-04-20 07:09:38,936 INFO] GOLD AVG SCORE: -6.1244, GOLD PPL: 456.8816\n","BLEU = 3.92, 25.4/8.1/2.1/0.8 (BP=0.900, ratio=0.905, hyp_len=2024, ref_len=2237)\n"," ===05==^======= \n","[2021-04-20 07:09:43,142 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:10:09,513 INFO] PRED AVG SCORE: -0.9810, PRED PPL: 2.6672\n","[2021-04-20 07:10:09,513 INFO] GOLD AVG SCORE: -7.1911, GOLD PPL: 1327.5082\n","BLEU = 2.68, 17.6/5.7/2.1/1.2 (BP=0.679, ratio=0.721, hyp_len=1612, ref_len=2237)\n"," ===10==^======= \n","[2021-04-20 07:10:12,974 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:10:36,209 INFO] PRED AVG SCORE: -0.5079, PRED PPL: 1.6618\n","[2021-04-20 07:10:36,209 INFO] GOLD AVG SCORE: -7.6400, GOLD PPL: 2079.8112\n","BLEU = 0.85, 10.5/2.7/0.5/0.1 (BP=0.701, ratio=0.738, hyp_len=1650, ref_len=2237)\n"," ===15==^======= \n","[2021-04-20 07:10:39,672 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:11:00,971 INFO] PRED AVG SCORE: -0.6143, PRED PPL: 1.8483\n","[2021-04-20 07:11:00,971 INFO] GOLD AVG SCORE: -7.1128, GOLD PPL: 1227.6130\n","BLEU = 0.79, 10.9/2.4/0.6/0.3 (BP=0.548, ratio=0.624, hyp_len=1397, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8AX0zOuPtfA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902816981,"user_tz":-420,"elapsed":155285,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"6c2f01e2-6165-49ec-f32a-a7673f3789f2"},"source":["!mkdir -p sif_law/output_law_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_law_100' \\\\\n","-train_tgt 'finetune/vi_finetune_law_100' \\\\\n","-save_data 'sif_law/output_law_100/en-vi' \n","\n","!mkdir -p sif_law/model_law_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_law/output_law_100/en-vi' \\\\\n","-save_model 'sif_law/model_law_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_law/model_law_100/en-vi_step_30005.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_law/model_law_100/en-vi_step_30010.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_law/model_law_100/en-vi_step_30015.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_law/model_law_100/en-vi_step_30020.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:11:03,458 INFO] Extracting features...\n","[2021-04-20 07:11:03,461 INFO]  * number of source features: 0.\n","[2021-04-20 07:11:03,461 INFO]  * number of target features: 0.\n","[2021-04-20 07:11:03,462 INFO] Building `Fields` object...\n","[2021-04-20 07:11:03,462 INFO] Building & saving training data...\n","[2021-04-20 07:11:03,473 INFO] Building shard 0.\n","[2021-04-20 07:11:03,477 INFO]  * saving 0th train data shard to sif_law/output_law_100/en-vi.train.0.pt.\n","[2021-04-20 07:11:03,574 INFO]  * tgt vocab size: 676.\n","[2021-04-20 07:11:03,575 INFO]  * src vocab size: 699.\n","[2021-04-20 07:11:06,044 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 07:11:07,277 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 07:11:07,280 INFO]  * src vocab size = 39660\n","[2021-04-20 07:11:07,280 INFO]  * tgt vocab size = 18250\n","[2021-04-20 07:11:07,280 INFO] Building model...\n","[2021-04-20 07:11:10,346 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 07:11:10,412 INFO] encoder: 39221248\n","[2021-04-20 07:11:10,412 INFO] decoder: 43931466\n","[2021-04-20 07:11:10,412 INFO] * number of parameters: 83152714\n","[2021-04-20 07:11:10,935 INFO] Starting training on GPU: [0]\n","[2021-04-20 07:11:10,935 INFO] Start training loop without validation...\n","[2021-04-20 07:11:10,935 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:10,938 INFO] number of examples: 82\n","[2021-04-20 07:11:11,339 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:11,343 INFO] number of examples: 82\n","[2021-04-20 07:11:11,718 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:11,721 INFO] number of examples: 82\n","[2021-04-20 07:11:12,094 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:12,098 INFO] number of examples: 82\n","[2021-04-20 07:11:12,470 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:12,473 INFO] number of examples: 82\n","[2021-04-20 07:11:12,852 INFO] Step 30005/30020; acc:  30.45; ppl: 126.46; xent: 4.84; lr: 0.00051; 3795/5058 tok/s;      2 sec\n","[2021-04-20 07:11:13,012 INFO] Saving checkpoint sif_law/model_law_100/en-vi_step_30005.pt\n","[2021-04-20 07:11:17,876 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:17,890 INFO] number of examples: 82\n","[2021-04-20 07:11:18,279 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:18,282 INFO] number of examples: 82\n","[2021-04-20 07:11:18,654 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:18,658 INFO] number of examples: 82\n","[2021-04-20 07:11:19,032 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:19,035 INFO] number of examples: 82\n","[2021-04-20 07:11:19,413 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:19,416 INFO] number of examples: 82\n","[2021-04-20 07:11:19,787 INFO] Step 30010/30020; acc:  67.01; ppl:  7.18; xent: 1.97; lr: 0.00051; 1049/1398 tok/s;      9 sec\n","[2021-04-20 07:11:19,948 INFO] Saving checkpoint sif_law/model_law_100/en-vi_step_30010.pt\n","[2021-04-20 07:11:24,108 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:24,363 INFO] number of examples: 82\n","[2021-04-20 07:11:24,760 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:24,763 INFO] number of examples: 82\n","[2021-04-20 07:11:25,157 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:25,162 INFO] number of examples: 82\n","[2021-04-20 07:11:25,558 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:25,561 INFO] number of examples: 82\n","[2021-04-20 07:11:25,948 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:25,952 INFO] number of examples: 82\n","[2021-04-20 07:11:26,328 INFO] Step 30015/30020; acc:  86.37; ppl:  2.16; xent: 0.77; lr: 0.00051; 1112/1482 tok/s;     15 sec\n","[2021-04-20 07:11:26,498 INFO] Saving checkpoint sif_law/model_law_100/en-vi_step_30015.pt\n","[2021-04-20 07:11:34,332 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:34,336 INFO] number of examples: 82\n","[2021-04-20 07:11:34,755 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:34,761 INFO] number of examples: 82\n","[2021-04-20 07:11:35,181 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:35,185 INFO] number of examples: 82\n","[2021-04-20 07:11:35,593 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:35,598 INFO] number of examples: 82\n","[2021-04-20 07:11:35,998 INFO] Loading dataset from sif_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:11:36,002 INFO] number of examples: 82\n","[2021-04-20 07:11:36,397 INFO] Step 30020/30020; acc:  95.03; ppl:  1.36; xent: 0.31; lr: 0.00051; 723/963 tok/s;     25 sec\n","[2021-04-20 07:11:36,635 INFO] Saving checkpoint sif_law/model_law_100/en-vi_step_30020.pt\n","[2021-04-20 07:11:54,647 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:12:24,342 INFO] PRED AVG SCORE: -1.0488, PRED PPL: 2.8543\n","[2021-04-20 07:12:24,342 INFO] GOLD AVG SCORE: -5.6048, GOLD PPL: 271.7223\n","BLEU = 4.75, 28.1/10.3/3.1/1.0 (BP=0.873, ratio=0.881, hyp_len=1970, ref_len=2237)\n"," ===05==^======= \n","[2021-04-20 07:12:27,839 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:12:52,877 INFO] PRED AVG SCORE: -0.9476, PRED PPL: 2.5795\n","[2021-04-20 07:12:52,877 INFO] GOLD AVG SCORE: -5.8648, GOLD PPL: 352.4236\n","BLEU = 3.63, 26.4/9.7/3.1/1.3 (BP=0.641, ratio=0.692, hyp_len=1548, ref_len=2237)\n"," ===10==^======= \n","[2021-04-20 07:12:56,382 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:13:15,416 INFO] PRED AVG SCORE: -0.6367, PRED PPL: 1.8903\n","[2021-04-20 07:13:15,417 INFO] GOLD AVG SCORE: -6.4859, GOLD PPL: 655.8604\n","BLEU = 1.13, 14.2/5.3/1.2/0.5 (BP=0.429, ratio=0.542, hyp_len=1212, ref_len=2237)\n"," ===15==^======= \n","[2021-04-20 07:13:18,940 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:13:36,224 INFO] PRED AVG SCORE: -0.7500, PRED PPL: 2.1171\n","[2021-04-20 07:13:36,225 INFO] GOLD AVG SCORE: -6.4387, GOLD PPL: 625.6212\n","BLEU = 0.90, 17.6/5.5/1.4/0.5 (BP=0.318, ratio=0.466, hyp_len=1042, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPF4HGOLHA0G","executionInfo":{"status":"ok","timestamp":1618902985253,"user_tz":-420,"elapsed":168270,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"887a12ab-31e8-4770-c14f-8d74cb09c486"},"source":["!mkdir -p sif_law/output_law_118\n","!onmt_preprocess -train_src 'finetune/en_finetune_law_118' \\\\\n","-train_tgt 'finetune/vi_finetune_law_118' \\\\\n","-save_data 'sif_law/output_law_118/en-vi' \n","\n","!mkdir -p sif_law/model_law_118\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_law/output_law_118/en-vi' \\\\\n","-save_model 'sif_law/model_law_118/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_law/model_law_118/en-vi_step_30005.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_118/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_118/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_law/model_law_118/en-vi_step_30010.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_118/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_118/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_law/model_law_118/en-vi_step_30015.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_118/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_118/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_law/model_law_118/en-vi_step_30020.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/model_law_118/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/model_law_118/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":10,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:13:38,773 INFO] Extracting features...\n","[2021-04-20 07:13:38,778 INFO]  * number of source features: 0.\n","[2021-04-20 07:13:38,778 INFO]  * number of target features: 0.\n","[2021-04-20 07:13:38,778 INFO] Building `Fields` object...\n","[2021-04-20 07:13:38,778 INFO] Building & saving training data...\n","[2021-04-20 07:13:38,789 INFO] Building shard 0.\n","[2021-04-20 07:13:38,794 INFO]  * saving 0th train data shard to sif_law/output_law_118/en-vi.train.0.pt.\n","[2021-04-20 07:13:38,890 INFO]  * tgt vocab size: 730.\n","[2021-04-20 07:13:38,891 INFO]  * src vocab size: 766.\n","[2021-04-20 07:13:41,309 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 07:13:42,511 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 07:13:42,515 INFO]  * src vocab size = 39660\n","[2021-04-20 07:13:42,515 INFO]  * tgt vocab size = 18250\n","[2021-04-20 07:13:42,515 INFO] Building model...\n","[2021-04-20 07:13:45,636 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 07:13:45,701 INFO] encoder: 39221248\n","[2021-04-20 07:13:45,701 INFO] decoder: 43931466\n","[2021-04-20 07:13:45,702 INFO] * number of parameters: 83152714\n","[2021-04-20 07:13:46,231 INFO] Starting training on GPU: [0]\n","[2021-04-20 07:13:46,231 INFO] Start training loop without validation...\n","[2021-04-20 07:13:46,231 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:46,235 INFO] number of examples: 98\n","[2021-04-20 07:13:46,635 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:46,639 INFO] number of examples: 98\n","[2021-04-20 07:13:47,005 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:47,009 INFO] number of examples: 98\n","[2021-04-20 07:13:47,385 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:47,389 INFO] number of examples: 98\n","[2021-04-20 07:13:47,758 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:47,761 INFO] number of examples: 98\n","[2021-04-20 07:13:48,128 INFO] Step 30005/30020; acc:  29.51; ppl: 136.95; xent: 4.92; lr: 0.00051; 4334/5811 tok/s;      2 sec\n","[2021-04-20 07:13:48,292 INFO] Saving checkpoint sif_law/model_law_118/en-vi_step_30005.pt\n","[2021-04-20 07:13:53,160 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:53,164 INFO] number of examples: 98\n","[2021-04-20 07:13:53,552 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:53,556 INFO] number of examples: 98\n","[2021-04-20 07:13:53,925 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:53,929 INFO] number of examples: 98\n","[2021-04-20 07:13:54,301 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:54,304 INFO] number of examples: 98\n","[2021-04-20 07:13:54,678 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:54,682 INFO] number of examples: 98\n","[2021-04-20 07:13:55,054 INFO] Step 30010/30020; acc:  63.48; ppl:  8.50; xent: 2.14; lr: 0.00051; 1187/1591 tok/s;      9 sec\n","[2021-04-20 07:13:55,217 INFO] Saving checkpoint sif_law/model_law_118/en-vi_step_30010.pt\n","[2021-04-20 07:13:59,089 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:13:59,745 INFO] number of examples: 98\n","[2021-04-20 07:14:00,112 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:00,600 INFO] number of examples: 98\n","[2021-04-20 07:14:00,981 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:00,986 INFO] number of examples: 98\n","[2021-04-20 07:14:01,380 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:01,384 INFO] number of examples: 98\n","[2021-04-20 07:14:01,755 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:01,759 INFO] number of examples: 98\n","[2021-04-20 07:14:02,147 INFO] Step 30015/30020; acc:  84.41; ppl:  2.36; xent: 0.86; lr: 0.00051; 1159/1554 tok/s;     16 sec\n","[2021-04-20 07:14:02,329 INFO] Saving checkpoint sif_law/model_law_118/en-vi_step_30015.pt\n","[2021-04-20 07:14:09,265 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:09,280 INFO] number of examples: 98\n","[2021-04-20 07:14:09,684 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:09,690 INFO] number of examples: 98\n","[2021-04-20 07:14:10,090 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:10,095 INFO] number of examples: 98\n","[2021-04-20 07:14:10,478 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:10,483 INFO] number of examples: 98\n","[2021-04-20 07:14:10,884 INFO] Loading dataset from sif_law/output_law_118/en-vi.train.0.pt\n","[2021-04-20 07:14:10,890 INFO] number of examples: 98\n","[2021-04-20 07:14:11,285 INFO] Step 30020/30020; acc:  94.56; ppl:  1.40; xent: 0.33; lr: 0.00051; 900/1206 tok/s;     25 sec\n","[2021-04-20 07:14:11,477 INFO] Saving checkpoint sif_law/model_law_118/en-vi_step_30020.pt\n","[2021-04-20 07:14:38,002 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:15:06,580 INFO] PRED AVG SCORE: -1.0296, PRED PPL: 2.8000\n","[2021-04-20 07:15:06,580 INFO] GOLD AVG SCORE: -5.5673, GOLD PPL: 261.7337\n","BLEU = 4.38, 29.4/11.0/3.4/1.3 (BP=0.709, ratio=0.744, hyp_len=1664, ref_len=2237)\n"," ===05==^======= \n","[2021-04-20 07:15:09,987 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:15:33,308 INFO] PRED AVG SCORE: -0.8587, PRED PPL: 2.3601\n","[2021-04-20 07:15:33,308 INFO] GOLD AVG SCORE: -5.6754, GOLD PPL: 291.6048\n","BLEU = 3.81, 27.3/9.9/3.4/1.6 (BP=0.617, ratio=0.674, hyp_len=1508, ref_len=2237)\n"," ===10==^======= \n","[2021-04-20 07:15:36,777 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:16:01,685 INFO] PRED AVG SCORE: -0.7630, PRED PPL: 2.1446\n","[2021-04-20 07:16:01,685 INFO] GOLD AVG SCORE: -6.3735, GOLD PPL: 586.0969\n","BLEU = 1.77, 13.9/4.9/1.2/0.5 (BP=0.697, ratio=0.735, hyp_len=1644, ref_len=2237)\n"," ===15==^======= \n","[2021-04-20 07:16:05,116 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:16:24,481 INFO] PRED AVG SCORE: -0.7662, PRED PPL: 2.1516\n","[2021-04-20 07:16:24,481 INFO] GOLD AVG SCORE: -6.3641, GOLD PPL: 580.6297\n","BLEU = 1.59, 20.2/6.9/1.7/0.7 (BP=0.442, ratio=0.551, hyp_len=1232, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618903015966,"user_tz":-420,"elapsed":198970,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"31437985-97cf-4c47-96ec-c7e2a29ecf03"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_law.en -tgt finetune/test_law.vi -output sif_law/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < sif_law/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":11,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:16:28,294 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:16:55,215 INFO] PRED AVG SCORE: -1.1685, PRED PPL: 3.2173\n","[2021-04-20 07:16:55,215 INFO] GOLD AVG SCORE: -8.3939, GOLD PPL: 4420.1260\n","BLEU = 2.95, 19.5/5.7/1.4/0.5 (BP=0.983, ratio=0.983, hyp_len=2200, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}