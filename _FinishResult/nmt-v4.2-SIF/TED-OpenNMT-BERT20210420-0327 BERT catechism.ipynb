{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-BERT20210420-0327 BERT catechism.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892359795,"user_tz":-420,"elapsed":19114,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"05d1aeaf-70f9-41ec-d55d-b564a56ad4fb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892393036,"user_tz":-420,"elapsed":12252,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"cd3b29cb-e1a1-4b4f-9b8a-408e2cd5ab21"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-BERT\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327'\n","os.chdir(path)\n","!pwd"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889998501,"user_tz":-420,"elapsed":920,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e8687b1f-6322-482b-b33d-3c18e373c856"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:39:58 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892393035,"user_tz":-420,"elapsed":15566,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"3d9b3336-19bb-4b94-f9c4-658227ed49ea"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 30.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 27.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 14.2MB/s \n","\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 20.9MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=d9fa3b234b53f4809863618e4844925701edb1d8bc09b780e36ebc4e8ce07c1b\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: pyonmttok, torchtext, configargparse, waitress, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAscbACoOygX","executionInfo":{"status":"ok","timestamp":1618887835865,"user_tz":-420,"elapsed":1201,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2c865aae-0635-4032-c4e5-9b4226bfb7c4"},"source":["!rm -rf SIF-finetune.tar.gz\n","!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889442257,"user_tz":-420,"elapsed":1687,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"12a7c093-4a7c-4137-8c78-b7085a63b06e"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","!tar -xvf 'SIF-finetune-bert.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 03:30:40--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 180907 (177K) [application/octet-stream]\n","Saving to: ‘SIF-finetune-bert.tar.gz’\n","\n","\rSIF-finetune-bert.t   0%[                    ]       0  --.-KB/s               \rSIF-finetune-bert.t 100%[===================>] 176.67K  --.-KB/s    in 0.03s   \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 03:30:41 (6.05 MB/s) - ‘SIF-finetune-bert.tar.gz’ saved [180907/180907]\n","\n","finetune/\n","finetune/en_finetune_climate_150\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_150\n","finetune/en_finetune_climate_100\n","finetune/vi_finetune_buddhism_162\n","finetune/en_finetune_law_50\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/test_climate.vi\n","finetune/en_finetune_climate_191\n","finetune/test_catechism.vi\n","finetune/vi_finetune_law_50\n","finetune/en_finetune_law_100\n","finetune/test_law.en\n","finetune/en_finetune_buddhism_162\n","finetune/vi_finetune_law_129\n","finetune/vi_finetune_catechism_50\n","finetune/en_finetune_climate_50\n","finetune/vi_finetune_catechism_100\n","finetune/en_finetune_buddhism_100\n","finetune/test_climate.en\n","finetune/en_finetune_catechism_112\n","finetune/vi_finetune_law_100\n","finetune/test_law.vi\n","finetune/en_finetune_buddhism_150\n","finetune/vi_finetune_catechism_112\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_climate_191\n","finetune/en_finetune_buddhism_50\n","finetune/test_catechism.en\n","finetune/test_buddhism.vi\n","finetune/test_buddhism.en\n","finetune/vi_finetune_climate_100\n","finetune/en_finetune_law_129\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618889447511,"user_tz":-420,"elapsed":1467,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2ab041d5-da8a-4d87-c2e4-0d8fa665e0fe"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 535\n","-rw------- 1 root root  9453 Apr 20 02:00 en_finetune_buddhism_100\n","-rw------- 1 root root 14772 Apr 20 02:00 en_finetune_buddhism_150\n","-rw------- 1 root root 16471 Apr 20 02:00 en_finetune_buddhism_162\n","-rw------- 1 root root  4892 Apr 20 02:00 en_finetune_buddhism_50\n","-rw------- 1 root root 13295 Apr 20 02:00 en_finetune_catechism_100\n","-rw------- 1 root root 14691 Apr 20 02:00 en_finetune_catechism_112\n","-rw------- 1 root root  6722 Apr 20 02:00 en_finetune_catechism_50\n","-rw------- 1 root root 13996 Apr 20 02:00 en_finetune_climate_100\n","-rw------- 1 root root 21088 Apr 20 02:00 en_finetune_climate_150\n","-rw------- 1 root root 27050 Apr 20 02:00 en_finetune_climate_191\n","-rw------- 1 root root  7039 Apr 20 02:00 en_finetune_climate_50\n","-rw------- 1 root root 13779 Apr 20 02:00 en_finetune_law_100\n","-rw------- 1 root root 18325 Apr 20 02:00 en_finetune_law_129\n","-rw------- 1 root root  6255 Apr 20 02:00 en_finetune_law_50\n","-rw------- 1 root root  7711 Apr 20 02:00 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 02:00 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 02:00 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 02:00 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 02:00 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 02:00 test_climate.vi\n","-rw------- 1 root root 11267 Apr 20 02:00 test_law.en\n","-rw------- 1 root root 15336 Apr 20 02:00 test_law.vi\n","-rw------- 1 root root 13704 Apr 20 02:00 vi_finetune_buddhism_100\n","-rw------- 1 root root 21690 Apr 20 02:00 vi_finetune_buddhism_150\n","-rw------- 1 root root 24138 Apr 20 02:00 vi_finetune_buddhism_162\n","-rw------- 1 root root  7050 Apr 20 02:00 vi_finetune_buddhism_50\n","-rw------- 1 root root 18233 Apr 20 02:00 vi_finetune_catechism_100\n","-rw------- 1 root root 20199 Apr 20 02:00 vi_finetune_catechism_112\n","-rw------- 1 root root  8920 Apr 20 02:00 vi_finetune_catechism_50\n","-rw------- 1 root root 18493 Apr 20 02:00 vi_finetune_climate_100\n","-rw------- 1 root root 27537 Apr 20 02:00 vi_finetune_climate_150\n","-rw------- 1 root root 35637 Apr 20 02:00 vi_finetune_climate_191\n","-rw------- 1 root root  9330 Apr 20 02:00 vi_finetune_climate_50\n","-rw------- 1 root root 17161 Apr 20 02:00 vi_finetune_law_100\n","-rw------- 1 root root 22753 Apr 20 02:00 vi_finetune_law_129\n","-rw------- 1 root root  7974 Apr 20 02:00 vi_finetune_law_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618889473587,"user_tz":-420,"elapsed":19250,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2c0ec67-ded8-4c4b-e9c3-760c282cf56c"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 23.94 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892583540,"user_tz":-420,"elapsed":171853,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"a36045b2-cf21-4563-82cc-b9406a922c2a"},"source":["!mkdir -p bert_catechism/output_catechism_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_50' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_50' \\\\\n","-save_data 'bert_catechism/output_catechism_50/en-vi' \n","\n","!mkdir -p bert_catechism/model_catechism_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_catechism/output_catechism_50/en-vi' \\\\\n","-save_model 'bert_catechism/model_catechism_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_catechism/model_catechism_50/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_50/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_50/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_50/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":5,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:20:18,195 INFO] Extracting features...\n","[2021-04-20 04:20:19,323 INFO]  * number of source features: 0.\n","[2021-04-20 04:20:19,324 INFO]  * number of target features: 0.\n","[2021-04-20 04:20:19,324 INFO] Building `Fields` object...\n","[2021-04-20 04:20:19,324 INFO] Building & saving training data...\n","[2021-04-20 04:20:19,334 INFO] Building shard 0.\n","[2021-04-20 04:20:19,337 INFO]  * saving 0th train data shard to bert_catechism/output_catechism_50/en-vi.train.0.pt.\n","[2021-04-20 04:20:19,434 INFO]  * tgt vocab size: 559.\n","[2021-04-20 04:20:19,434 INFO]  * src vocab size: 482.\n","[2021-04-20 04:20:38,736 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:20:40,006 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:20:40,009 INFO]  * src vocab size = 39660\n","[2021-04-20 04:20:40,010 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:20:40,010 INFO] Building model...\n","[2021-04-20 04:20:47,709 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:20:47,743 INFO] encoder: 39221248\n","[2021-04-20 04:20:47,744 INFO] decoder: 43931466\n","[2021-04-20 04:20:47,744 INFO] * number of parameters: 83152714\n","[2021-04-20 04:20:48,294 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:20:48,294 INFO] Start training loop without validation...\n","[2021-04-20 04:20:48,294 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:48,297 INFO] number of examples: 45\n","[2021-04-20 04:20:48,305 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:48,309 INFO] number of examples: 45\n","[2021-04-20 04:20:48,620 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:48,623 INFO] number of examples: 45\n","[2021-04-20 04:20:48,626 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:48,629 INFO] number of examples: 45\n","[2021-04-20 04:20:48,860 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:48,864 INFO] number of examples: 45\n","[2021-04-20 04:20:48,867 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:48,869 INFO] number of examples: 45\n","[2021-04-20 04:20:49,098 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:49,101 INFO] number of examples: 45\n","[2021-04-20 04:20:49,104 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:49,107 INFO] number of examples: 45\n","[2021-04-20 04:20:49,337 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:49,340 INFO] number of examples: 45\n","[2021-04-20 04:20:49,343 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:49,346 INFO] number of examples: 45\n","[2021-04-20 04:20:49,577 INFO] Step 30005/30020; acc:  25.21; ppl: 208.71; xent: 5.34; lr: 0.00051; 7561/9183 tok/s;      1 sec\n","[2021-04-20 04:20:49,751 INFO] Saving checkpoint bert_catechism/model_catechism_50/en-vi_step_30005.pt\n","[2021-04-20 04:20:53,835 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:53,838 INFO] number of examples: 45\n","[2021-04-20 04:20:53,842 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:53,845 INFO] number of examples: 45\n","[2021-04-20 04:20:54,096 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,099 INFO] number of examples: 45\n","[2021-04-20 04:20:54,102 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,105 INFO] number of examples: 45\n","[2021-04-20 04:20:54,340 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,343 INFO] number of examples: 45\n","[2021-04-20 04:20:54,346 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,349 INFO] number of examples: 45\n","[2021-04-20 04:20:54,581 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,585 INFO] number of examples: 45\n","[2021-04-20 04:20:54,588 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,591 INFO] number of examples: 45\n","[2021-04-20 04:20:54,824 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,827 INFO] number of examples: 45\n","[2021-04-20 04:20:54,830 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:54,832 INFO] number of examples: 45\n","[2021-04-20 04:20:55,063 INFO] Step 30010/30020; acc:  65.81; ppl:  7.82; xent: 2.06; lr: 0.00051; 1768/2147 tok/s;      7 sec\n","[2021-04-20 04:20:55,239 INFO] Saving checkpoint bert_catechism/model_catechism_50/en-vi_step_30010.pt\n","[2021-04-20 04:20:59,262 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,265 INFO] number of examples: 45\n","[2021-04-20 04:20:59,268 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,270 INFO] number of examples: 45\n","[2021-04-20 04:20:59,505 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,508 INFO] number of examples: 45\n","[2021-04-20 04:20:59,511 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,513 INFO] number of examples: 45\n","[2021-04-20 04:20:59,742 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,745 INFO] number of examples: 45\n","[2021-04-20 04:20:59,748 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,751 INFO] number of examples: 45\n","[2021-04-20 04:20:59,983 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,986 INFO] number of examples: 45\n","[2021-04-20 04:20:59,989 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:20:59,991 INFO] number of examples: 45\n","[2021-04-20 04:21:00,223 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:00,226 INFO] number of examples: 45\n","[2021-04-20 04:21:00,229 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:00,232 INFO] number of examples: 45\n","[2021-04-20 04:21:00,461 INFO] Step 30015/30020; acc:  88.23; ppl:  1.93; xent: 0.66; lr: 0.00051; 1797/2182 tok/s;     12 sec\n","[2021-04-20 04:21:00,660 INFO] Saving checkpoint bert_catechism/model_catechism_50/en-vi_step_30015.pt\n","[2021-04-20 04:21:10,293 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:10,296 INFO] number of examples: 45\n","[2021-04-20 04:21:10,300 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:10,303 INFO] number of examples: 45\n","[2021-04-20 04:21:10,553 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:10,557 INFO] number of examples: 45\n","[2021-04-20 04:21:10,560 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:10,564 INFO] number of examples: 45\n","[2021-04-20 04:21:10,807 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:10,810 INFO] number of examples: 45\n","[2021-04-20 04:21:10,813 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:10,816 INFO] number of examples: 45\n","[2021-04-20 04:21:11,069 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:11,073 INFO] number of examples: 45\n","[2021-04-20 04:21:11,077 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:11,080 INFO] number of examples: 45\n","[2021-04-20 04:21:11,332 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:11,336 INFO] number of examples: 45\n","[2021-04-20 04:21:11,340 INFO] Loading dataset from bert_catechism/output_catechism_50/en-vi.train.0.pt\n","[2021-04-20 04:21:11,343 INFO] number of examples: 45\n","[2021-04-20 04:21:11,595 INFO] Step 30020/30020; acc:  96.21; ppl:  1.29; xent: 0.25; lr: 0.00051; 871/1058 tok/s;     23 sec\n","[2021-04-20 04:21:11,825 INFO] Saving checkpoint bert_catechism/model_catechism_50/en-vi_step_30020.pt\n","[2021-04-20 04:21:29,718 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:21:50,939 INFO] PRED AVG SCORE: -1.1431, PRED PPL: 3.1365\n","[2021-04-20 04:21:50,940 INFO] GOLD AVG SCORE: -6.4677, GOLD PPL: 644.0028\n","BLEU = 2.42, 29.7/7.6/2.1/0.5 (BP=0.603, ratio=0.664, hyp_len=1533, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:21:56,126 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:22:18,717 INFO] PRED AVG SCORE: -0.8720, PRED PPL: 2.3916\n","[2021-04-20 04:22:18,717 INFO] GOLD AVG SCORE: -7.3003, GOLD PPL: 1480.7555\n","BLEU = 2.70, 23.1/6.2/1.9/0.5 (BP=0.801, ratio=0.818, hyp_len=1888, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:22:22,509 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:22:39,980 INFO] PRED AVG SCORE: -0.6003, PRED PPL: 1.8227\n","[2021-04-20 04:22:39,980 INFO] GOLD AVG SCORE: -7.7271, GOLD PPL: 2269.0493\n","BLEU = 0.59, 11.0/2.6/0.6/0.1 (BP=0.525, ratio=0.608, hyp_len=1404, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:22:43,653 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:23:02,634 INFO] PRED AVG SCORE: -0.6563, PRED PPL: 1.9277\n","[2021-04-20 04:23:02,635 INFO] GOLD AVG SCORE: -7.6418, GOLD PPL: 2083.5766\n","BLEU = 1.67, 14.5/3.0/1.2/0.6 (BP=0.697, ratio=0.735, hyp_len=1696, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8AX0zOuPtfA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892732777,"user_tz":-420,"elapsed":148107,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"435ba5cc-615d-4946-d5ac-01dd247e783d"},"source":["!mkdir -p bert_catechism/output_catechism_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_100' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_100' \\\\\n","-save_data 'bert_catechism/output_catechism_100/en-vi' \n","\n","!mkdir -p bert_catechism/model_catechism_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_catechism/output_catechism_100/en-vi' \\\\\n","-save_model 'bert_catechism/model_catechism_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_catechism/model_catechism_100/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_100/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_100/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_100/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:23:06,186 INFO] Extracting features...\n","[2021-04-20 04:23:07,389 INFO]  * number of source features: 0.\n","[2021-04-20 04:23:07,389 INFO]  * number of target features: 0.\n","[2021-04-20 04:23:07,389 INFO] Building `Fields` object...\n","[2021-04-20 04:23:07,389 INFO] Building & saving training data...\n","[2021-04-20 04:23:07,402 INFO] Building shard 0.\n","[2021-04-20 04:23:07,406 INFO]  * saving 0th train data shard to bert_catechism/output_catechism_100/en-vi.train.0.pt.\n","[2021-04-20 04:23:07,503 INFO]  * tgt vocab size: 804.\n","[2021-04-20 04:23:07,504 INFO]  * src vocab size: 734.\n","[2021-04-20 04:23:10,327 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:23:11,711 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:23:11,715 INFO]  * src vocab size = 39660\n","[2021-04-20 04:23:11,716 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:23:11,716 INFO] Building model...\n","[2021-04-20 04:23:15,972 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:23:16,018 INFO] encoder: 39221248\n","[2021-04-20 04:23:16,018 INFO] decoder: 43931466\n","[2021-04-20 04:23:16,018 INFO] * number of parameters: 83152714\n","[2021-04-20 04:23:16,614 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:23:16,614 INFO] Start training loop without validation...\n","[2021-04-20 04:23:16,614 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:16,619 INFO] number of examples: 87\n","[2021-04-20 04:23:16,910 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:16,915 INFO] number of examples: 87\n","[2021-04-20 04:23:17,161 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:17,165 INFO] number of examples: 87\n","[2021-04-20 04:23:17,411 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:17,415 INFO] number of examples: 87\n","[2021-04-20 04:23:17,663 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:17,667 INFO] number of examples: 87\n","[2021-04-20 04:23:17,913 INFO] Step 30005/30020; acc:  20.08; ppl: 295.31; xent: 5.69; lr: 0.00051; 6803/8627 tok/s;      1 sec\n","[2021-04-20 04:23:18,108 INFO] Saving checkpoint bert_catechism/model_catechism_100/en-vi_step_30005.pt\n","[2021-04-20 04:23:22,200 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:22,206 INFO] number of examples: 87\n","[2021-04-20 04:23:22,473 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:22,478 INFO] number of examples: 87\n","[2021-04-20 04:23:22,731 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:22,735 INFO] number of examples: 87\n","[2021-04-20 04:23:22,990 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:22,995 INFO] number of examples: 87\n","[2021-04-20 04:23:23,242 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:23,246 INFO] number of examples: 87\n","[2021-04-20 04:23:23,489 INFO] Step 30010/30020; acc:  50.82; ppl: 16.46; xent: 2.80; lr: 0.00051; 1586/2011 tok/s;      7 sec\n","[2021-04-20 04:23:23,678 INFO] Saving checkpoint bert_catechism/model_catechism_100/en-vi_step_30010.pt\n","[2021-04-20 04:23:27,939 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:27,943 INFO] number of examples: 87\n","[2021-04-20 04:23:28,194 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:28,198 INFO] number of examples: 87\n","[2021-04-20 04:23:28,448 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:28,452 INFO] number of examples: 87\n","[2021-04-20 04:23:28,698 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:28,702 INFO] number of examples: 87\n","[2021-04-20 04:23:28,945 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:28,949 INFO] number of examples: 87\n","[2021-04-20 04:23:29,196 INFO] Step 30015/30020; acc:  78.88; ppl:  3.03; xent: 1.11; lr: 0.00051; 1549/1964 tok/s;     13 sec\n","[2021-04-20 04:23:29,390 INFO] Saving checkpoint bert_catechism/model_catechism_100/en-vi_step_30015.pt\n","[2021-04-20 04:23:39,822 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:39,828 INFO] number of examples: 87\n","[2021-04-20 04:23:40,092 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:40,098 INFO] number of examples: 87\n","[2021-04-20 04:23:40,381 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:40,386 INFO] number of examples: 87\n","[2021-04-20 04:23:40,674 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:40,679 INFO] number of examples: 87\n","[2021-04-20 04:23:40,964 INFO] Loading dataset from bert_catechism/output_catechism_100/en-vi.train.0.pt\n","[2021-04-20 04:23:40,969 INFO] number of examples: 87\n","[2021-04-20 04:23:41,258 INFO] Step 30020/30020; acc:  92.85; ppl:  1.48; xent: 0.39; lr: 0.00051; 733/929 tok/s;     25 sec\n","[2021-04-20 04:23:41,504 INFO] Saving checkpoint bert_catechism/model_catechism_100/en-vi_step_30020.pt\n","[2021-04-20 04:24:00,368 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:24:19,726 INFO] PRED AVG SCORE: -1.2459, PRED PPL: 3.4762\n","[2021-04-20 04:24:19,726 INFO] GOLD AVG SCORE: -6.0992, GOLD PPL: 445.4976\n","BLEU = 2.26, 29.4/8.7/3.4/1.6 (BP=0.369, ratio=0.500, hyp_len=1155, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:24:23,620 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:24:47,086 INFO] PRED AVG SCORE: -0.8771, PRED PPL: 2.4040\n","[2021-04-20 04:24:47,086 INFO] GOLD AVG SCORE: -6.2049, GOLD PPL: 495.1685\n","BLEU = 5.08, 25.4/7.6/2.9/1.4 (BP=0.970, ratio=0.971, hyp_len=2240, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:24:50,857 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:25:08,588 INFO] PRED AVG SCORE: -0.9972, PRED PPL: 2.7106\n","[2021-04-20 04:25:08,588 INFO] GOLD AVG SCORE: -6.8024, GOLD PPL: 899.9759\n","BLEU = 1.27, 19.0/5.4/1.3/0.5 (BP=0.445, ratio=0.552, hyp_len=1275, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:25:12,389 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:25:31,918 INFO] PRED AVG SCORE: -0.6921, PRED PPL: 1.9979\n","[2021-04-20 04:25:31,918 INFO] GOLD AVG SCORE: -7.3032, GOLD PPL: 1485.0040\n","BLEU = 1.39, 13.3/2.9/1.0/0.3 (BP=0.745, ratio=0.773, hyp_len=1784, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmEAHw2aVdLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892890124,"user_tz":-420,"elapsed":157345,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"7236b4e4-f415-40f7-a856-1f94b026ca0d"},"source":["!mkdir -p bert_catechism/output_catechism_112\n","!onmt_preprocess -train_src 'finetune/en_finetune_catechism_112' \\\\\n","-train_tgt 'finetune/vi_finetune_catechism_112' \\\\\n","-save_data 'bert_catechism/output_catechism_112/en-vi' \n","\n","!mkdir -p bert_catechism/model_catechism_112\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_catechism/output_catechism_112/en-vi' \\\\\n","-save_model 'bert_catechism/model_catechism_112/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_catechism/model_catechism_112/en-vi_step_30005.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_112/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_112/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_112/en-vi_step_30010.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_112/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_112/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_112/en-vi_step_30015.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_112/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_112/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_catechism/model_catechism_112/en-vi_step_30020.pt -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/model_catechism_112/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/model_catechism_112/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:25:34,655 INFO] Extracting features...\n","[2021-04-20 04:25:35,855 INFO]  * number of source features: 0.\n","[2021-04-20 04:25:35,855 INFO]  * number of target features: 0.\n","[2021-04-20 04:25:35,856 INFO] Building `Fields` object...\n","[2021-04-20 04:25:35,856 INFO] Building & saving training data...\n","[2021-04-20 04:25:35,868 INFO] Building shard 0.\n","[2021-04-20 04:25:35,872 INFO]  * saving 0th train data shard to bert_catechism/output_catechism_112/en-vi.train.0.pt.\n","[2021-04-20 04:25:35,969 INFO]  * tgt vocab size: 875.\n","[2021-04-20 04:25:35,969 INFO]  * src vocab size: 822.\n","[2021-04-20 04:25:38,623 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 04:25:39,967 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 04:25:39,971 INFO]  * src vocab size = 39660\n","[2021-04-20 04:25:39,971 INFO]  * tgt vocab size = 18250\n","[2021-04-20 04:25:39,971 INFO] Building model...\n","[2021-04-20 04:25:44,272 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 04:25:44,314 INFO] encoder: 39221248\n","[2021-04-20 04:25:44,314 INFO] decoder: 43931466\n","[2021-04-20 04:25:44,314 INFO] * number of parameters: 83152714\n","[2021-04-20 04:25:44,886 INFO] Starting training on GPU: [0]\n","[2021-04-20 04:25:44,886 INFO] Start training loop without validation...\n","[2021-04-20 04:25:44,886 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:44,891 INFO] number of examples: 99\n","[2021-04-20 04:25:45,167 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:45,172 INFO] number of examples: 99\n","[2021-04-20 04:25:45,415 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:45,419 INFO] number of examples: 99\n","[2021-04-20 04:25:45,657 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:45,661 INFO] number of examples: 99\n","[2021-04-20 04:25:45,904 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:45,909 INFO] number of examples: 99\n","[2021-04-20 04:25:46,144 INFO] Step 30005/30020; acc:  19.55; ppl: 315.29; xent: 5.75; lr: 0.00051; 7993/10240 tok/s;      1 sec\n","[2021-04-20 04:25:46,332 INFO] Saving checkpoint bert_catechism/model_catechism_112/en-vi_step_30005.pt\n","[2021-04-20 04:25:51,053 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:51,064 INFO] number of examples: 99\n","[2021-04-20 04:25:51,318 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:51,323 INFO] number of examples: 99\n","[2021-04-20 04:25:51,565 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:51,569 INFO] number of examples: 99\n","[2021-04-20 04:25:51,808 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:51,813 INFO] number of examples: 99\n","[2021-04-20 04:25:52,048 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:52,053 INFO] number of examples: 99\n","[2021-04-20 04:25:52,292 INFO] Step 30010/30020; acc:  47.42; ppl: 19.77; xent: 2.98; lr: 0.00051; 1635/2094 tok/s;      7 sec\n","[2021-04-20 04:25:52,480 INFO] Saving checkpoint bert_catechism/model_catechism_112/en-vi_step_30010.pt\n","[2021-04-20 04:25:59,228 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:59,233 INFO] number of examples: 99\n","[2021-04-20 04:25:59,478 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:59,486 INFO] number of examples: 99\n","[2021-04-20 04:25:59,770 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:25:59,775 INFO] number of examples: 99\n","[2021-04-20 04:26:00,022 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:00,027 INFO] number of examples: 99\n","[2021-04-20 04:26:00,271 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:00,275 INFO] number of examples: 99\n","[2021-04-20 04:26:00,534 INFO] Step 30015/30020; acc:  75.29; ppl:  3.45; xent: 1.24; lr: 0.00051; 1219/1562 tok/s;     16 sec\n","[2021-04-20 04:26:00,745 INFO] Saving checkpoint bert_catechism/model_catechism_112/en-vi_step_30015.pt\n","[2021-04-20 04:26:06,760 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:11,615 INFO] number of examples: 99\n","[2021-04-20 04:26:11,888 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:11,893 INFO] number of examples: 99\n","[2021-04-20 04:26:12,162 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:12,168 INFO] number of examples: 99\n","[2021-04-20 04:26:12,443 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:12,450 INFO] number of examples: 99\n","[2021-04-20 04:26:12,718 INFO] Loading dataset from bert_catechism/output_catechism_112/en-vi.train.0.pt\n","[2021-04-20 04:26:12,723 INFO] number of examples: 99\n","[2021-04-20 04:26:12,997 INFO] Step 30020/30020; acc:  92.02; ppl:  1.53; xent: 0.42; lr: 0.00051; 806/1033 tok/s;     28 sec\n","[2021-04-20 04:26:13,232 INFO] Saving checkpoint bert_catechism/model_catechism_112/en-vi_step_30020.pt\n","[2021-04-20 04:26:38,985 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:26:56,932 INFO] PRED AVG SCORE: -1.2818, PRED PPL: 3.6033\n","[2021-04-20 04:26:56,933 INFO] GOLD AVG SCORE: -6.0598, GOLD PPL: 428.2877\n","BLEU = 1.94, 29.7/8.6/3.5/1.8 (BP=0.306, ratio=0.458, hyp_len=1057, ref_len=2308)\n"," ===05==^======= \n","[2021-04-20 04:27:00,776 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:27:22,848 INFO] PRED AVG SCORE: -0.8871, PRED PPL: 2.4281\n","[2021-04-20 04:27:22,848 INFO] GOLD AVG SCORE: -6.0815, GOLD PPL: 437.6951\n","BLEU = 3.92, 27.2/7.7/2.4/0.9 (BP=0.850, ratio=0.860, hyp_len=1985, ref_len=2308)\n"," ===10==^======= \n","[2021-04-20 04:27:26,609 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:27:45,468 INFO] PRED AVG SCORE: -0.9967, PRED PPL: 2.7092\n","[2021-04-20 04:27:45,468 INFO] GOLD AVG SCORE: -6.6201, GOLD PPL: 750.0410\n","BLEU = 2.21, 22.7/6.7/2.2/0.6 (BP=0.580, ratio=0.647, hyp_len=1494, ref_len=2308)\n"," ===15==^======= \n","[2021-04-20 04:27:49,215 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:28:09,221 INFO] PRED AVG SCORE: -0.7269, PRED PPL: 2.0686\n","[2021-04-20 04:28:09,221 INFO] GOLD AVG SCORE: -7.1265, GOLD PPL: 1244.5553\n","BLEU = 1.53, 16.1/3.1/1.0/0.3 (BP=0.775, ratio=0.797, hyp_len=1840, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618892919136,"user_tz":-420,"elapsed":29010,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"73d6b867-262d-418e-c9ea-92f6539612d8"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_catechism.en -tgt finetune/test_catechism.vi -output bert_catechism/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_catechism.vi < bert_catechism/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 04:28:14,298 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 04:28:38,251 INFO] PRED AVG SCORE: -1.0610, PRED PPL: 2.8893\n","[2021-04-20 04:28:38,251 INFO] GOLD AVG SCORE: -8.7200, GOLD PPL: 6124.4530\n","BLEU = 3.31, 22.2/5.5/1.8/0.6 (BP=0.973, ratio=0.973, hyp_len=2246, ref_len=2308)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}