{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-20210420-0301 SIF buddhism.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888952710,"user_tz":-420,"elapsed":34119,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"76d01db9-7cbb-444c-d07b-5bbd05de3f82"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888957973,"user_tz":-420,"elapsed":1684,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"348466b7-62af-4710-8e9e-dade10299194"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-20210420-0301\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888369728,"user_tz":-420,"elapsed":801,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"02eeb39a-0b26-46b1-be15-824372db85ce"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:12:49 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888974540,"user_tz":-420,"elapsed":15955,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"64173c85-a695-40f7-c6f7-836af2bbc61f"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 16.0MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n","\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 29.8MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=3df86a7ec8a0cb1dc946c1043ae4added89aa92d0a348352829ff43b8d5fc65e\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: torchtext, pyonmttok, configargparse, waitress, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618887841008,"user_tz":-420,"elapsed":1781,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2d9dcd2-26ed-49c2-a523-9ad4d474e26a"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","!tar -xvf 'SIF-finetune.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 03:03:59--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 97919 (96K) [application/octet-stream]\n","Saving to: ‘SIF-finetune.tar.gz’\n","\n","\rSIF-finetune.tar.gz   0%[                    ]       0  --.-KB/s               \rSIF-finetune.tar.gz 100%[===================>]  95.62K  --.-KB/s    in 0.008s  \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 03:03:59 (12.0 MB/s) - ‘SIF-finetune.tar.gz’ saved [97919/97919]\n","\n","finetune/\n","finetune/en_finetune_law_50\n","finetune/en_finetune_law_95\n","finetune/en_finetune_buddhism_50\n","finetune/en_finetune_buddhism_100\n","finetune/en_finetune_buddhism_150\n","finetune/en_finetune_buddhism_193\n","finetune/en_finetune_climate_50\n","finetune/en_finetune_climate_100\n","finetune/en_finetune_climate_120\n","finetune/en_finetune_catechism_50\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_150\n","finetune/en_finetune_catechism_152\n","finetune/vi_finetune_law_50\n","finetune/vi_finetune_law_95\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_buddhism_193\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_100\n","finetune/vi_finetune_climate_120\n","finetune/vi_finetune_catechism_50\n","finetune/vi_finetune_catechism_100\n","finetune/vi_finetune_catechism_150\n","finetune/vi_finetune_catechism_152\n","finetune/test_law.vi\n","finetune/test_buddhism.vi\n","finetune/test_climate.vi\n","finetune/test_catechism.vi\n","finetune/test_law.en\n","finetune/test_buddhism.en\n","finetune/test_climate.en\n","finetune/test_catechism.en\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618887850544,"user_tz":-420,"elapsed":872,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"38a22dd4-2b96-4702-b250-82d5a7368a6a"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 491\n","-rw------- 1 root root  9813 Apr 20 02:12 en_finetune_buddhism_100\n","-rw------- 1 root root 14812 Apr 20 02:12 en_finetune_buddhism_150\n","-rw------- 1 root root 19723 Apr 20 02:12 en_finetune_buddhism_193\n","-rw------- 1 root root  5246 Apr 20 02:12 en_finetune_buddhism_50\n","-rw------- 1 root root 13614 Apr 20 02:12 en_finetune_catechism_100\n","-rw------- 1 root root 20281 Apr 20 02:12 en_finetune_catechism_150\n","-rw------- 1 root root 20539 Apr 20 02:12 en_finetune_catechism_152\n","-rw------- 1 root root  7151 Apr 20 02:12 en_finetune_catechism_50\n","-rw------- 1 root root 13647 Apr 20 02:12 en_finetune_climate_100\n","-rw------- 1 root root 16493 Apr 20 02:12 en_finetune_climate_120\n","-rw------- 1 root root  6498 Apr 20 02:12 en_finetune_climate_50\n","-rw------- 1 root root  6664 Apr 20 02:12 en_finetune_law_50\n","-rw------- 1 root root 12797 Apr 20 02:12 en_finetune_law_95\n","-rw------- 1 root root  7711 Apr 20 02:12 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 02:12 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 02:12 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 02:12 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 02:12 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 02:12 test_climate.vi\n","-rw------- 1 root root 11267 Apr 20 02:12 test_law.en\n","-rw------- 1 root root 15336 Apr 20 02:12 test_law.vi\n","-rw------- 1 root root 13723 Apr 20 02:12 vi_finetune_buddhism_100\n","-rw------- 1 root root 21466 Apr 20 02:12 vi_finetune_buddhism_150\n","-rw------- 1 root root 28181 Apr 20 02:12 vi_finetune_buddhism_193\n","-rw------- 1 root root  7370 Apr 20 02:12 vi_finetune_buddhism_50\n","-rw------- 1 root root 18512 Apr 20 02:12 vi_finetune_catechism_100\n","-rw------- 1 root root 27677 Apr 20 02:12 vi_finetune_catechism_150\n","-rw------- 1 root root 28052 Apr 20 02:12 vi_finetune_catechism_152\n","-rw------- 1 root root  9740 Apr 20 02:12 vi_finetune_catechism_50\n","-rw------- 1 root root 18253 Apr 20 02:12 vi_finetune_climate_100\n","-rw------- 1 root root 22006 Apr 20 02:12 vi_finetune_climate_120\n","-rw------- 1 root root  8917 Apr 20 02:12 vi_finetune_climate_50\n","-rw------- 1 root root  8413 Apr 20 02:12 vi_finetune_law_50\n","-rw------- 1 root root 16118 Apr 20 02:12 vi_finetune_law_95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618887872728,"user_tz":-420,"elapsed":19319,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"dea9bd89-6e12-4441-eb1e-6a1eaeb208c4"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 24.10 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889164707,"user_tz":-420,"elapsed":167626,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"4851e0b4-9734-425c-ae0d-256db67dd736"},"source":["!mkdir -p sif_buddhism/output_buddhism_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_50' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_50' \\\\\n","-save_data 'sif_buddhism/output_buddhism_50/en-vi' \n","\n","!mkdir -p sif_buddhism/model_buddhism_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_buddhism/output_buddhism_50/en-vi' \\\\\n","-save_model 'sif_buddhism/model_buddhism_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_buddhism/model_buddhism_50/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_50/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_50/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_50/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":4,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:23:21,319 INFO] Extracting features...\n","[2021-04-20 03:23:22,107 INFO]  * number of source features: 0.\n","[2021-04-20 03:23:22,107 INFO]  * number of target features: 0.\n","[2021-04-20 03:23:22,107 INFO] Building `Fields` object...\n","[2021-04-20 03:23:22,107 INFO] Building & saving training data...\n","[2021-04-20 03:23:22,118 INFO] Building shard 0.\n","[2021-04-20 03:23:22,120 INFO]  * saving 0th train data shard to sif_buddhism/output_buddhism_50/en-vi.train.0.pt.\n","[2021-04-20 03:23:22,217 INFO]  * tgt vocab size: 468.\n","[2021-04-20 03:23:22,218 INFO]  * src vocab size: 387.\n","[2021-04-20 03:23:47,587 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:23:48,823 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:23:48,826 INFO]  * src vocab size = 39660\n","[2021-04-20 03:23:48,826 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:23:48,826 INFO] Building model...\n","[2021-04-20 03:23:56,229 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:23:56,251 INFO] encoder: 39221248\n","[2021-04-20 03:23:56,251 INFO] decoder: 43931466\n","[2021-04-20 03:23:56,251 INFO] * number of parameters: 83152714\n","[2021-04-20 03:23:56,794 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:23:56,794 INFO] Start training loop without validation...\n","[2021-04-20 03:23:56,794 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:56,796 INFO] number of examples: 44\n","[2021-04-20 03:23:56,804 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:56,807 INFO] number of examples: 44\n","[2021-04-20 03:23:57,090 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,093 INFO] number of examples: 44\n","[2021-04-20 03:23:57,095 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,097 INFO] number of examples: 44\n","[2021-04-20 03:23:57,300 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,302 INFO] number of examples: 44\n","[2021-04-20 03:23:57,305 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,307 INFO] number of examples: 44\n","[2021-04-20 03:23:57,505 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,508 INFO] number of examples: 44\n","[2021-04-20 03:23:57,510 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,512 INFO] number of examples: 44\n","[2021-04-20 03:23:57,709 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,711 INFO] number of examples: 44\n","[2021-04-20 03:23:57,714 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:23:57,716 INFO] number of examples: 44\n","[2021-04-20 03:23:57,914 INFO] Step 30005/30020; acc:  41.12; ppl: 66.50; xent: 4.20; lr: 0.00051; 5553/7883 tok/s;      1 sec\n","[2021-04-20 03:23:58,088 INFO] Saving checkpoint sif_buddhism/model_buddhism_50/en-vi_step_30005.pt\n","[2021-04-20 03:24:02,202 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,204 INFO] number of examples: 44\n","[2021-04-20 03:24:02,207 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,209 INFO] number of examples: 44\n","[2021-04-20 03:24:02,424 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,427 INFO] number of examples: 44\n","[2021-04-20 03:24:02,430 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,432 INFO] number of examples: 44\n","[2021-04-20 03:24:02,636 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,639 INFO] number of examples: 44\n","[2021-04-20 03:24:02,641 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,644 INFO] number of examples: 44\n","[2021-04-20 03:24:02,844 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,847 INFO] number of examples: 44\n","[2021-04-20 03:24:02,850 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:02,852 INFO] number of examples: 44\n","[2021-04-20 03:24:03,051 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:03,054 INFO] number of examples: 44\n","[2021-04-20 03:24:03,056 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:03,058 INFO] number of examples: 44\n","[2021-04-20 03:24:03,256 INFO] Step 30010/30020; acc:  79.48; ppl:  4.21; xent: 1.44; lr: 0.00051; 1164/1653 tok/s;      6 sec\n","[2021-04-20 03:24:03,443 INFO] Saving checkpoint sif_buddhism/model_buddhism_50/en-vi_step_30010.pt\n","[2021-04-20 03:24:07,650 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:07,652 INFO] number of examples: 44\n","[2021-04-20 03:24:07,655 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:07,657 INFO] number of examples: 44\n","[2021-04-20 03:24:07,855 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:07,858 INFO] number of examples: 44\n","[2021-04-20 03:24:07,860 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:07,862 INFO] number of examples: 44\n","[2021-04-20 03:24:08,062 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:08,065 INFO] number of examples: 44\n","[2021-04-20 03:24:08,067 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:08,070 INFO] number of examples: 44\n","[2021-04-20 03:24:08,273 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:08,276 INFO] number of examples: 44\n","[2021-04-20 03:24:08,279 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:08,281 INFO] number of examples: 44\n","[2021-04-20 03:24:08,480 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:08,483 INFO] number of examples: 44\n","[2021-04-20 03:24:08,487 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:08,489 INFO] number of examples: 44\n","[2021-04-20 03:24:08,692 INFO] Step 30015/30020; acc:  91.33; ppl:  1.75; xent: 0.56; lr: 0.00051; 1144/1624 tok/s;     12 sec\n","[2021-04-20 03:24:08,878 INFO] Saving checkpoint sif_buddhism/model_buddhism_50/en-vi_step_30015.pt\n","[2021-04-20 03:24:19,415 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:19,418 INFO] number of examples: 44\n","[2021-04-20 03:24:19,422 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:19,425 INFO] number of examples: 44\n","[2021-04-20 03:24:19,637 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:19,639 INFO] number of examples: 44\n","[2021-04-20 03:24:19,642 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:19,645 INFO] number of examples: 44\n","[2021-04-20 03:24:19,858 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:19,862 INFO] number of examples: 44\n","[2021-04-20 03:24:19,865 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:19,867 INFO] number of examples: 44\n","[2021-04-20 03:24:20,083 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:20,086 INFO] number of examples: 44\n","[2021-04-20 03:24:20,089 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:20,091 INFO] number of examples: 44\n","[2021-04-20 03:24:20,301 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:20,303 INFO] number of examples: 44\n","[2021-04-20 03:24:20,305 INFO] Loading dataset from sif_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:24:20,307 INFO] number of examples: 44\n","[2021-04-20 03:24:20,515 INFO] Step 30020/30020; acc:  98.44; ppl:  1.20; xent: 0.18; lr: 0.00051; 526/747 tok/s;     24 sec\n","[2021-04-20 03:24:20,754 INFO] Saving checkpoint sif_buddhism/model_buddhism_50/en-vi_step_30020.pt\n","[2021-04-20 03:24:36,691 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:25:04,000 INFO] PRED AVG SCORE: -0.8496, PRED PPL: 2.3386\n","[2021-04-20 03:25:04,000 INFO] GOLD AVG SCORE: -5.4316, GOLD PPL: 228.5115\n","BLEU = 10.02, 33.3/14.0/6.7/3.4 (BP=0.990, ratio=0.990, hyp_len=1871, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:25:09,825 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:25:31,472 INFO] PRED AVG SCORE: -1.2385, PRED PPL: 3.4503\n","[2021-04-20 03:25:31,472 INFO] GOLD AVG SCORE: -6.5358, GOLD PPL: 689.4189\n","BLEU = 6.84, 26.9/11.0/5.6/2.7 (BP=0.837, ratio=0.849, hyp_len=1604, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:25:35,041 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:25:48,315 INFO] PRED AVG SCORE: -0.6450, PRED PPL: 1.9060\n","[2021-04-20 03:25:48,315 INFO] GOLD AVG SCORE: -7.2413, GOLD PPL: 1395.9431\n","BLEU = 1.40, 25.8/11.2/6.2/3.8 (BP=0.154, ratio=0.348, hyp_len=658, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:25:52,007 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:26:03,954 INFO] PRED AVG SCORE: -1.0222, PRED PPL: 2.7794\n","[2021-04-20 03:26:03,954 INFO] GOLD AVG SCORE: -6.0467, GOLD PPL: 422.7063\n","BLEU = 1.30, 29.9/13.5/7.3/3.9 (BP=0.126, ratio=0.326, hyp_len=615, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XBsEa57JRgsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889310285,"user_tz":-420,"elapsed":143838,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"3a63222d-49fb-4ab6-d190-c60a973261f4"},"source":["!mkdir -p sif_buddhism/output_buddhism_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_100' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_100' \\\\\n","-save_data 'sif_buddhism/output_buddhism_100/en-vi' \n","\n","!mkdir -p sif_buddhism/model_buddhism_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_buddhism/output_buddhism_100/en-vi' \\\\\n","-save_model 'sif_buddhism/model_buddhism_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_buddhism/model_buddhism_100/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_100/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_100/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_100/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":5,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:26:07,915 INFO] Extracting features...\n","[2021-04-20 03:26:08,868 INFO]  * number of source features: 0.\n","[2021-04-20 03:26:08,868 INFO]  * number of target features: 0.\n","[2021-04-20 03:26:08,869 INFO] Building `Fields` object...\n","[2021-04-20 03:26:08,869 INFO] Building & saving training data...\n","[2021-04-20 03:26:08,880 INFO] Building shard 0.\n","[2021-04-20 03:26:08,884 INFO]  * saving 0th train data shard to sif_buddhism/output_buddhism_100/en-vi.train.0.pt.\n","[2021-04-20 03:26:08,981 INFO]  * tgt vocab size: 847.\n","[2021-04-20 03:26:08,982 INFO]  * src vocab size: 730.\n","[2021-04-20 03:26:11,579 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:26:12,875 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:26:12,880 INFO]  * src vocab size = 39660\n","[2021-04-20 03:26:12,880 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:26:12,880 INFO] Building model...\n","[2021-04-20 03:26:16,903 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:26:16,939 INFO] encoder: 39221248\n","[2021-04-20 03:26:16,939 INFO] decoder: 43931466\n","[2021-04-20 03:26:16,939 INFO] * number of parameters: 83152714\n","[2021-04-20 03:26:17,481 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:26:17,481 INFO] Start training loop without validation...\n","[2021-04-20 03:26:17,481 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:17,484 INFO] number of examples: 93\n","[2021-04-20 03:26:17,732 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:17,735 INFO] number of examples: 93\n","[2021-04-20 03:26:17,959 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:17,964 INFO] number of examples: 93\n","[2021-04-20 03:26:18,186 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:18,190 INFO] number of examples: 93\n","[2021-04-20 03:26:18,408 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:18,411 INFO] number of examples: 93\n","[2021-04-20 03:26:18,629 INFO] Step 30005/30020; acc:  33.76; ppl: 112.71; xent: 4.72; lr: 0.00051; 5863/8411 tok/s;      1 sec\n","[2021-04-20 03:26:18,805 INFO] Saving checkpoint sif_buddhism/model_buddhism_100/en-vi_step_30005.pt\n","[2021-04-20 03:26:23,682 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:23,686 INFO] number of examples: 93\n","[2021-04-20 03:26:23,924 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:23,928 INFO] number of examples: 93\n","[2021-04-20 03:26:24,161 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:24,165 INFO] number of examples: 93\n","[2021-04-20 03:26:24,387 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:24,390 INFO] number of examples: 93\n","[2021-04-20 03:26:24,611 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:24,614 INFO] number of examples: 93\n","[2021-04-20 03:26:24,835 INFO] Step 30010/30020; acc:  66.03; ppl:  8.48; xent: 2.14; lr: 0.00051; 1084/1556 tok/s;      7 sec\n","[2021-04-20 03:26:25,017 INFO] Saving checkpoint sif_buddhism/model_buddhism_100/en-vi_step_30010.pt\n","[2021-04-20 03:26:31,126 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:31,138 INFO] number of examples: 93\n","[2021-04-20 03:26:31,361 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:31,364 INFO] number of examples: 93\n","[2021-04-20 03:26:31,607 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:31,611 INFO] number of examples: 93\n","[2021-04-20 03:26:31,848 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:31,852 INFO] number of examples: 93\n","[2021-04-20 03:26:32,085 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:32,089 INFO] number of examples: 93\n","[2021-04-20 03:26:32,314 INFO] Step 30015/30020; acc:  85.44; ppl:  2.35; xent: 0.85; lr: 0.00051; 900/1291 tok/s;     15 sec\n","[2021-04-20 03:26:32,508 INFO] Saving checkpoint sif_buddhism/model_buddhism_100/en-vi_step_30015.pt\n","[2021-04-20 03:26:38,978 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:44,243 INFO] number of examples: 93\n","[2021-04-20 03:26:44,491 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:44,495 INFO] number of examples: 93\n","[2021-04-20 03:26:44,739 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:44,743 INFO] number of examples: 93\n","[2021-04-20 03:26:44,995 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:45,000 INFO] number of examples: 93\n","[2021-04-20 03:26:45,259 INFO] Loading dataset from sif_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:26:45,263 INFO] number of examples: 93\n","[2021-04-20 03:26:45,524 INFO] Step 30020/30020; acc:  95.85; ppl:  1.32; xent: 0.28; lr: 0.00051; 509/731 tok/s;     28 sec\n","[2021-04-20 03:26:45,764 INFO] Saving checkpoint sif_buddhism/model_buddhism_100/en-vi_step_30020.pt\n","[2021-04-20 03:27:00,435 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:27:26,358 INFO] PRED AVG SCORE: -0.8464, PRED PPL: 2.3313\n","[2021-04-20 03:27:26,358 INFO] GOLD AVG SCORE: -5.0034, GOLD PPL: 148.9139\n","BLEU = 11.18, 35.1/16.1/7.3/3.8 (BP=1.000, ratio=1.024, hyp_len=1935, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:27:30,025 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:27:53,965 INFO] PRED AVG SCORE: -0.8465, PRED PPL: 2.3315\n","[2021-04-20 03:27:53,965 INFO] GOLD AVG SCORE: -5.3656, GOLD PPL: 213.9184\n","BLEU = 10.73, 32.2/14.6/7.5/3.8 (BP=1.000, ratio=1.039, hyp_len=1962, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:27:57,602 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:28:12,632 INFO] PRED AVG SCORE: -0.7291, PRED PPL: 2.0733\n","[2021-04-20 03:28:12,632 INFO] GOLD AVG SCORE: -5.8860, GOLD PPL: 359.9653\n","BLEU = 4.22, 28.7/12.4/6.3/3.2 (BP=0.461, ratio=0.563, hyp_len=1064, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:28:16,312 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:28:29,441 INFO] PRED AVG SCORE: -0.7346, PRED PPL: 2.0846\n","[2021-04-20 03:28:29,441 INFO] GOLD AVG SCORE: -5.9585, GOLD PPL: 387.0189\n","BLEU = 4.23, 30.7/14.4/7.6/3.7 (BP=0.399, ratio=0.521, hyp_len=984, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMljWicBRswk","executionInfo":{"status":"ok","timestamp":1618889475339,"user_tz":-420,"elapsed":308875,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"bb1ca82e-c0ff-488f-aec0-8c98d355ba0e"},"source":["!mkdir -p sif_buddhism/output_buddhism_150\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_150' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_150' \\\\\n","-save_data 'sif_buddhism/output_buddhism_150/en-vi' \n","\n","!mkdir -p sif_buddhism/model_buddhism_150\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_buddhism/output_buddhism_150/en-vi' \\\\\n","-save_model 'sif_buddhism/model_buddhism_150/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_buddhism/model_buddhism_150/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_150/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_150/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_150/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_150/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_150/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_150/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_150/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_150/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_150/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_150/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_150/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:28:31,041 INFO] Extracting features...\n","[2021-04-20 03:28:31,831 INFO]  * number of source features: 0.\n","[2021-04-20 03:28:31,831 INFO]  * number of target features: 0.\n","[2021-04-20 03:28:31,831 INFO] Building `Fields` object...\n","[2021-04-20 03:28:31,831 INFO] Building & saving training data...\n","[2021-04-20 03:28:31,844 INFO] Building shard 0.\n","[2021-04-20 03:28:31,850 INFO]  * saving 0th train data shard to sif_buddhism/output_buddhism_150/en-vi.train.0.pt.\n","[2021-04-20 03:28:31,946 INFO]  * tgt vocab size: 1120.\n","[2021-04-20 03:28:31,947 INFO]  * src vocab size: 1029.\n","[2021-04-20 03:28:34,556 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:28:35,857 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:28:35,862 INFO]  * src vocab size = 39660\n","[2021-04-20 03:28:35,862 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:28:35,862 INFO] Building model...\n","[2021-04-20 03:28:40,013 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:28:40,031 INFO] encoder: 39221248\n","[2021-04-20 03:28:40,031 INFO] decoder: 43931466\n","[2021-04-20 03:28:40,031 INFO] * number of parameters: 83152714\n","[2021-04-20 03:28:40,629 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:28:40,629 INFO] Start training loop without validation...\n","[2021-04-20 03:28:40,629 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:40,639 INFO] number of examples: 140\n","[2021-04-20 03:28:40,882 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:40,887 INFO] number of examples: 140\n","[2021-04-20 03:28:41,123 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:41,128 INFO] number of examples: 140\n","[2021-04-20 03:28:41,370 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:41,375 INFO] number of examples: 140\n","[2021-04-20 03:28:41,609 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:41,614 INFO] number of examples: 140\n","[2021-04-20 03:28:41,850 INFO] Step 30005/30020; acc:  31.91; ppl: 120.43; xent: 4.79; lr: 0.00051; 8602/12551 tok/s;      1 sec\n","[2021-04-20 03:28:42,029 INFO] Saving checkpoint sif_buddhism/model_buddhism_150/en-vi_step_30005.pt\n","[2021-04-20 03:28:46,985 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:46,990 INFO] number of examples: 140\n","[2021-04-20 03:28:47,252 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:47,258 INFO] number of examples: 140\n","[2021-04-20 03:28:47,503 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:47,508 INFO] number of examples: 140\n","[2021-04-20 03:28:47,746 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:47,752 INFO] number of examples: 140\n","[2021-04-20 03:28:47,992 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:47,998 INFO] number of examples: 140\n","[2021-04-20 03:28:48,238 INFO] Step 30010/30020; acc:  59.50; ppl: 11.77; xent: 2.47; lr: 0.00051; 1644/2398 tok/s;      8 sec\n","[2021-04-20 03:28:48,417 INFO] Saving checkpoint sif_buddhism/model_buddhism_150/en-vi_step_30010.pt\n","[2021-04-20 03:28:53,863 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:53,871 INFO] number of examples: 140\n","[2021-04-20 03:28:54,110 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:54,115 INFO] number of examples: 140\n","[2021-04-20 03:28:54,369 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:54,375 INFO] number of examples: 140\n","[2021-04-20 03:28:54,623 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:54,630 INFO] number of examples: 140\n","[2021-04-20 03:28:54,873 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:28:54,879 INFO] number of examples: 140\n","[2021-04-20 03:28:55,129 INFO] Step 30015/30020; acc:  80.59; ppl:  3.05; xent: 1.12; lr: 0.00051; 1524/2223 tok/s;     14 sec\n","[2021-04-20 03:28:55,320 INFO] Saving checkpoint sif_buddhism/model_buddhism_150/en-vi_step_30015.pt\n","[2021-04-20 03:29:02,286 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:29:02,773 INFO] number of examples: 140\n","[2021-04-20 03:29:03,041 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:29:03,047 INFO] number of examples: 140\n","[2021-04-20 03:29:03,320 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:29:03,328 INFO] number of examples: 140\n","[2021-04-20 03:29:03,600 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:29:03,606 INFO] number of examples: 140\n","[2021-04-20 03:29:03,878 INFO] Loading dataset from sif_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:29:03,886 INFO] number of examples: 140\n","[2021-04-20 03:29:04,166 INFO] Step 30020/30020; acc:  93.93; ppl:  1.41; xent: 0.34; lr: 0.00051; 1162/1695 tok/s;     24 sec\n","[2021-04-20 03:29:04,416 INFO] Saving checkpoint sif_buddhism/model_buddhism_150/en-vi_step_30020.pt\n","[2021-04-20 03:29:34,964 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:30:07,883 INFO] PRED AVG SCORE: -0.9321, PRED PPL: 2.5398\n","[2021-04-20 03:30:07,883 INFO] GOLD AVG SCORE: -4.8860, GOLD PPL: 132.4230\n","BLEU = 11.60, 33.3/15.5/8.0/4.3 (BP=1.000, ratio=1.125, hyp_len=2126, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:30:11,712 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:30:35,491 INFO] PRED AVG SCORE: -0.8265, PRED PPL: 2.2853\n","[2021-04-20 03:30:35,491 INFO] GOLD AVG SCORE: -4.9540, GOLD PPL: 141.7338\n","BLEU = 12.04, 34.3/16.2/8.6/4.4 (BP=1.000, ratio=1.075, hyp_len=2030, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:30:39,219 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:30:55,297 INFO] PRED AVG SCORE: -0.8897, PRED PPL: 2.4345\n","[2021-04-20 03:30:55,297 INFO] GOLD AVG SCORE: -5.4464, GOLD PPL: 231.9137\n","BLEU = 6.46, 34.1/14.8/6.9/3.5 (BP=0.615, ratio=0.673, hyp_len=1271, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:30:59,067 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:31:14,546 INFO] PRED AVG SCORE: -0.6421, PRED PPL: 1.9004\n","[2021-04-20 03:31:14,546 INFO] GOLD AVG SCORE: -5.4830, GOLD PPL: 240.5741\n","BLEU = 7.37, 35.0/16.3/7.9/3.5 (BP=0.658, ratio=0.705, hyp_len=1331, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2ffz_YpTl3i","executionInfo":{"status":"ok","timestamp":1618889642664,"user_tz":-420,"elapsed":165811,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"7a656c54-782c-47c2-8aba-8f2e0938b26e"},"source":["!mkdir -p sif_buddhism/output_buddhism_193\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_193' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_193' \\\\\n","-save_data 'sif_buddhism/output_buddhism_193/en-vi' \n","\n","!mkdir -p sif_buddhism/model_buddhism_193\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'sif_buddhism/output_buddhism_193/en-vi' \\\\\n","-save_model 'sif_buddhism/model_buddhism_193/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model sif_buddhism/model_buddhism_193/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_193/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_193/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_193/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_193/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_193/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_193/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_193/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_193/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model sif_buddhism/model_buddhism_193/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/model_buddhism_193/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/model_buddhism_193/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:31:18,384 INFO] Extracting features...\n","[2021-04-20 03:31:19,306 INFO]  * number of source features: 0.\n","[2021-04-20 03:31:19,306 INFO]  * number of target features: 0.\n","[2021-04-20 03:31:19,306 INFO] Building `Fields` object...\n","[2021-04-20 03:31:19,307 INFO] Building & saving training data...\n","[2021-04-20 03:31:19,319 INFO] Building shard 0.\n","[2021-04-20 03:31:19,326 INFO]  * saving 0th train data shard to sif_buddhism/output_buddhism_193/en-vi.train.0.pt.\n","[2021-04-20 03:31:19,420 INFO]  * tgt vocab size: 1343.\n","[2021-04-20 03:31:19,422 INFO]  * src vocab size: 1300.\n","[2021-04-20 03:31:22,121 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:31:23,461 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:31:23,468 INFO]  * src vocab size = 39660\n","[2021-04-20 03:31:23,468 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:31:23,468 INFO] Building model...\n","[2021-04-20 03:31:27,815 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:31:27,844 INFO] encoder: 39221248\n","[2021-04-20 03:31:27,844 INFO] decoder: 43931466\n","[2021-04-20 03:31:27,845 INFO] * number of parameters: 83152714\n","[2021-04-20 03:31:28,421 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:31:28,421 INFO] Start training loop without validation...\n","[2021-04-20 03:31:28,421 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:28,431 INFO] number of examples: 181\n","[2021-04-20 03:31:28,760 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:28,767 INFO] number of examples: 181\n","[2021-04-20 03:31:29,065 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:29,071 INFO] number of examples: 181\n","[2021-04-20 03:31:29,365 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:29,371 INFO] number of examples: 181\n","[2021-04-20 03:31:29,668 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:29,674 INFO] number of examples: 181\n","[2021-04-20 03:31:29,973 INFO] Step 30005/30020; acc:  31.13; ppl: 128.45; xent: 4.86; lr: 0.00051; 9165/13121 tok/s;      2 sec\n","[2021-04-20 03:31:30,150 INFO] Saving checkpoint sif_buddhism/model_buddhism_193/en-vi_step_30005.pt\n","[2021-04-20 03:31:34,437 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:34,444 INFO] number of examples: 181\n","[2021-04-20 03:31:34,752 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:34,759 INFO] number of examples: 181\n","[2021-04-20 03:31:35,057 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:35,063 INFO] number of examples: 181\n","[2021-04-20 03:31:35,362 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:35,370 INFO] number of examples: 181\n","[2021-04-20 03:31:35,667 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:35,673 INFO] number of examples: 181\n","[2021-04-20 03:31:35,970 INFO] Step 30010/30020; acc:  56.14; ppl: 14.28; xent: 2.66; lr: 0.00051; 2372/3396 tok/s;      8 sec\n","[2021-04-20 03:31:36,155 INFO] Saving checkpoint sif_buddhism/model_buddhism_193/en-vi_step_30010.pt\n","[2021-04-20 03:31:42,886 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:42,896 INFO] number of examples: 181\n","[2021-04-20 03:31:43,200 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:43,207 INFO] number of examples: 181\n","[2021-04-20 03:31:43,512 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:43,521 INFO] number of examples: 181\n","[2021-04-20 03:31:43,825 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:43,832 INFO] number of examples: 181\n","[2021-04-20 03:31:44,136 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:44,143 INFO] number of examples: 181\n","[2021-04-20 03:31:44,447 INFO] Step 30015/30020; acc:  77.02; ppl:  3.63; xent: 1.29; lr: 0.00051; 1678/2402 tok/s;     16 sec\n","[2021-04-20 03:31:44,651 INFO] Saving checkpoint sif_buddhism/model_buddhism_193/en-vi_step_30015.pt\n","[2021-04-20 03:31:56,050 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:56,057 INFO] number of examples: 181\n","[2021-04-20 03:31:56,370 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:56,378 INFO] number of examples: 181\n","[2021-04-20 03:31:56,687 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:56,714 INFO] number of examples: 181\n","[2021-04-20 03:31:57,025 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:57,033 INFO] number of examples: 181\n","[2021-04-20 03:31:57,343 INFO] Loading dataset from sif_buddhism/output_buddhism_193/en-vi.train.0.pt\n","[2021-04-20 03:31:57,350 INFO] number of examples: 181\n","[2021-04-20 03:31:57,660 INFO] Step 30020/30020; acc:  91.78; ppl:  1.55; xent: 0.44; lr: 0.00051; 1077/1541 tok/s;     29 sec\n","[2021-04-20 03:31:57,907 INFO] Saving checkpoint sif_buddhism/model_buddhism_193/en-vi_step_30020.pt\n","[2021-04-20 03:32:28,521 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:32:53,853 INFO] PRED AVG SCORE: -0.9178, PRED PPL: 2.5037\n","[2021-04-20 03:32:53,853 INFO] GOLD AVG SCORE: -4.8063, GOLD PPL: 122.2779\n","BLEU = 11.94, 36.6/16.9/8.1/4.1 (BP=1.000, ratio=1.052, hyp_len=1987, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:32:59,869 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:33:21,160 INFO] PRED AVG SCORE: -0.8163, PRED PPL: 2.2621\n","[2021-04-20 03:33:21,160 INFO] GOLD AVG SCORE: -4.7527, GOLD PPL: 115.8932\n","BLEU = 12.94, 35.9/17.3/9.3/4.8 (BP=1.000, ratio=1.001, hyp_len=1890, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:33:26,776 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:33:42,916 INFO] PRED AVG SCORE: -0.9321, PRED PPL: 2.5399\n","[2021-04-20 03:33:42,916 INFO] GOLD AVG SCORE: -5.1557, GOLD PPL: 173.4172\n","BLEU = 8.19, 35.7/16.1/8.2/4.4 (BP=0.681, ratio=0.722, hyp_len=1364, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:33:46,618 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:34:01,712 INFO] PRED AVG SCORE: -0.6197, PRED PPL: 1.8583\n","[2021-04-20 03:34:01,712 INFO] GOLD AVG SCORE: -5.2709, GOLD PPL: 194.5817\n","BLEU = 8.66, 37.1/17.2/9.4/5.0 (BP=0.657, ratio=0.704, hyp_len=1330, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889678410,"user_tz":-420,"elapsed":35737,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"4ecbf228-db60-44d6-b15f-517b09485a86"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output sif_buddhism/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < sif_buddhism/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:34:19,459 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:34:37,499 INFO] PRED AVG SCORE: -0.9317, PRED PPL: 2.5388\n","[2021-04-20 03:34:37,499 INFO] GOLD AVG SCORE: -7.0192, GOLD PPL: 1117.8830\n","BLEU = 11.03, 33.3/14.6/7.8/4.5 (BP=0.965, ratio=0.966, hyp_len=1824, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}