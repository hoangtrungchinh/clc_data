{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-BERT20210420-0327 BERT law.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902174036,"user_tz":-420,"elapsed":21511,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"62314ebc-0731-484a-8054-de6435c1b9fb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902185217,"user_tz":-420,"elapsed":3402,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"52f46062-f4f2-46c0-efbd-ec532ad5d7fe"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-BERT\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327'\n","os.chdir(path)\n","!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618887722822,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ad093d9c-5838-4dac-d5b5-e7682b1da442"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:02:02 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902204779,"user_tz":-420,"elapsed":14690,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"4d3aeeae-867c-4212-e4e1-85c5ce0cd288"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 8.3MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 19.8MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=d2555c89e2affe85274dd16916f3161b57dbc6b7b8d05271c9e42e6f2ede73cf\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 63.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAscbACoOygX","executionInfo":{"status":"ok","timestamp":1618902242191,"user_tz":-420,"elapsed":1094,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"5ea73f3b-bcd2-412e-abdb-ba3fcf7dcf7e"},"source":["!rm -rf SIF-finetune-bert.tar.gz\n","!rm -rf finetune\n","!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 996620\n","drwx------ 2 root root       4096 Apr 20 03:40 bert_buddhism\n","drwx------ 2 root root       4096 Apr 20 04:20 bert_catechism\n","drwx------ 2 root root       4096 Apr 20 03:36 bert_climate\n","drwx------ 2 root root       4096 Apr 20 03:31 bert_law\n","-rw------- 1 root root 1020517439 Apr 20 03:29 en-vi_step_30000.pt\n","drwx------ 2 root root       4096 Apr 20 03:30 OpenNMT-py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902256496,"user_tz":-420,"elapsed":1733,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"71417afa-c6c1-4df9-cbd5-11c061532a63"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","!tar -xvf 'SIF-finetune-bert.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 07:04:15--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 179427 (175K) [application/octet-stream]\n","Saving to: ‘SIF-finetune-bert.tar.gz’\n","\n","SIF-finetune-bert.t 100%[===================>] 175.22K  --.-KB/s    in 0.02s   \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 07:04:15 (10.8 MB/s) - ‘SIF-finetune-bert.tar.gz’ saved [179427/179427]\n","\n","finetune/\n","finetune/en_finetune_climate_150\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_150\n","finetune/en_finetune_climate_100\n","finetune/vi_finetune_buddhism_162\n","finetune/en_finetune_law_50\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/test_climate.vi\n","finetune/en_finetune_climate_191\n","finetune/test_catechism.vi\n","finetune/vi_finetune_law_50\n","finetune/en_finetune_law_100\n","finetune/test_law.en\n","finetune/en_finetune_buddhism_162\n","finetune/vi_finetune_catechism_50\n","finetune/en_finetune_climate_50\n","finetune/vi_finetune_catechism_100\n","finetune/en_finetune_buddhism_100\n","finetune/test_climate.en\n","finetune/en_finetune_catechism_112\n","finetune/vi_finetune_law_100\n","finetune/test_law.vi\n","finetune/en_finetune_buddhism_150\n","finetune/vi_finetune_catechism_112\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_climate_191\n","finetune/en_finetune_law_117\n","finetune/en_finetune_buddhism_50\n","finetune/test_catechism.en\n","finetune/test_buddhism.vi\n","finetune/test_buddhism.en\n","finetune/vi_finetune_climate_100\n","finetune/en_finetune_catechism_100\n","finetune/vi_finetune_law_117\n","finetune/en_finetune_catechism_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618902264572,"user_tz":-420,"elapsed":1350,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ba1ab13d-9983-44b9-8e78-199b855d66fa"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 525\n","-rw------- 1 root root  9453 Apr 20 06:46 en_finetune_buddhism_100\n","-rw------- 1 root root 14772 Apr 20 06:46 en_finetune_buddhism_150\n","-rw------- 1 root root 16471 Apr 20 06:46 en_finetune_buddhism_162\n","-rw------- 1 root root  4892 Apr 20 06:46 en_finetune_buddhism_50\n","-rw------- 1 root root 13295 Apr 20 06:46 en_finetune_catechism_100\n","-rw------- 1 root root 14691 Apr 20 06:46 en_finetune_catechism_112\n","-rw------- 1 root root  6722 Apr 20 06:46 en_finetune_catechism_50\n","-rw------- 1 root root 13996 Apr 20 06:46 en_finetune_climate_100\n","-rw------- 1 root root 21088 Apr 20 06:46 en_finetune_climate_150\n","-rw------- 1 root root 27050 Apr 20 06:46 en_finetune_climate_191\n","-rw------- 1 root root  7039 Apr 20 06:46 en_finetune_climate_50\n","-rw------- 1 root root 13382 Apr 20 06:46 en_finetune_law_100\n","-rw------- 1 root root 14565 Apr 20 06:46 en_finetune_law_117\n","-rw------- 1 root root  6457 Apr 20 06:46 en_finetune_law_50\n","-rw------- 1 root root  7711 Apr 20 06:46 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 06:46 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 06:46 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 06:46 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 06:46 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 06:46 test_climate.vi\n","-rw------- 1 root root 11306 Apr 20 06:46 test_law.en\n","-rw------- 1 root root 13801 Apr 20 06:46 test_law.vi\n","-rw------- 1 root root 13704 Apr 20 06:46 vi_finetune_buddhism_100\n","-rw------- 1 root root 21690 Apr 20 06:46 vi_finetune_buddhism_150\n","-rw------- 1 root root 24138 Apr 20 06:46 vi_finetune_buddhism_162\n","-rw------- 1 root root  7050 Apr 20 06:46 vi_finetune_buddhism_50\n","-rw------- 1 root root 18233 Apr 20 06:46 vi_finetune_catechism_100\n","-rw------- 1 root root 20199 Apr 20 06:46 vi_finetune_catechism_112\n","-rw------- 1 root root  8920 Apr 20 06:46 vi_finetune_catechism_50\n","-rw------- 1 root root 18493 Apr 20 06:46 vi_finetune_climate_100\n","-rw------- 1 root root 27537 Apr 20 06:46 vi_finetune_climate_150\n","-rw------- 1 root root 35637 Apr 20 06:46 vi_finetune_climate_191\n","-rw------- 1 root root  9330 Apr 20 06:46 vi_finetune_climate_50\n","-rw------- 1 root root 16722 Apr 20 06:46 vi_finetune_law_100\n","-rw------- 1 root root 18177 Apr 20 06:46 vi_finetune_law_117\n","-rw------- 1 root root  7993 Apr 20 06:46 vi_finetune_law_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618889473587,"user_tz":-420,"elapsed":19250,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2c0ec67-ded8-4c4b-e9c3-760c282cf56c"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 23.94 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902486923,"user_tz":-420,"elapsed":181336,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"3cf1b9df-e0b1-4976-e4b3-af4785321416"},"source":["!mkdir -p bert_law/output_law_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_law_50' \\\\\n","-train_tgt 'finetune/vi_finetune_law_50' \\\\\n","-save_data 'bert_law/output_law_50/en-vi' \n","\n","!mkdir -p bert_law/model_law_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_law/output_law_50/en-vi' \\\\\n","-save_model 'bert_law/model_law_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_law/model_law_50/en-vi_step_30005.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_law/model_law_50/en-vi_step_30010.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_law/model_law_50/en-vi_step_30015.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_law/model_law_50/en-vi_step_30020.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:05:09,043 INFO] Extracting features...\n","[2021-04-20 07:05:09,045 INFO]  * number of source features: 0.\n","[2021-04-20 07:05:09,045 INFO]  * number of target features: 0.\n","[2021-04-20 07:05:09,045 INFO] Building `Fields` object...\n","[2021-04-20 07:05:09,045 INFO] Building & saving training data...\n","[2021-04-20 07:05:09,054 INFO] Building shard 0.\n","[2021-04-20 07:05:09,057 INFO]  * saving 0th train data shard to bert_law/output_law_50/en-vi.train.0.pt.\n","[2021-04-20 07:05:09,154 INFO]  * tgt vocab size: 364.\n","[2021-04-20 07:05:09,154 INFO]  * src vocab size: 319.\n","[2021-04-20 07:05:25,078 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 07:05:26,315 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 07:05:26,318 INFO]  * src vocab size = 39660\n","[2021-04-20 07:05:26,318 INFO]  * tgt vocab size = 18250\n","[2021-04-20 07:05:26,318 INFO] Building model...\n","[2021-04-20 07:05:33,013 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 07:05:33,046 INFO] encoder: 39221248\n","[2021-04-20 07:05:33,046 INFO] decoder: 43931466\n","[2021-04-20 07:05:33,046 INFO] * number of parameters: 83152714\n","[2021-04-20 07:05:33,589 INFO] Starting training on GPU: [0]\n","[2021-04-20 07:05:33,590 INFO] Start training loop without validation...\n","[2021-04-20 07:05:33,590 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:33,592 INFO] number of examples: 43\n","[2021-04-20 07:05:33,599 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:33,601 INFO] number of examples: 43\n","[2021-04-20 07:05:34,065 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:34,068 INFO] number of examples: 43\n","[2021-04-20 07:05:34,070 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:34,072 INFO] number of examples: 43\n","[2021-04-20 07:05:34,465 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:34,467 INFO] number of examples: 43\n","[2021-04-20 07:05:34,469 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:34,471 INFO] number of examples: 43\n","[2021-04-20 07:05:34,862 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:34,865 INFO] number of examples: 43\n","[2021-04-20 07:05:34,867 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:34,869 INFO] number of examples: 43\n","[2021-04-20 07:05:35,259 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:35,262 INFO] number of examples: 43\n","[2021-04-20 07:05:35,264 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:35,266 INFO] number of examples: 43\n","[2021-04-20 07:05:35,659 INFO] Step 30005/30020; acc:  40.70; ppl: 65.92; xent: 4.19; lr: 0.00051; 2972/3746 tok/s;      2 sec\n","[2021-04-20 07:05:35,821 INFO] Saving checkpoint bert_law/model_law_50/en-vi_step_30005.pt\n","[2021-04-20 07:05:40,781 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:40,784 INFO] number of examples: 43\n","[2021-04-20 07:05:40,786 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:40,788 INFO] number of examples: 43\n","[2021-04-20 07:05:41,190 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:41,193 INFO] number of examples: 43\n","[2021-04-20 07:05:41,195 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:41,197 INFO] number of examples: 43\n","[2021-04-20 07:05:41,587 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:41,589 INFO] number of examples: 43\n","[2021-04-20 07:05:41,592 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:41,593 INFO] number of examples: 43\n","[2021-04-20 07:05:41,985 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:41,988 INFO] number of examples: 43\n","[2021-04-20 07:05:41,990 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:41,992 INFO] number of examples: 43\n","[2021-04-20 07:05:42,383 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:42,386 INFO] number of examples: 43\n","[2021-04-20 07:05:42,388 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:42,390 INFO] number of examples: 43\n","[2021-04-20 07:05:42,781 INFO] Step 30010/30020; acc:  81.99; ppl:  3.40; xent: 1.23; lr: 0.00051; 864/1088 tok/s;      9 sec\n","[2021-04-20 07:05:42,942 INFO] Saving checkpoint bert_law/model_law_50/en-vi_step_30010.pt\n","[2021-04-20 07:05:46,629 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:46,631 INFO] number of examples: 43\n","[2021-04-20 07:05:46,633 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:46,635 INFO] number of examples: 43\n","[2021-04-20 07:05:47,030 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:47,033 INFO] number of examples: 43\n","[2021-04-20 07:05:47,036 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:47,038 INFO] number of examples: 43\n","[2021-04-20 07:05:47,441 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:47,443 INFO] number of examples: 43\n","[2021-04-20 07:05:47,446 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:47,449 INFO] number of examples: 43\n","[2021-04-20 07:05:47,849 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:47,851 INFO] number of examples: 43\n","[2021-04-20 07:05:47,854 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:47,855 INFO] number of examples: 43\n","[2021-04-20 07:05:48,253 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:48,256 INFO] number of examples: 43\n","[2021-04-20 07:05:48,258 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:48,260 INFO] number of examples: 43\n","[2021-04-20 07:05:48,653 INFO] Step 30015/30020; acc:  91.79; ppl:  1.63; xent: 0.49; lr: 0.00051; 1047/1320 tok/s;     15 sec\n","[2021-04-20 07:05:48,824 INFO] Saving checkpoint bert_law/model_law_50/en-vi_step_30015.pt\n","[2021-04-20 07:05:56,180 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:56,183 INFO] number of examples: 43\n","[2021-04-20 07:05:56,187 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:56,189 INFO] number of examples: 43\n","[2021-04-20 07:05:56,592 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:56,594 INFO] number of examples: 43\n","[2021-04-20 07:05:56,597 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:56,599 INFO] number of examples: 43\n","[2021-04-20 07:05:56,999 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:57,002 INFO] number of examples: 43\n","[2021-04-20 07:05:57,005 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:57,008 INFO] number of examples: 43\n","[2021-04-20 07:05:57,405 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:57,407 INFO] number of examples: 43\n","[2021-04-20 07:05:57,410 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:57,412 INFO] number of examples: 43\n","[2021-04-20 07:05:57,813 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:57,816 INFO] number of examples: 43\n","[2021-04-20 07:05:57,819 INFO] Loading dataset from bert_law/output_law_50/en-vi.train.0.pt\n","[2021-04-20 07:05:57,821 INFO] number of examples: 43\n","[2021-04-20 07:05:58,230 INFO] Step 30020/30020; acc:  97.33; ppl:  1.25; xent: 0.22; lr: 0.00051; 642/809 tok/s;     25 sec\n","[2021-04-20 07:05:58,416 INFO] Saving checkpoint bert_law/model_law_50/en-vi_step_30020.pt\n","[2021-04-20 07:06:16,002 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:06:48,699 INFO] PRED AVG SCORE: -0.8530, PRED PPL: 2.3466\n","[2021-04-20 07:06:48,699 INFO] GOLD AVG SCORE: -6.2897, GOLD PPL: 538.9767\n","BLEU = 5.06, 23.5/7.7/2.8/1.7 (BP=0.937, ratio=0.939, hyp_len=2101, ref_len=2237)\n"," ===05==^======= \n","[2021-04-20 07:06:53,097 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:07:23,666 INFO] PRED AVG SCORE: -0.9428, PRED PPL: 2.5671\n","[2021-04-20 07:07:23,666 INFO] GOLD AVG SCORE: -7.4651, GOLD PPL: 1746.1082\n","BLEU = 1.79, 15.6/5.0/1.8/0.9 (BP=0.534, ratio=0.614, hyp_len=1374, ref_len=2237)\n"," ===10==^======= \n","[2021-04-20 07:07:27,042 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:07:44,279 INFO] PRED AVG SCORE: -0.5169, PRED PPL: 1.6767\n","[2021-04-20 07:07:44,279 INFO] GOLD AVG SCORE: -8.1627, GOLD PPL: 3507.7224\n","BLEU = 0.74, 10.7/5.2/3.2/2.5 (BP=0.162, ratio=0.355, hyp_len=794, ref_len=2237)\n"," ===15==^======= \n","[2021-04-20 07:07:47,680 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:08:06,167 INFO] PRED AVG SCORE: -0.6678, PRED PPL: 1.9500\n","[2021-04-20 07:08:06,167 INFO] GOLD AVG SCORE: -7.1733, GOLD PPL: 1304.1835\n","BLEU = 1.20, 11.0/4.8/2.6/2.1 (BP=0.293, ratio=0.449, hyp_len=1004, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8AX0zOuPtfA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902640384,"user_tz":-420,"elapsed":328830,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"9ca97e94-eec5-4398-87d4-9e2a488d64c9"},"source":["!mkdir -p bert_law/output_law_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_law_100' \\\\\n","-train_tgt 'finetune/vi_finetune_law_100' \\\\\n","-save_data 'bert_law/output_law_100/en-vi' \n","\n","!mkdir -p bert_law/model_law_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_law/output_law_100/en-vi' \\\\\n","-save_model 'bert_law/model_law_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_law/model_law_100/en-vi_step_30005.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_law/model_law_100/en-vi_step_30010.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_law/model_law_100/en-vi_step_30015.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_law/model_law_100/en-vi_step_30020.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:08:07,659 INFO] Extracting features...\n","[2021-04-20 07:08:07,661 INFO]  * number of source features: 0.\n","[2021-04-20 07:08:07,661 INFO]  * number of target features: 0.\n","[2021-04-20 07:08:07,661 INFO] Building `Fields` object...\n","[2021-04-20 07:08:07,661 INFO] Building & saving training data...\n","[2021-04-20 07:08:07,671 INFO] Building shard 0.\n","[2021-04-20 07:08:07,675 INFO]  * saving 0th train data shard to bert_law/output_law_100/en-vi.train.0.pt.\n","[2021-04-20 07:08:07,772 INFO]  * tgt vocab size: 574.\n","[2021-04-20 07:08:07,773 INFO]  * src vocab size: 579.\n","[2021-04-20 07:08:10,225 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 07:08:11,423 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 07:08:11,426 INFO]  * src vocab size = 39660\n","[2021-04-20 07:08:11,426 INFO]  * tgt vocab size = 18250\n","[2021-04-20 07:08:11,426 INFO] Building model...\n","[2021-04-20 07:08:14,601 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 07:08:14,620 INFO] encoder: 39221248\n","[2021-04-20 07:08:14,620 INFO] decoder: 43931466\n","[2021-04-20 07:08:14,621 INFO] * number of parameters: 83152714\n","[2021-04-20 07:08:15,190 INFO] Starting training on GPU: [0]\n","[2021-04-20 07:08:15,190 INFO] Start training loop without validation...\n","[2021-04-20 07:08:15,190 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:15,193 INFO] number of examples: 85\n","[2021-04-20 07:08:15,604 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:15,608 INFO] number of examples: 85\n","[2021-04-20 07:08:16,001 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:16,004 INFO] number of examples: 85\n","[2021-04-20 07:08:16,377 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:16,380 INFO] number of examples: 85\n","[2021-04-20 07:08:16,753 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:16,765 INFO] number of examples: 85\n","[2021-04-20 07:08:17,138 INFO] Step 30005/30020; acc:  32.48; ppl: 105.27; xent: 4.66; lr: 0.00051; 3154/4198 tok/s;      2 sec\n","[2021-04-20 07:08:17,298 INFO] Saving checkpoint bert_law/model_law_100/en-vi_step_30005.pt\n","[2021-04-20 07:08:21,289 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:21,293 INFO] number of examples: 85\n","[2021-04-20 07:08:21,682 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:21,685 INFO] number of examples: 85\n","[2021-04-20 07:08:22,064 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:22,068 INFO] number of examples: 85\n","[2021-04-20 07:08:22,445 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:22,448 INFO] number of examples: 85\n","[2021-04-20 07:08:22,832 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:22,835 INFO] number of examples: 85\n","[2021-04-20 07:08:23,215 INFO] Step 30010/30020; acc:  71.89; ppl:  5.47; xent: 1.70; lr: 0.00051; 1011/1346 tok/s;      8 sec\n","[2021-04-20 07:08:23,374 INFO] Saving checkpoint bert_law/model_law_100/en-vi_step_30010.pt\n","[2021-04-20 07:08:29,531 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:29,620 INFO] number of examples: 85\n","[2021-04-20 07:08:30,022 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:30,025 INFO] number of examples: 85\n","[2021-04-20 07:08:30,421 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:30,424 INFO] number of examples: 85\n","[2021-04-20 07:08:30,811 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:30,815 INFO] number of examples: 85\n","[2021-04-20 07:08:31,209 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:31,213 INFO] number of examples: 85\n","[2021-04-20 07:08:31,598 INFO] Step 30015/30020; acc:  87.91; ppl:  1.99; xent: 0.69; lr: 0.00051; 733/976 tok/s;     16 sec\n","[2021-04-20 07:08:31,772 INFO] Saving checkpoint bert_law/model_law_100/en-vi_step_30015.pt\n","[2021-04-20 07:08:37,487 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:42,897 INFO] number of examples: 85\n","[2021-04-20 07:08:43,289 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:43,292 INFO] number of examples: 85\n","[2021-04-20 07:08:43,684 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:43,687 INFO] number of examples: 85\n","[2021-04-20 07:08:44,083 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:44,088 INFO] number of examples: 85\n","[2021-04-20 07:08:44,506 INFO] Loading dataset from bert_law/output_law_100/en-vi.train.0.pt\n","[2021-04-20 07:08:44,510 INFO] number of examples: 85\n","[2021-04-20 07:08:44,918 INFO] Step 30020/30020; acc:  95.99; ppl:  1.33; xent: 0.29; lr: 0.00051; 461/614 tok/s;     30 sec\n","[2021-04-20 07:08:45,139 INFO] Saving checkpoint bert_law/model_law_100/en-vi_step_30020.pt\n","[2021-04-20 07:08:58,268 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:09:27,962 INFO] PRED AVG SCORE: -0.9354, PRED PPL: 2.5482\n","[2021-04-20 07:09:27,962 INFO] GOLD AVG SCORE: -5.7446, GOLD PPL: 312.4898\n","BLEU = 4.48, 28.4/10.3/3.3/1.5 (BP=0.720, ratio=0.753, hyp_len=1684, ref_len=2237)\n"," ===05==^======= \n","[2021-04-20 07:09:31,354 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:09:53,684 INFO] PRED AVG SCORE: -0.8414, PRED PPL: 2.3196\n","[2021-04-20 07:09:53,684 INFO] GOLD AVG SCORE: -6.0329, GOLD PPL: 416.9036\n","BLEU = 3.90, 24.4/9.1/3.3/1.6 (BP=0.664, ratio=0.709, hyp_len=1587, ref_len=2237)\n"," ===10==^======= \n","[2021-04-20 07:09:57,127 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:10:16,107 INFO] PRED AVG SCORE: -0.7276, PRED PPL: 2.0701\n","[2021-04-20 07:10:16,107 INFO] GOLD AVG SCORE: -6.8083, GOLD PPL: 905.3598\n","BLEU = 1.80, 15.3/7.0/3.5/2.7 (BP=0.319, ratio=0.467, hyp_len=1044, ref_len=2237)\n"," ===15==^======= \n","[2021-04-20 07:10:19,520 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:10:39,075 INFO] PRED AVG SCORE: -0.7435, PRED PPL: 2.1033\n","[2021-04-20 07:10:39,076 INFO] GOLD AVG SCORE: -6.5389, GOLD PPL: 691.4997\n","BLEU = 3.08, 18.2/7.7/3.7/2.4 (BP=0.516, ratio=0.602, hyp_len=1347, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmEAHw2aVdLh","executionInfo":{"status":"ok","timestamp":1618902803869,"user_tz":-420,"elapsed":163473,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"44584715-fcd3-4438-de21-886d7599a7f3"},"source":["!mkdir -p bert_law/output_law_117\n","!onmt_preprocess -train_src 'finetune/en_finetune_law_117' \\\\\n","-train_tgt 'finetune/vi_finetune_law_117' \\\\\n","-save_data 'bert_law/output_law_117/en-vi' \n","\n","!mkdir -p bert_law/model_law_117\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_law/output_law_117/en-vi' \\\\\n","-save_model 'bert_law/model_law_117/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_law/model_law_117/en-vi_step_30005.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_117/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_117/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_law/model_law_117/en-vi_step_30010.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_117/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_117/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_law/model_law_117/en-vi_step_30015.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_117/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_117/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_law/model_law_117/en-vi_step_30020.pt -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/model_law_117/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/model_law_117/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:10:42,140 INFO] Extracting features...\n","[2021-04-20 07:10:42,145 INFO]  * number of source features: 0.\n","[2021-04-20 07:10:42,145 INFO]  * number of target features: 0.\n","[2021-04-20 07:10:42,145 INFO] Building `Fields` object...\n","[2021-04-20 07:10:42,145 INFO] Building & saving training data...\n","[2021-04-20 07:10:42,155 INFO] Building shard 0.\n","[2021-04-20 07:10:42,160 INFO]  * saving 0th train data shard to bert_law/output_law_117/en-vi.train.0.pt.\n","[2021-04-20 07:10:42,257 INFO]  * tgt vocab size: 608.\n","[2021-04-20 07:10:42,258 INFO]  * src vocab size: 629.\n","[2021-04-20 07:10:44,658 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 07:10:45,868 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 07:10:45,871 INFO]  * src vocab size = 39660\n","[2021-04-20 07:10:45,872 INFO]  * tgt vocab size = 18250\n","[2021-04-20 07:10:45,872 INFO] Building model...\n","[2021-04-20 07:10:49,090 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 07:10:49,176 INFO] encoder: 39221248\n","[2021-04-20 07:10:49,176 INFO] decoder: 43931466\n","[2021-04-20 07:10:49,176 INFO] * number of parameters: 83152714\n","[2021-04-20 07:10:49,722 INFO] Starting training on GPU: [0]\n","[2021-04-20 07:10:49,722 INFO] Start training loop without validation...\n","[2021-04-20 07:10:49,722 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:49,725 INFO] number of examples: 101\n","[2021-04-20 07:10:50,128 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:50,132 INFO] number of examples: 101\n","[2021-04-20 07:10:50,520 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:50,523 INFO] number of examples: 101\n","[2021-04-20 07:10:50,901 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:50,904 INFO] number of examples: 101\n","[2021-04-20 07:10:51,282 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:51,285 INFO] number of examples: 101\n","[2021-04-20 07:10:51,663 INFO] Step 30005/30020; acc:  31.22; ppl: 117.53; xent: 4.77; lr: 0.00051; 3540/4707 tok/s;      2 sec\n","[2021-04-20 07:10:51,830 INFO] Saving checkpoint bert_law/model_law_117/en-vi_step_30005.pt\n","[2021-04-20 07:10:55,658 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:56,265 INFO] number of examples: 101\n","[2021-04-20 07:10:56,657 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:56,661 INFO] number of examples: 101\n","[2021-04-20 07:10:57,040 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:57,043 INFO] number of examples: 101\n","[2021-04-20 07:10:57,425 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:57,428 INFO] number of examples: 101\n","[2021-04-20 07:10:57,813 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:10:57,817 INFO] number of examples: 101\n","[2021-04-20 07:10:58,193 INFO] Step 30010/30020; acc:  69.13; ppl:  6.28; xent: 1.84; lr: 0.00051; 1052/1399 tok/s;      8 sec\n","[2021-04-20 07:10:58,356 INFO] Saving checkpoint bert_law/model_law_117/en-vi_step_30010.pt\n","[2021-04-20 07:11:03,083 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:03,096 INFO] number of examples: 101\n","[2021-04-20 07:11:03,487 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:03,491 INFO] number of examples: 101\n","[2021-04-20 07:11:03,884 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:03,888 INFO] number of examples: 101\n","[2021-04-20 07:11:04,269 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:04,273 INFO] number of examples: 101\n","[2021-04-20 07:11:04,660 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:04,664 INFO] number of examples: 101\n","[2021-04-20 07:11:05,059 INFO] Step 30015/30020; acc:  87.33; ppl:  2.08; xent: 0.73; lr: 0.00051; 1001/1331 tok/s;     15 sec\n","[2021-04-20 07:11:05,244 INFO] Saving checkpoint bert_law/model_law_117/en-vi_step_30015.pt\n","[2021-04-20 07:11:16,977 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:17,046 INFO] number of examples: 101\n","[2021-04-20 07:11:17,469 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:17,473 INFO] number of examples: 101\n","[2021-04-20 07:11:17,875 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:17,879 INFO] number of examples: 101\n","[2021-04-20 07:11:18,286 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:18,290 INFO] number of examples: 101\n","[2021-04-20 07:11:18,689 INFO] Loading dataset from bert_law/output_law_117/en-vi.train.0.pt\n","[2021-04-20 07:11:18,694 INFO] number of examples: 101\n","[2021-04-20 07:11:19,107 INFO] Step 30020/30020; acc:  95.31; ppl:  1.35; xent: 0.30; lr: 0.00051; 489/650 tok/s;     29 sec\n","[2021-04-20 07:11:19,305 INFO] Saving checkpoint bert_law/model_law_117/en-vi_step_30020.pt\n","[2021-04-20 07:11:43,487 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:12:10,087 INFO] PRED AVG SCORE: -0.9546, PRED PPL: 2.5977\n","[2021-04-20 07:12:10,087 INFO] GOLD AVG SCORE: -5.7232, GOLD PPL: 305.8868\n","BLEU = 4.26, 27.4/10.0/3.1/1.2 (BP=0.757, ratio=0.782, hyp_len=1750, ref_len=2237)\n"," ===05==^======= \n","[2021-04-20 07:12:13,569 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:12:40,367 INFO] PRED AVG SCORE: -0.8482, PRED PPL: 2.3355\n","[2021-04-20 07:12:40,367 INFO] GOLD AVG SCORE: -5.8594, GOLD PPL: 350.5003\n","BLEU = 5.26, 27.0/10.3/4.3/2.3 (BP=0.730, ratio=0.760, hyp_len=1701, ref_len=2237)\n"," ===10==^======= \n","[2021-04-20 07:12:43,831 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:13:02,696 INFO] PRED AVG SCORE: -0.8057, PRED PPL: 2.2383\n","[2021-04-20 07:13:02,696 INFO] GOLD AVG SCORE: -6.4155, GOLD PPL: 611.2534\n","BLEU = 2.17, 19.9/9.1/4.5/3.2 (BP=0.303, ratio=0.456, hyp_len=1019, ref_len=2237)\n"," ===15==^======= \n","[2021-04-20 07:13:06,090 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:13:22,555 INFO] PRED AVG SCORE: -0.7875, PRED PPL: 2.1978\n","[2021-04-20 07:13:22,555 INFO] GOLD AVG SCORE: -6.4161, GOLD PPL: 611.5879\n","BLEU = 2.08, 22.8/10.2/5.2/3.3 (BP=0.262, ratio=0.427, hyp_len=956, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618902834271,"user_tz":-420,"elapsed":193870,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"216cde80-e5d9-47a8-c5a1-65d8b9ee60cc"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_law.en -tgt finetune/test_law.vi -output bert_law/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_law.vi < bert_law/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2021-04-20 07:13:26,282 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 07:13:53,067 INFO] PRED AVG SCORE: -1.1685, PRED PPL: 3.2173\n","[2021-04-20 07:13:53,067 INFO] GOLD AVG SCORE: -8.3939, GOLD PPL: 4420.1260\n","BLEU = 2.95, 19.5/5.7/1.4/0.5 (BP=0.983, ratio=0.983, hyp_len=2200, ref_len=2237)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}