{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TED-OpenNMT-BERT20210420-0327 BERT buddhism.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LOhk_Tcumu7c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889992214,"user_tz":-420,"elapsed":18753,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"d90004bb-fe24-410e-df6f-064f7cc13a49"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42yosgiGoLTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889995351,"user_tz":-420,"elapsed":1699,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2ccb3c56-75b4-49e3-c5f9-c9e2cf77ef5d"},"source":["# import os\n","# # path = \"\"\n","# path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/'\n","# os.chdir(path)\n","# import time\n","# FOLDERNAME = \"TED-OpenNMT-BERT\" + str(time.strftime(\"%Y%m%d-%H%M\"))\n","# !mkdir $FOLDERNAME\n","\n","# path = path + FOLDERNAME\n","# os.chdir(path)\n","# !pwd\n","\n","import os\n","path = '/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327'\n","os.chdir(path)\n","!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/chinh-share/nmt-v4.2-SIF/TED-OpenNMT-BERT20210420-0327\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHu74LOYETUA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889998501,"user_tz":-420,"elapsed":920,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e8687b1f-6322-482b-b33d-3c18e373c856"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tue Apr 20 03:39:58 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdmPYNIGrNdj"},"source":["## **Install libraries**"]},{"cell_type":"code","metadata":{"id":"r03SCFfjXABE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890016471,"user_tz":-420,"elapsed":15824,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e1a27e4a-f983-4b53-8afb-df0eeeec9ce9"},"source":["!pip install OpenNMT-py==1.2.0\n","!pip install -U scikit-learn"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting OpenNMT-py==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n","\r\u001b[K     |█▊                              | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 30.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 122kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 133kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (0.16.0)\n","Collecting configargparse\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hCollecting waitress\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cf/a9e9590023684dbf4e7861e261b0cfd6498a62396c748e661577ca720a29/waitress-2.0.0-py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.15.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.1.2)\n","Collecting pyonmttok==1.*; platform_system == \"Linux\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/67/cd64b4c2fd0a83eb1088e31e0217b612281d014299993424420f933df3e7/pyonmttok-1.26.0-cp37-cp37m-manylinux1_x86_64.whl (14.3MB)\n","\u001b[K     |████████████████████████████████| 14.3MB 26.9MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (2.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (4.41.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (1.8.1+cu101)\n","Collecting torchtext==0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==1.2.0) (3.13)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.0.1)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (2.11.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (1.1.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py==1.2.0) (7.1.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.28.1)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.19.5)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.3.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (1.32.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (2.23.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.36.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->OpenNMT-py==1.2.0) (54.2.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->OpenNMT-py==1.2.0) (3.7.4.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py==1.2.0) (1.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (4.7.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.10.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py==1.2.0) (2020.12.5)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py==1.2.0) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py==1.2.0) (3.1.0)\n","Building wheels for collected packages: configargparse\n","  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=e1664091850aa6ee1d226d776bc4f87d95f1c266d6ee310382de509b52a71c6e\n","  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n","Successfully built configargparse\n","Installing collected packages: configargparse, waitress, pyonmttok, torchtext, OpenNMT-py\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","Successfully installed OpenNMT-py-1.2.0 configargparse-1.4 pyonmttok-1.26.0 torchtext-0.4.0 waitress-2.0.0\n","Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAscbACoOygX","executionInfo":{"status":"ok","timestamp":1618887835865,"user_tz":-420,"elapsed":1201,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2c865aae-0635-4032-c4e5-9b4226bfb7c4"},"source":["!rm -rf SIF-finetune.tar.gz\n","!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQX3CyRxJPn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618889442257,"user_tz":-420,"elapsed":1687,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"12a7c093-4a7c-4137-8c78-b7085a63b06e"},"source":["!wget -N https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","!tar -xvf 'SIF-finetune-bert.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-20 03:30:40--  https://raw.githubusercontent.com/hoangtrungchinh/clc_data/master/dataset2/SIF-finetune-bert.tar.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 180907 (177K) [application/octet-stream]\n","Saving to: ‘SIF-finetune-bert.tar.gz’\n","\n","\rSIF-finetune-bert.t   0%[                    ]       0  --.-KB/s               \rSIF-finetune-bert.t 100%[===================>] 176.67K  --.-KB/s    in 0.03s   \n","\n","Last-modified header missing -- time-stamps turned off.\n","2021-04-20 03:30:41 (6.05 MB/s) - ‘SIF-finetune-bert.tar.gz’ saved [180907/180907]\n","\n","finetune/\n","finetune/en_finetune_climate_150\n","finetune/vi_finetune_climate_50\n","finetune/vi_finetune_climate_150\n","finetune/en_finetune_climate_100\n","finetune/vi_finetune_buddhism_162\n","finetune/en_finetune_law_50\n","finetune/vi_finetune_buddhism_50\n","finetune/vi_finetune_buddhism_100\n","finetune/test_climate.vi\n","finetune/en_finetune_climate_191\n","finetune/test_catechism.vi\n","finetune/vi_finetune_law_50\n","finetune/en_finetune_law_100\n","finetune/test_law.en\n","finetune/en_finetune_buddhism_162\n","finetune/vi_finetune_law_129\n","finetune/vi_finetune_catechism_50\n","finetune/en_finetune_climate_50\n","finetune/vi_finetune_catechism_100\n","finetune/en_finetune_buddhism_100\n","finetune/test_climate.en\n","finetune/en_finetune_catechism_112\n","finetune/vi_finetune_law_100\n","finetune/test_law.vi\n","finetune/en_finetune_buddhism_150\n","finetune/vi_finetune_catechism_112\n","finetune/vi_finetune_buddhism_150\n","finetune/vi_finetune_climate_191\n","finetune/en_finetune_buddhism_50\n","finetune/test_catechism.en\n","finetune/test_buddhism.vi\n","finetune/test_buddhism.en\n","finetune/vi_finetune_climate_100\n","finetune/en_finetune_law_129\n","finetune/en_finetune_catechism_100\n","finetune/en_finetune_catechism_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOXWYOWlHkcG","executionInfo":{"status":"ok","timestamp":1618889447511,"user_tz":-420,"elapsed":1467,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"2ab041d5-da8a-4d87-c2e4-0d8fa665e0fe"},"source":["!ls -al finetune"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 535\n","-rw------- 1 root root  9453 Apr 20 02:00 en_finetune_buddhism_100\n","-rw------- 1 root root 14772 Apr 20 02:00 en_finetune_buddhism_150\n","-rw------- 1 root root 16471 Apr 20 02:00 en_finetune_buddhism_162\n","-rw------- 1 root root  4892 Apr 20 02:00 en_finetune_buddhism_50\n","-rw------- 1 root root 13295 Apr 20 02:00 en_finetune_catechism_100\n","-rw------- 1 root root 14691 Apr 20 02:00 en_finetune_catechism_112\n","-rw------- 1 root root  6722 Apr 20 02:00 en_finetune_catechism_50\n","-rw------- 1 root root 13996 Apr 20 02:00 en_finetune_climate_100\n","-rw------- 1 root root 21088 Apr 20 02:00 en_finetune_climate_150\n","-rw------- 1 root root 27050 Apr 20 02:00 en_finetune_climate_191\n","-rw------- 1 root root  7039 Apr 20 02:00 en_finetune_climate_50\n","-rw------- 1 root root 13779 Apr 20 02:00 en_finetune_law_100\n","-rw------- 1 root root 18325 Apr 20 02:00 en_finetune_law_129\n","-rw------- 1 root root  6255 Apr 20 02:00 en_finetune_law_50\n","-rw------- 1 root root  7711 Apr 20 02:00 test_buddhism.en\n","-rw------- 1 root root 11275 Apr 20 02:00 test_buddhism.vi\n","-rw------- 1 root root  9983 Apr 20 02:00 test_catechism.en\n","-rw------- 1 root root 14226 Apr 20 02:00 test_catechism.vi\n","-rw------- 1 root root 11880 Apr 20 02:00 test_climate.en\n","-rw------- 1 root root 15534 Apr 20 02:00 test_climate.vi\n","-rw------- 1 root root 11267 Apr 20 02:00 test_law.en\n","-rw------- 1 root root 15336 Apr 20 02:00 test_law.vi\n","-rw------- 1 root root 13704 Apr 20 02:00 vi_finetune_buddhism_100\n","-rw------- 1 root root 21690 Apr 20 02:00 vi_finetune_buddhism_150\n","-rw------- 1 root root 24138 Apr 20 02:00 vi_finetune_buddhism_162\n","-rw------- 1 root root  7050 Apr 20 02:00 vi_finetune_buddhism_50\n","-rw------- 1 root root 18233 Apr 20 02:00 vi_finetune_catechism_100\n","-rw------- 1 root root 20199 Apr 20 02:00 vi_finetune_catechism_112\n","-rw------- 1 root root  8920 Apr 20 02:00 vi_finetune_catechism_50\n","-rw------- 1 root root 18493 Apr 20 02:00 vi_finetune_climate_100\n","-rw------- 1 root root 27537 Apr 20 02:00 vi_finetune_climate_150\n","-rw------- 1 root root 35637 Apr 20 02:00 vi_finetune_climate_191\n","-rw------- 1 root root  9330 Apr 20 02:00 vi_finetune_climate_50\n","-rw------- 1 root root 17161 Apr 20 02:00 vi_finetune_law_100\n","-rw------- 1 root root 22753 Apr 20 02:00 vi_finetune_law_129\n","-rw------- 1 root root  7974 Apr 20 02:00 vi_finetune_law_50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG8iWKzpItWb","executionInfo":{"status":"ok","timestamp":1618889473587,"user_tz":-420,"elapsed":19250,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"e2c0ec67-ded8-4c4b-e9c3-760c282cf56c"},"source":["!git clone https://github.com/OpenNMT/OpenNMT-py.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'OpenNMT-py'...\n","remote: Enumerating objects: 17272, done.\u001b[K\n","remote: Counting objects: 100% (228/228), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 17272 (delta 139), reused 101 (delta 67), pack-reused 17044\u001b[K\n","Receiving objects: 100% (17272/17272), 273.37 MiB | 23.94 MiB/s, done.\n","Resolving deltas: 100% (12439/12439), done.\n","Checking out files: 100% (228/228), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LswvFB4cxzSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890224333,"user_tz":-420,"elapsed":166818,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ddf95ff8-1ab7-4728-d508-591a5176a60c"},"source":["!mkdir -p bert_buddhism/output_buddhism_50\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_50' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_50' \\\\\n","-save_data 'bert_buddhism/output_buddhism_50/en-vi' \n","\n","!mkdir -p bert_buddhism/model_buddhism_50\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_buddhism/output_buddhism_50/en-vi' \\\\\n","-save_model 'bert_buddhism/model_buddhism_50/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_buddhism/model_buddhism_50/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_50/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_50/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_50/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_50/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_50/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_50/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_50/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_50/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_50/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_50/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_50/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":5,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:41:02,302 INFO] Extracting features...\n","[2021-04-20 03:41:03,827 INFO]  * number of source features: 0.\n","[2021-04-20 03:41:03,827 INFO]  * number of target features: 0.\n","[2021-04-20 03:41:03,827 INFO] Building `Fields` object...\n","[2021-04-20 03:41:03,828 INFO] Building & saving training data...\n","[2021-04-20 03:41:03,839 INFO] Building shard 0.\n","[2021-04-20 03:41:03,841 INFO]  * saving 0th train data shard to bert_buddhism/output_buddhism_50/en-vi.train.0.pt.\n","[2021-04-20 03:41:03,938 INFO]  * tgt vocab size: 535.\n","[2021-04-20 03:41:03,939 INFO]  * src vocab size: 436.\n","[2021-04-20 03:41:22,940 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:41:24,249 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:41:24,252 INFO]  * src vocab size = 39660\n","[2021-04-20 03:41:24,252 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:41:24,252 INFO] Building model...\n","[2021-04-20 03:41:31,807 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:41:31,850 INFO] encoder: 39221248\n","[2021-04-20 03:41:31,851 INFO] decoder: 43931466\n","[2021-04-20 03:41:31,851 INFO] * number of parameters: 83152714\n","[2021-04-20 03:41:32,416 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:41:32,416 INFO] Start training loop without validation...\n","[2021-04-20 03:41:32,416 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:32,418 INFO] number of examples: 46\n","[2021-04-20 03:41:32,427 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:32,430 INFO] number of examples: 46\n","[2021-04-20 03:41:32,731 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:32,734 INFO] number of examples: 46\n","[2021-04-20 03:41:32,737 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:32,739 INFO] number of examples: 46\n","[2021-04-20 03:41:32,952 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:32,955 INFO] number of examples: 46\n","[2021-04-20 03:41:32,958 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:32,961 INFO] number of examples: 46\n","[2021-04-20 03:41:33,169 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:33,172 INFO] number of examples: 46\n","[2021-04-20 03:41:33,175 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:33,177 INFO] number of examples: 46\n","[2021-04-20 03:41:33,384 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:33,387 INFO] number of examples: 46\n","[2021-04-20 03:41:33,390 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:33,393 INFO] number of examples: 46\n","[2021-04-20 03:41:33,601 INFO] Step 30005/30020; acc:  38.22; ppl: 81.71; xent: 4.40; lr: 0.00051; 5841/8534 tok/s;      1 sec\n","[2021-04-20 03:41:33,799 INFO] Saving checkpoint bert_buddhism/model_buddhism_50/en-vi_step_30005.pt\n","[2021-04-20 03:41:37,851 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:37,854 INFO] number of examples: 46\n","[2021-04-20 03:41:37,857 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:37,859 INFO] number of examples: 46\n","[2021-04-20 03:41:38,096 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,099 INFO] number of examples: 46\n","[2021-04-20 03:41:38,102 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,104 INFO] number of examples: 46\n","[2021-04-20 03:41:38,319 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,323 INFO] number of examples: 46\n","[2021-04-20 03:41:38,326 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,329 INFO] number of examples: 46\n","[2021-04-20 03:41:38,539 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,543 INFO] number of examples: 46\n","[2021-04-20 03:41:38,546 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,548 INFO] number of examples: 46\n","[2021-04-20 03:41:38,756 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,759 INFO] number of examples: 46\n","[2021-04-20 03:41:38,762 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:38,764 INFO] number of examples: 46\n","[2021-04-20 03:41:38,975 INFO] Step 30010/30020; acc:  77.23; ppl:  4.79; xent: 1.57; lr: 0.00051; 1288/1881 tok/s;      7 sec\n","[2021-04-20 03:41:39,181 INFO] Saving checkpoint bert_buddhism/model_buddhism_50/en-vi_step_30010.pt\n","[2021-04-20 03:41:43,639 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:43,642 INFO] number of examples: 46\n","[2021-04-20 03:41:43,645 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:43,647 INFO] number of examples: 46\n","[2021-04-20 03:41:43,857 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:43,860 INFO] number of examples: 46\n","[2021-04-20 03:41:43,863 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:43,866 INFO] number of examples: 46\n","[2021-04-20 03:41:44,075 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:44,078 INFO] number of examples: 46\n","[2021-04-20 03:41:44,081 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:44,083 INFO] number of examples: 46\n","[2021-04-20 03:41:44,290 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:44,293 INFO] number of examples: 46\n","[2021-04-20 03:41:44,295 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:44,298 INFO] number of examples: 46\n","[2021-04-20 03:41:44,507 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:44,510 INFO] number of examples: 46\n","[2021-04-20 03:41:44,513 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:44,516 INFO] number of examples: 46\n","[2021-04-20 03:41:44,726 INFO] Step 30015/30020; acc:  91.53; ppl:  1.79; xent: 0.58; lr: 0.00051; 1203/1758 tok/s;     12 sec\n","[2021-04-20 03:41:44,921 INFO] Saving checkpoint bert_buddhism/model_buddhism_50/en-vi_step_30015.pt\n","[2021-04-20 03:41:53,355 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:53,358 INFO] number of examples: 46\n","[2021-04-20 03:41:53,361 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:53,364 INFO] number of examples: 46\n","[2021-04-20 03:41:53,586 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:53,589 INFO] number of examples: 46\n","[2021-04-20 03:41:53,593 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:53,600 INFO] number of examples: 46\n","[2021-04-20 03:41:53,848 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:53,852 INFO] number of examples: 46\n","[2021-04-20 03:41:53,857 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:53,860 INFO] number of examples: 46\n","[2021-04-20 03:41:54,099 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:54,103 INFO] number of examples: 46\n","[2021-04-20 03:41:54,106 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:54,109 INFO] number of examples: 46\n","[2021-04-20 03:41:54,358 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:54,363 INFO] number of examples: 46\n","[2021-04-20 03:41:54,368 INFO] Loading dataset from bert_buddhism/output_buddhism_50/en-vi.train.0.pt\n","[2021-04-20 03:41:54,371 INFO] number of examples: 46\n","[2021-04-20 03:41:54,620 INFO] Step 30020/30020; acc:  97.95; ppl:  1.22; xent: 0.20; lr: 0.00051; 699/1022 tok/s;     22 sec\n","[2021-04-20 03:41:54,878 INFO] Saving checkpoint bert_buddhism/model_buddhism_50/en-vi_step_30020.pt\n","[2021-04-20 03:42:17,236 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:42:40,828 INFO] PRED AVG SCORE: -0.8386, PRED PPL: 2.3132\n","[2021-04-20 03:42:40,828 INFO] GOLD AVG SCORE: -5.3549, GOLD PPL: 211.6434\n","BLEU = 10.46, 32.9/14.3/6.9/3.7 (BP=0.999, ratio=0.999, hyp_len=1887, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:42:46,162 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:43:04,779 INFO] PRED AVG SCORE: -1.1423, PRED PPL: 3.1341\n","[2021-04-20 03:43:04,779 INFO] GOLD AVG SCORE: -6.5539, GOLD PPL: 701.9417\n","BLEU = 7.37, 28.2/12.5/6.2/3.4 (BP=0.797, ratio=0.815, hyp_len=1539, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:43:08,660 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:43:24,940 INFO] PRED AVG SCORE: -0.6121, PRED PPL: 1.8442\n","[2021-04-20 03:43:24,940 INFO] GOLD AVG SCORE: -7.0214, GOLD PPL: 1120.3952\n","BLEU = 1.88, 21.4/7.1/3.3/1.3 (BP=0.376, ratio=0.506, hyp_len=955, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:43:28,731 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:43:43,268 INFO] PRED AVG SCORE: -0.9022, PRED PPL: 2.4651\n","[2021-04-20 03:43:43,268 INFO] GOLD AVG SCORE: -6.1022, GOLD PPL: 446.8224\n","BLEU = 2.00, 24.8/10.0/4.3/1.8 (BP=0.304, ratio=0.456, hyp_len=862, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X8AX0zOuPtfA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890368429,"user_tz":-420,"elapsed":142722,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"ef6725d5-b7cd-4bdd-9741-d18b721f2a67"},"source":["!mkdir -p bert_buddhism/output_buddhism_100\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_100' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_100' \\\\\n","-save_data 'bert_buddhism/output_buddhism_100/en-vi' \n","\n","!mkdir -p bert_buddhism/model_buddhism_100\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_buddhism/output_buddhism_100/en-vi' \\\\\n","-save_model 'bert_buddhism/model_buddhism_100/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_buddhism/model_buddhism_100/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_100/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_100/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_100/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_100/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_100/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_100/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_100/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_100/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_100/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_100/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_100/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":6,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:43:47,264 INFO] Extracting features...\n","[2021-04-20 03:43:48,583 INFO]  * number of source features: 0.\n","[2021-04-20 03:43:48,583 INFO]  * number of target features: 0.\n","[2021-04-20 03:43:48,583 INFO] Building `Fields` object...\n","[2021-04-20 03:43:48,583 INFO] Building & saving training data...\n","[2021-04-20 03:43:48,595 INFO] Building shard 0.\n","[2021-04-20 03:43:48,600 INFO]  * saving 0th train data shard to bert_buddhism/output_buddhism_100/en-vi.train.0.pt.\n","[2021-04-20 03:43:48,696 INFO]  * tgt vocab size: 853.\n","[2021-04-20 03:43:48,697 INFO]  * src vocab size: 740.\n","[2021-04-20 03:43:51,529 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:43:52,898 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:43:52,902 INFO]  * src vocab size = 39660\n","[2021-04-20 03:43:52,902 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:43:52,902 INFO] Building model...\n","[2021-04-20 03:43:57,218 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:43:57,309 INFO] encoder: 39221248\n","[2021-04-20 03:43:57,309 INFO] decoder: 43931466\n","[2021-04-20 03:43:57,309 INFO] * number of parameters: 83152714\n","[2021-04-20 03:43:57,879 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:43:57,879 INFO] Start training loop without validation...\n","[2021-04-20 03:43:57,879 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:43:57,883 INFO] number of examples: 94\n","[2021-04-20 03:43:58,153 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:43:58,158 INFO] number of examples: 94\n","[2021-04-20 03:43:58,392 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:43:58,396 INFO] number of examples: 94\n","[2021-04-20 03:43:58,636 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:43:58,640 INFO] number of examples: 94\n","[2021-04-20 03:43:58,874 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:43:58,878 INFO] number of examples: 94\n","[2021-04-20 03:43:59,113 INFO] Step 30005/30020; acc:  33.80; ppl: 103.51; xent: 4.64; lr: 0.00051; 5647/8191 tok/s;      1 sec\n","[2021-04-20 03:43:59,308 INFO] Saving checkpoint bert_buddhism/model_buddhism_100/en-vi_step_30005.pt\n","[2021-04-20 03:44:04,130 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:04,134 INFO] number of examples: 94\n","[2021-04-20 03:44:04,393 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:04,397 INFO] number of examples: 94\n","[2021-04-20 03:44:04,640 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:04,645 INFO] number of examples: 94\n","[2021-04-20 03:44:04,884 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:04,888 INFO] number of examples: 94\n","[2021-04-20 03:44:05,125 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:05,130 INFO] number of examples: 94\n","[2021-04-20 03:44:05,366 INFO] Step 30010/30020; acc:  66.23; ppl:  8.26; xent: 2.11; lr: 0.00051; 1115/1617 tok/s;      7 sec\n","[2021-04-20 03:44:05,571 INFO] Saving checkpoint bert_buddhism/model_buddhism_100/en-vi_step_30010.pt\n","[2021-04-20 03:44:10,056 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:10,065 INFO] number of examples: 94\n","[2021-04-20 03:44:10,318 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:10,323 INFO] number of examples: 94\n","[2021-04-20 03:44:10,605 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:10,610 INFO] number of examples: 94\n","[2021-04-20 03:44:10,861 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:10,865 INFO] number of examples: 94\n","[2021-04-20 03:44:11,124 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:11,128 INFO] number of examples: 94\n","[2021-04-20 03:44:11,386 INFO] Step 30015/30020; acc:  85.32; ppl:  2.45; xent: 0.89; lr: 0.00051; 1158/1679 tok/s;     14 sec\n","[2021-04-20 03:44:11,594 INFO] Saving checkpoint bert_buddhism/model_buddhism_100/en-vi_step_30015.pt\n","[2021-04-20 03:44:20,212 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:20,227 INFO] number of examples: 94\n","[2021-04-20 03:44:20,519 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:20,523 INFO] number of examples: 94\n","[2021-04-20 03:44:20,797 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:20,803 INFO] number of examples: 94\n","[2021-04-20 03:44:21,081 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:21,086 INFO] number of examples: 94\n","[2021-04-20 03:44:21,352 INFO] Loading dataset from bert_buddhism/output_buddhism_100/en-vi.train.0.pt\n","[2021-04-20 03:44:21,357 INFO] number of examples: 94\n","[2021-04-20 03:44:21,639 INFO] Step 30020/30020; acc:  96.07; ppl:  1.32; xent: 0.28; lr: 0.00051; 680/986 tok/s;     24 sec\n","[2021-04-20 03:44:21,917 INFO] Saving checkpoint bert_buddhism/model_buddhism_100/en-vi_step_30020.pt\n","[2021-04-20 03:44:42,215 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:45:07,968 INFO] PRED AVG SCORE: -0.8498, PRED PPL: 2.3391\n","[2021-04-20 03:45:07,968 INFO] GOLD AVG SCORE: -5.0223, GOLD PPL: 151.7632\n","BLEU = 11.70, 35.2/15.9/7.8/4.3 (BP=1.000, ratio=1.004, hyp_len=1896, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:45:11,733 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:45:32,067 INFO] PRED AVG SCORE: -0.8244, PRED PPL: 2.2806\n","[2021-04-20 03:45:32,068 INFO] GOLD AVG SCORE: -5.2491, GOLD PPL: 190.3917\n","BLEU = 10.76, 34.2/15.3/7.4/3.7 (BP=0.980, ratio=0.980, hyp_len=1852, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:45:35,841 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:45:49,680 INFO] PRED AVG SCORE: -0.8401, PRED PPL: 2.3166\n","[2021-04-20 03:45:49,680 INFO] GOLD AVG SCORE: -5.7915, GOLD PPL: 327.5002\n","BLEU = 3.29, 34.0/14.7/7.6/3.5 (BP=0.306, ratio=0.458, hyp_len=865, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:45:53,493 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:46:07,660 INFO] PRED AVG SCORE: -0.6715, PRED PPL: 1.9571\n","[2021-04-20 03:46:07,660 INFO] GOLD AVG SCORE: -5.6771, GOLD PPL: 292.1071\n","BLEU = 5.81, 33.6/15.2/7.9/4.4 (BP=0.503, ratio=0.592, hyp_len=1119, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HmEAHw2aVdLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890714299,"user_tz":-420,"elapsed":160956,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"9a6f01cd-dfa0-45da-ad56-f8f015867d12"},"source":["!mkdir -p bert_buddhism/output_buddhism_150\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_150' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_150' \\\\\n","-save_data 'bert_buddhism/output_buddhism_150/en-vi' \n","\n","!mkdir -p bert_buddhism/model_buddhism_150\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_buddhism/output_buddhism_150/en-vi' \\\\\n","-save_model 'bert_buddhism/model_buddhism_150/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_buddhism/model_buddhism_150/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_150/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_150/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_150/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_150/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_150/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_150/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_150/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_150/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_150/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_150/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_150/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":9,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:49:14,895 INFO] Extracting features...\n","[2021-04-20 03:49:16,105 INFO]  * number of source features: 0.\n","[2021-04-20 03:49:16,105 INFO]  * number of target features: 0.\n","[2021-04-20 03:49:16,105 INFO] Building `Fields` object...\n","[2021-04-20 03:49:16,105 INFO] Building & saving training data...\n","[2021-04-20 03:49:16,118 INFO] Building shard 0.\n","[2021-04-20 03:49:16,124 INFO]  * saving 0th train data shard to bert_buddhism/output_buddhism_150/en-vi.train.0.pt.\n","[2021-04-20 03:49:16,220 INFO]  * tgt vocab size: 1096.\n","[2021-04-20 03:49:16,221 INFO]  * src vocab size: 1022.\n","[2021-04-20 03:49:18,995 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:49:20,394 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:49:20,399 INFO]  * src vocab size = 39660\n","[2021-04-20 03:49:20,400 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:49:20,400 INFO] Building model...\n","[2021-04-20 03:49:24,784 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:49:24,836 INFO] encoder: 39221248\n","[2021-04-20 03:49:24,836 INFO] decoder: 43931466\n","[2021-04-20 03:49:24,836 INFO] * number of parameters: 83152714\n","[2021-04-20 03:49:25,425 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:49:25,426 INFO] Start training loop without validation...\n","[2021-04-20 03:49:25,426 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:25,435 INFO] number of examples: 139\n","[2021-04-20 03:49:25,708 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:25,714 INFO] number of examples: 139\n","[2021-04-20 03:49:25,957 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:25,963 INFO] number of examples: 139\n","[2021-04-20 03:49:26,201 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:26,207 INFO] number of examples: 139\n","[2021-04-20 03:49:26,447 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:26,453 INFO] number of examples: 139\n","[2021-04-20 03:49:26,693 INFO] Step 30005/30020; acc:  32.19; ppl: 116.18; xent: 4.76; lr: 0.00051; 8387/12249 tok/s;      1 sec\n","[2021-04-20 03:49:26,884 INFO] Saving checkpoint bert_buddhism/model_buddhism_150/en-vi_step_30005.pt\n","[2021-04-20 03:49:31,177 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:31,182 INFO] number of examples: 139\n","[2021-04-20 03:49:31,438 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:31,443 INFO] number of examples: 139\n","[2021-04-20 03:49:31,690 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:31,696 INFO] number of examples: 139\n","[2021-04-20 03:49:31,942 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:31,948 INFO] number of examples: 139\n","[2021-04-20 03:49:32,192 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:32,197 INFO] number of examples: 139\n","[2021-04-20 03:49:32,434 INFO] Step 30010/30020; acc:  59.94; ppl: 11.44; xent: 2.44; lr: 0.00051; 1852/2704 tok/s;      7 sec\n","[2021-04-20 03:49:32,620 INFO] Saving checkpoint bert_buddhism/model_buddhism_150/en-vi_step_30010.pt\n","[2021-04-20 03:49:36,837 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:36,843 INFO] number of examples: 139\n","[2021-04-20 03:49:37,083 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:37,092 INFO] number of examples: 139\n","[2021-04-20 03:49:37,335 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:37,340 INFO] number of examples: 139\n","[2021-04-20 03:49:37,579 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:37,585 INFO] number of examples: 139\n","[2021-04-20 03:49:37,827 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:37,833 INFO] number of examples: 139\n","[2021-04-20 03:49:38,072 INFO] Step 30015/30020; acc:  80.62; ppl:  3.00; xent: 1.10; lr: 0.00051; 1886/2754 tok/s;     13 sec\n","[2021-04-20 03:49:38,267 INFO] Saving checkpoint bert_buddhism/model_buddhism_150/en-vi_step_30015.pt\n","[2021-04-20 03:49:52,897 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:52,904 INFO] number of examples: 139\n","[2021-04-20 03:49:53,164 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:53,172 INFO] number of examples: 139\n","[2021-04-20 03:49:53,450 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:53,457 INFO] number of examples: 139\n","[2021-04-20 03:49:53,721 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:53,728 INFO] number of examples: 139\n","[2021-04-20 03:49:54,009 INFO] Loading dataset from bert_buddhism/output_buddhism_150/en-vi.train.0.pt\n","[2021-04-20 03:49:54,015 INFO] number of examples: 139\n","[2021-04-20 03:49:54,291 INFO] Step 30020/30020; acc:  93.99; ppl:  1.42; xent: 0.35; lr: 0.00051; 655/957 tok/s;     29 sec\n","[2021-04-20 03:49:54,520 INFO] Saving checkpoint bert_buddhism/model_buddhism_150/en-vi_step_30020.pt\n","[2021-04-20 03:50:20,482 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:50:45,069 INFO] PRED AVG SCORE: -0.9021, PRED PPL: 2.4648\n","[2021-04-20 03:50:45,069 INFO] GOLD AVG SCORE: -4.8627, GOLD PPL: 129.3762\n","BLEU = 11.59, 35.1/15.0/7.6/4.5 (BP=1.000, ratio=1.049, hyp_len=1981, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:50:53,043 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:51:12,965 INFO] PRED AVG SCORE: -0.7976, PRED PPL: 2.2202\n","[2021-04-20 03:51:12,965 INFO] GOLD AVG SCORE: -4.8851, GOLD PPL: 132.2987\n","BLEU = 13.55, 39.3/19.3/10.1/5.2 (BP=0.959, ratio=0.960, hyp_len=1813, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:51:17,013 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:51:34,932 INFO] PRED AVG SCORE: -0.9365, PRED PPL: 2.5509\n","[2021-04-20 03:51:34,932 INFO] GOLD AVG SCORE: -5.4026, GOLD PPL: 221.9863\n","BLEU = 6.00, 27.5/12.3/6.0/2.8 (BP=0.689, ratio=0.728, hyp_len=1376, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:51:38,802 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:51:53,548 INFO] PRED AVG SCORE: -0.6323, PRED PPL: 1.8819\n","[2021-04-20 03:51:53,548 INFO] GOLD AVG SCORE: -5.4435, GOLD PPL: 231.2402\n","BLEU = 8.10, 35.7/17.7/9.9/5.5 (BP=0.596, ratio=0.659, hyp_len=1245, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gSOWqRkDWk_T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890526923,"user_tz":-420,"elapsed":156934,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"1105e31c-5849-4f1b-e745-8bb4dd0a8f82"},"source":["!mkdir -p bert_buddhism/output_buddhism_162\n","!onmt_preprocess -train_src 'finetune/en_finetune_buddhism_162' \\\\\n","-train_tgt 'finetune/vi_finetune_buddhism_162' \\\\\n","-save_data 'bert_buddhism/output_buddhism_162/en-vi' \n","\n","!mkdir -p bert_buddhism/model_buddhism_162\n","!onmt_train -train_from \"en-vi_step_30000.pt\" -data 'bert_buddhism/output_buddhism_162/en-vi' \\\\\n","-save_model 'bert_buddhism/model_buddhism_162/en-vi' \\\\\n","-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","-encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","-train_steps 30020  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","-batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 \\\\\n","-decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 \\\\\n","-param_init_glorot -label_smoothing 0.1 -valid_steps 5 -save_checkpoint_steps 5 \\\\\n","-report_every 5 -world_size 1 -gpu_ranks 0\n","\n","!onmt_translate -model bert_buddhism/model_buddhism_162/en-vi_step_30005.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_162/predict-30005.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_162/predict-30005.txt\n","!echo \" ===05==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_162/en-vi_step_30010.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_162/predict-30010.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_162/predict-30010.txt\n","!echo \" ===10==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_162/en-vi_step_30015.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_162/predict-30015.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_162/predict-30015.txt\n","!echo \" ===15==^======= \"\n","!onmt_translate -model bert_buddhism/model_buddhism_162/en-vi_step_30020.pt -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/model_buddhism_162/predict-30020.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/model_buddhism_162/predict-30020.txt\n","!echo \" ===20==^======= \""],"execution_count":7,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:46:11,534 INFO] Extracting features...\n","[2021-04-20 03:46:12,779 INFO]  * number of source features: 0.\n","[2021-04-20 03:46:12,779 INFO]  * number of target features: 0.\n","[2021-04-20 03:46:12,779 INFO] Building `Fields` object...\n","[2021-04-20 03:46:12,779 INFO] Building & saving training data...\n","[2021-04-20 03:46:12,791 INFO] Building shard 0.\n","[2021-04-20 03:46:12,797 INFO]  * saving 0th train data shard to bert_buddhism/output_buddhism_162/en-vi.train.0.pt.\n","[2021-04-20 03:46:12,892 INFO]  * tgt vocab size: 1173.\n","[2021-04-20 03:46:12,894 INFO]  * src vocab size: 1094.\n","[2021-04-20 03:46:15,581 INFO] Loading checkpoint from en-vi_step_30000.pt\n","[2021-04-20 03:46:16,890 INFO] Loading vocab from checkpoint at en-vi_step_30000.pt.\n","[2021-04-20 03:46:16,895 INFO]  * src vocab size = 39660\n","[2021-04-20 03:46:16,895 INFO]  * tgt vocab size = 18250\n","[2021-04-20 03:46:16,895 INFO] Building model...\n","[2021-04-20 03:46:21,162 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(39660, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(18250, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (relu): ReLU()\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=18250, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2021-04-20 03:46:21,182 INFO] encoder: 39221248\n","[2021-04-20 03:46:21,182 INFO] decoder: 43931466\n","[2021-04-20 03:46:21,182 INFO] * number of parameters: 83152714\n","[2021-04-20 03:46:21,765 INFO] Starting training on GPU: [0]\n","[2021-04-20 03:46:21,765 INFO] Start training loop without validation...\n","[2021-04-20 03:46:21,765 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:21,775 INFO] number of examples: 150\n","[2021-04-20 03:46:22,066 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:22,072 INFO] number of examples: 150\n","[2021-04-20 03:46:22,327 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:22,333 INFO] number of examples: 150\n","[2021-04-20 03:46:22,588 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:22,594 INFO] number of examples: 150\n","[2021-04-20 03:46:22,845 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:22,851 INFO] number of examples: 150\n","[2021-04-20 03:46:23,105 INFO] Step 30005/30020; acc:  31.50; ppl: 122.90; xent: 4.81; lr: 0.00051; 8727/12760 tok/s;      1 sec\n","[2021-04-20 03:46:23,294 INFO] Saving checkpoint bert_buddhism/model_buddhism_162/en-vi_step_30005.pt\n","[2021-04-20 03:46:27,374 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:27,379 INFO] number of examples: 150\n","[2021-04-20 03:46:27,660 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:27,667 INFO] number of examples: 150\n","[2021-04-20 03:46:27,930 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:27,936 INFO] number of examples: 150\n","[2021-04-20 03:46:28,195 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:28,202 INFO] number of examples: 150\n","[2021-04-20 03:46:28,460 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:28,465 INFO] number of examples: 150\n","[2021-04-20 03:46:28,720 INFO] Step 30010/30020; acc:  58.36; ppl: 12.64; xent: 2.54; lr: 0.00051; 2083/3046 tok/s;      7 sec\n","[2021-04-20 03:46:28,911 INFO] Saving checkpoint bert_buddhism/model_buddhism_162/en-vi_step_30010.pt\n","[2021-04-20 03:46:33,128 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:33,133 INFO] number of examples: 150\n","[2021-04-20 03:46:33,389 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:33,395 INFO] number of examples: 150\n","[2021-04-20 03:46:33,651 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:33,657 INFO] number of examples: 150\n","[2021-04-20 03:46:33,911 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:33,917 INFO] number of examples: 150\n","[2021-04-20 03:46:34,178 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:34,184 INFO] number of examples: 150\n","[2021-04-20 03:46:34,440 INFO] Step 30015/30020; acc:  79.33; ppl:  3.26; xent: 1.18; lr: 0.00051; 2045/2989 tok/s;     13 sec\n","[2021-04-20 03:46:34,643 INFO] Saving checkpoint bert_buddhism/model_buddhism_162/en-vi_step_30015.pt\n","[2021-04-20 03:46:45,207 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:46,695 INFO] number of examples: 150\n","[2021-04-20 03:46:47,004 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:47,011 INFO] number of examples: 150\n","[2021-04-20 03:46:47,288 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:47,295 INFO] number of examples: 150\n","[2021-04-20 03:46:47,561 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:47,568 INFO] number of examples: 150\n","[2021-04-20 03:46:47,845 INFO] Loading dataset from bert_buddhism/output_buddhism_162/en-vi.train.0.pt\n","[2021-04-20 03:46:47,852 INFO] number of examples: 150\n","[2021-04-20 03:46:48,131 INFO] Step 30020/30020; acc:  93.49; ppl:  1.46; xent: 0.38; lr: 0.00051; 854/1249 tok/s;     26 sec\n","[2021-04-20 03:46:48,362 INFO] Saving checkpoint bert_buddhism/model_buddhism_162/en-vi_step_30020.pt\n","[2021-04-20 03:47:15,922 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:47:41,914 INFO] PRED AVG SCORE: -0.9064, PRED PPL: 2.4753\n","[2021-04-20 03:47:41,915 INFO] GOLD AVG SCORE: -4.8254, GOLD PPL: 124.6301\n","BLEU = 11.93, 36.0/16.6/8.0/4.2 (BP=1.000, ratio=1.021, hyp_len=1928, ref_len=1889)\n"," ===05==^======= \n","[2021-04-20 03:47:45,788 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:48:06,621 INFO] PRED AVG SCORE: -0.7919, PRED PPL: 2.2075\n","[2021-04-20 03:48:06,621 INFO] GOLD AVG SCORE: -4.8191, GOLD PPL: 123.8516\n","BLEU = 11.69, 36.0/16.6/8.2/4.0 (BP=0.991, ratio=0.992, hyp_len=1873, ref_len=1889)\n"," ===10==^======= \n","[2021-04-20 03:48:10,416 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:48:26,231 INFO] PRED AVG SCORE: -0.9725, PRED PPL: 2.6446\n","[2021-04-20 03:48:26,231 INFO] GOLD AVG SCORE: -5.3293, GOLD PPL: 206.3033\n","BLEU = 6.76, 32.5/14.7/7.0/3.0 (BP=0.672, ratio=0.716, hyp_len=1352, ref_len=1889)\n"," ===15==^======= \n","[2021-04-20 03:48:29,962 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:48:46,122 INFO] PRED AVG SCORE: -0.6229, PRED PPL: 1.8643\n","[2021-04-20 03:48:46,122 INFO] GOLD AVG SCORE: -5.4603, GOLD PPL: 235.1705\n","BLEU = 8.20, 34.7/16.2/8.0/4.0 (BP=0.708, ratio=0.743, hyp_len=1404, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBqhujISk_V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618890550054,"user_tz":-420,"elapsed":180032,"user":{"displayName":"Chinh hoang trung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioUTYb9Muh5-9urDiUXdmGavW-fw5E7QZ4KyH6=s64","userId":"05604119531382760831"}},"outputId":"025d93a9-4cda-45db-eef6-3499e9b9a1db"},"source":["# TEST Model\n","!onmt_translate -model \"en-vi_step_30000.pt\" -src finetune/test_buddhism.en -tgt finetune/test_buddhism.vi -output bert_buddhism/predict.txt\n","!perl OpenNMT-py/tools/multi-bleu.perl finetune/test_buddhism.vi < bert_buddhism/predict.txt\n","!echo \" ===20==^======= \""],"execution_count":8,"outputs":[{"output_type":"stream","text":["[2021-04-20 03:48:51,163 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [20, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:19.)\n","  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n","[2021-04-20 03:49:09,330 INFO] PRED AVG SCORE: -0.9317, PRED PPL: 2.5388\n","[2021-04-20 03:49:09,330 INFO] GOLD AVG SCORE: -7.0192, GOLD PPL: 1117.8830\n","BLEU = 11.03, 33.3/14.6/7.8/4.5 (BP=0.965, ratio=0.966, hyp_len=1824, ref_len=1889)\n"," ===20==^======= \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hWhYQsRu_CtS"},"source":[""],"execution_count":null,"outputs":[]}]}